 <!DOCTYPE html>
<html><head>
<meta charset="utf-8">
<title>讲义</title>
</head>
<body style="font-family: verdana, sans-serif;font-size: 12px;color: #202122;">
<p><span style="font-size: 19px; ">Today I first gave an <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=230447&amp;type=content&amp;rcode=dalhousie-2471119" target="_self">introduction to this course</a>.</span></p>
<h2><span style="font-size: 24px;">RAM, Algorithm Analysis and Order Notation</span></h2>
<p><span style="font-size: 19px;">At the beginning of this course, let us learn some basic stuff about algorithm analysis.</span></p>
<h2><span style="font-size: 30px; color: rgb(250, 8, 8);" >Chapter1&2</span> </h2>
<h3><span style="font-size: 19px;">1. Pseudocode</span></h3>
<p><span style="font-size: 19px;">Before we analyze algorithms, let us learn how to describe an algorithm precisely. Instead of using "real" code, we use pseudocode to specify an algorithm. In a piece of pseudocode, we use the most clean and concise expressive method to describe a given algorithm. Sometimes, the pseudocode of an algorithm could even use an English sentence to describe a step of this algorithm. For example, if a complicated algorithm requires us to first sort the given input array, then we could write something like "sort the array A in ascending order using mergesort" as its first step. When writing pseudocode, we are not concerned with software engineering issues, such as data abstraction, modularity, and exception &amp; error handling. Thus, pseudocode focuses on conveying the essence of an algorithm.</span></p>
<p><span style="font-size: 19px;">In this course, you are allowed to follow any convention of pseudocode that you prefer, as long as your pseudocode is readable and consistent. If you do not have any preference yourself, feel free to follow the way in which I write pseudocode, or the convention described in the textbook (Chapter 2).</span></p>
<h3><span style="font-size: 19px;">2. Insertion Sort</span></h3>
<p><span style="font-size: 19px;">Now, let's write down the pseudocode of insertion sort, which is a sorting algorithm taught in a previous course, before we analyze its running time.</span></p>
<pre><span style="font-size: 19px;">INSERTION-SORT(A[1..N])
  for j ← 2 to n
    key ← A[j]</span><br><span style="font-size: 19px;">    i ← j - 1
    while i &gt; 0 and A[i] &gt; key</span><br><span style="font-size: 19px;">      A[i+1] ← A[i]</span><br><span style="font-size: 19px;">      i ← i - 1</span><br><span style="font-size: 19px;">    A[i+1] ← key</span></pre>
<p><span style="font-size: 19px;">How to measure the running time of an algorithm? We may be tempted to use wall-clock time, but this is a bad measurement as it highly depends on the computer on which the algorithm is run. Hence, to analyze an algorithm, we need remove computer details such as processor speed, disk and memory, and one way of achieving this is to count the number of elementary operations only. The goal of algorithm analysis is to predict the resources required by an algorithm, and to do this, we need a machine model under which we express our algorithms, and some way of assigning costs of the operations under this model.</span></p>
<h3><span style="font-size: 19px;">3. Random Access Machine (RAM)</span></h3>
<p><span style="font-size: 19px;">We analyze our algorithms under the random-access machine (RAM) model of computation. Algorithms are implimented as computer programs in a RAM.</span></p>
<p><span style="font-size: 19px;">A figure illustrating RAM:</span></p>
<p><span style="font-size: 19px;"><img src="Pictures/ram.png" alt="RAM" title="RAM" width="1426" height="751"></span></p>
<p><span style="font-size: 19px;">In this machine model, there are unbounded number of local memory cells (words). An integer or a floating-point number can be stored in a memory cell. Program instructions are executed sequentially, i.e., one after another starting from the first to the last; there is no concurrency. The location counter indicates the instruction of the program that is currently being executed.</span></p>
<p><span style="font-size: 19px;">In this model, the time complexity (running time) is defined as the number of instructions executed, and the space complexity is the number of memory cells accessed.</span></p>
<p><span style="font-size: 19px;">The instruction set of RAM contains instructions commonly found in real computers, such as arithmetic operations (add, subtract, multiply, divide, remainder, floor, ceiling), data movement (load, store, copy) and control (conditional &amp; unconditional branch). Each instruction uses 1 unit of time.</span></p>
<p><span style="font-size: 19px;">[Note: I didn't have time to talk about the following paragraph in Section 2 today, but I will talk about it in Section 2 on Friday] There are instructions in real computers that are not listed above. If your algorithm requires them, then make sure to make reasonable assumptions. For example, the exponentiation operation, i.e., computing x to the power of y, would require a numerical algorithm that is costly, and we would NOT assume that there is an instruction in the RAM that can perform this operation. On the other hand, when k is an integer &lt;= the number of bits in a word, then 2<sup>k</sup>&nbsp;can be computed using a left shift. Thus we would assume that 2<sup>k</sup>&nbsp;is a constant-time operation.</span></p>
<p><span style="font-size: 19px;">&nbsp;</span></p>
<p><span style="font-size: 19px;">[section 2 students need to finish reading the last paragraph of lecture 1 notes]</span></p>
<p><span style="font-size: 19px;">There are some limitations of the RAM, and more advanced models have been proposed to address these limitations. For example, to introduce concurrency to the RAM model, the PRAM model of computation was proposed. RAM does not consider memory hierarchy; to address this, models such as the external memory model were proposed. These are typically covered in graduate-level courses. To understand these models, a solid background of analyzing algorithms in the RAM is required. Furthermore, in many cases, algorithm analysis under RAM is sufficient. Thus, in this course, we mainly focus on RAM.</span></p>
<h3><span style="font-size: 19px;">Analysis of Insertion Sort</span></h3>
<p><span style="font-size: 19px;"><strong>1. </strong>To analyze algorithms in RAM, we first need understand that the running time of an algorithm is expressed as a function of its input size. This is because the running time of an algorithm normally grows when the size of input grows. We choose reasonable parameters associated with the problem and use them as input size. For example, in sorting, the array size is the input size. In graph algorithms, we often choose the number of vertices and the number of edges as parameters.</span></p>
<p><span style="font-size: 19px;"><strong>2.</strong> The general steps of analyzing pseudocode are</span></p>
<ol>
<li><span style="font-size: 19px;">For each line of pseudocode, count the number of primitive operations in it. Pay attention to the word "primitive" here; sorting an array is not a primitive operation.</span></li>
<li><span style="font-size: 19px;">Multiply this count with the number of times this line is executed.</span></li>
<li><span style="font-size: 19px;">Sum up over all lines.</span></li>
</ol>
<p><span style="font-size: 19px;"><strong>3.</strong> To use this to analyze insertion sort, we first observe that each line of pseudocode can be implemented using a constant number of RAM instructions. Thus, we let c<sub>i</sub>&nbsp;be the cost of line i, which is a constant. Then, for each value of j in the outer loop, we let t<sub>j</sub>&nbsp;be the number of times that the while loop test in line 4 is executed. With these definitions, we have</span></p>
<pre><span style="font-size: 19px;"> INSERTION-SORT(A[1..N])              cost                times
1   for j ← 2 to n                    c<sub>1</sub>                  n
2     key ← A[j]                      c<sub>2</sub>                  n-1</span><br><span style="font-size: 19px;">3     i ← j - 1                       c<sub>3</sub>                  n-1
4     while i &gt; 0 and A[i] &gt; key      c<sub>4</sub>                  ∑<sub>j=2,3,...,n</sub> t<sub>j</sub></span><br><span style="font-size: 19px;">5       A[i+1] ← A[i]                 c<sub>5</sub>                  ∑<sub>j=2,3,...,n</sub> (t<sub>j</sub>-1)</span><br><span style="font-size: 19px;">6       i ← i - 1                     c<sub>6</sub>                  ∑<sub>j=2,3,...,n</sub> (t<sub>j</sub>-1)</span><br><span style="font-size: 19px;">7     A[i+1] ← key                    c<sub>7</sub>                  n-1</span></pre>
<p><span style="font-size: 19px;">Now let's calculate the running time as a function of n:</span></p>
<p><img src="Pictures/tn.jpg" alt="T(n)" title="T(n)" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></p>
<p><span style="font-size: 19px;">This does not tell us much about the time complexity, as t<sub>j</sub>&nbsp;is not known and it depends on the actual input. The only thing that we can say about t<sub>j</sub>&nbsp;is that its value is between 1 and j (inclusive). To make sense of this function, let us see how the input affects the running time.</span></p>
<p><span style="font-size: 19px;"><strong>4. Best Case</strong>: In the best case, the input array A is already sorted, as this guarantees that t<sub>j</sub>&nbsp;is equal to 1. In this case,</span></p>
<p><span style="font-size: 19px;">T(n) = c<sub>1</sub>n + c<sub>2</sub>(n-1) + c<sub>3</sub>(n-1) + c<sub>4</sub>(n-1) + c<sub>7</sub>(n-1) = (c<sub>1</sub>&nbsp;+ c<sub>2</sub>&nbsp;+ c<sub>3&nbsp;</sub>+ c<sub>4</sub>&nbsp;+ c<sub>7</sub>) n - (c<sub>2</sub>&nbsp;+ c<sub>3&nbsp;</sub>+ c<sub>4</sub>&nbsp;+ c<sub>7</sub>)</span></p>
<p><span style="font-size: 19px;">This is a linear function of n, and thus we say the running time is Θ(n).</span></p>
<p><span style="font-size: 19px;"><strong>5. Worst Case</strong>: The case in which the input array A is reverse sorted is the worst case, as this guarantees that t<sub>j</sub>&nbsp;is equal to j. In this case,</span></p>
<p><img src="Pictures/tn2.jpg" alt="worst case" title="worst case" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></p>
<p><span style="font-size: 19px;">This is a quadratic function of n, and thus we say the running time is Θ(n<sup>2</sup>).</span></p>
<p><span style="font-size: 19px;"><strong> 6. Average Case</strong>: To perform the average-case analysis, we need assume that each of the n! permutations of A is equally likely. This is much more complicated, and if you are interested, you could read the following book (available in our library) to find out (not required):</span></p>
<p><span style="font-size: 19px;">The Art of Computer Programming, Volume 3: Sorting and Searching (2nd Edition), Donald E. Knuth.</span></p>
<p><span style="font-size: 19px;">If you intend to read this to enrich your knowledge, you could start by reading the discussions on inversions in the input array (5.1.1) and other properties of permutations in the same chapter, Before reading 5.2.1: Sorting by Insertion.</span></p>
<p><span style="font-size: 19px;">In this course, we mainly (though not always) concentrate on worst-case analysis. As pointed out by the textbook, there are many good reasons: Worst-case analysis gives an upper bound on the running time of the algorithm, guaranteeing the algorithm will not be more costly than the result of the analysis. In many practical applications, the worst-case happens very frequently. For example, when searching for records in a real database application, it is very often that the query would look for some key value that is not in the database. Finally, for many algorithms, the average-case running time is roughly as bad the their worst-case running time.</span></p>
<p><span style="font-size: 19px;">To appreciate the importance of good algorithm design, take the problem of sorting an array of SIN numbers for example. According to the data published by Statistics Canada, at the end of 2015, the population in Canada is roughly 34,880,000. We use this number as the value of n. Then, n<sup>2</sup>&nbsp;is roughly 10<sup>15</sup>. Let's first throw away the constants in the analysis of insertion sort, and say this is the number of instructions required. If we use faster sorting algorithms whose cost is proportional to n lg n (in algorithms, lg n is log<sub>2</sub>&nbsp;n), then for this input, lg n is roughly 25 and n lg n is roughly 10<sup>9</sup>. If the CPU performs 10<sup>11</sup>&nbsp;(100 billion) instructions per second, then insertion sort would require 10<sup>4</sup>&nbsp;seconds which are roughly 3 hours, while mergesort and heapsort would require 1/100 second. There is a big difference. In fact, the number of instructions of insertion sort should be a constant times n<sup>2</sup>, and if the constant is about 20, then it would require a long weekend. If you use mergesort/heapsort, then, even if the constant is 200, the sorting would be done in a matter of seconds.</span></p>

<h2><span style="font-size: 30px;color: rgb(250, 8, 8)">Chapter3</span> </h2>
<h3><span style="font-size: 19px;">Order (Rate) of Growth</span></h3>
<p><span style="font-size: 19px;">Another thing to note is that, when presenting the result of analysis, we used Θ to throw away the constant factors. This is because we care about the order (rate) of the growth of the function that represents the running time, and we often care less about the coefficients. Asymptotic notation allows us to make more simplifying abstraction.</span></p>
<h3><span style="font-size: 19px;">1. Θ-Notation</span></h3>
<p><span style="font-size: 19px;">First, the definition:</span></p>
<p><span style="font-size: 19px;">Θ(g(n)) = {f(n): there exist positive constants c<sub>1</sub>, c<sub>2</sub>&nbsp;and n<sub>0</sub>, s.t. 0 &lt;= c<sub>1</sub>g(n) &lt;= f(n) &lt;= c<sub>2</sub>g(n) for all n &gt;= n<sub>0</sub>}</span></p>
<p><span style="font-size: 19px;">Here we can say that g(n) is an asymptotically tight bound for f(n).</span></p>
<p><span style="font-size: 19px;">Observe that in the definition, Θ(g(n)) is defined as a set of functions. We however often abuse notation (acceptable here) and say f(n) = Θ(g(n)), and this means that f(n) ∈ Θ(g(n)). Sometimes this allows us to write shorter identities. For example, 20n<sup>2</sup>&nbsp;+ 13n + 1 = 20n<sup>2</sup>&nbsp;+ Θ(n) means 20n<sup>2</sup>&nbsp;+ 13n + 1 = 20n<sup>2</sup>&nbsp;+ f(n) and f(n) ∈ Θ(n) </span></p>
<p><span style="font-size: 19px;">Now, let's use the definition to prove that n<sup>2</sup>/2 + lg n = Θ(n<sup>2</sup>).</span></p>
<p><span style="font-size: 19px;">Proof. To prove this claim, we must determine positive constants c<sub>1</sub>, c<sub>2</sub>&nbsp;and n<sub>0</sub>, s.t.</span></p>
<p><span style="font-size: 19px;">c<sub>1</sub>&nbsp;n<sup>2</sup>&lt;= n<sup>2</sup>/2 + lg n &lt;= c<sub>2</sub>&nbsp;n<sup>2</sup></span></p>
<p><span style="font-size: 19px;">This is equivalent to c<sub>1</sub>&nbsp;&lt;= 1/2 + (lg n) / n<sup>2</sup>&nbsp;&lt;= c<sub>2</sub></span></p>
<p><span style="font-size: 19px;">Setting c<sub>1</sub>&nbsp;= 1/4, c<sub>2</sub>&nbsp;= 3/4 and n<sub>0</sub>&nbsp;= 2 would make this inequality hold (make sure that you do verify this).</span></p>
<p><span style="font-size: 19px;">There are also other choices of values of these constants that would guarantee the inequality, and in your proof, you just have to show the existence of one set, according to the definition.</span></p>
<p><span style="font-size: 19px;">Note that asymptotic notation applies to asymptotically positive functions only, which are functions whose values are positive for all sufficiently large n.</span></p>
<h3><span style="font-size: 19px;">2. O-Notation (Asymptotic Upper Bound)</span></h3>
<p><span style="font-size: 19px;">Definition: O(g(n)) = {f(n): there exist positive constants c and n<sub>0</sub>, s.t. 0 &lt;= f(n) &lt;= c g(n) for all n &gt;= n<sub>0</sub>}</span></p>
<p><span style="font-size: 19px;">Following this definition, we can determine that n, n<sup>2</sup>, 3n<sup>2</sup>&nbsp;+ 4n + 5 are all O(n<sup>2</sup>), while n<sup>3</sup>&nbsp;is not.</span></p>
<p><span style="font-size: 19px;">When we say that the running time of insertion sort is O(n<sup>2</sup>), we mean that the worst-case running time of insertion sort is O(n<sup>2</sup>). When we say that an algorithm runs in polynomial time, we mean that there exists a constant k, s.t. the worst-case running time of this algorithm is O(n<sup>k</sup>).</span></p>
<h3><span style="font-size: 19px;">3. Ω-Notation (Asymptotic Lower Bound)</span></h3>
<p><span style="font-size: 19px;">Definition: Ω(g(n)) = {f(n): there exist positive constants c and n<sub>0</sub>, s.t. 0 &lt;= c g(n) &lt;= f(n) for all n &gt;= n<sub>0</sub>}</span></p>
<p><span style="font-size: 19px;">Based on the definitions, we have the following theorem: f(n) = Θ(g(n)) if and only if f(n) = O(g(n)) and f(n) = Ω(g(n)). For example, the statement n<sup>2</sup>/2 + lg n = Θ(n<sup>2</sup>) is equivalent to n<sup>2</sup>/2 + lg n = O(n<sup>2</sup>) and n<sup>2</sup>/2 + lg n = Ω(n<sup>2</sup>).</span></p>
<p><span style="font-size: 19px;">Some quick examples: n<sup>2</sup>, (lg n) n<sup>2</sup>, 4n<sup>2</sup>+5 are all in Ω(g(n)), while n is not.</span></p>
<h3><span style="font-size: 19px;">4. o-Notation</span></h3>
<p><span style="font-size: 19px;">Definition: o(g(n)) = {f(n): for any positive constant c, there exists a constant n<sub>0</sub>, s.t. 0 &lt;= f(n) &lt; c g(n) for all n &gt;= n<sub>0</sub>}</span></p>
<p><span style="font-size: 19px;">This definition shows that f(n) grows more slowly than g(n).</span></p>
<p><span style="font-size: 19px;">Another definition of o-Notation is that f(n) = o(g(n)) if</span></p>
<pre><span style="font-size: 19px;">lim  f(n)/g(n) = 0</span><br><span style="font-size: 19px;">n→∞
</span></pre>
<p><span style="font-size: 19px;">By the definition of limits in calculus and the first definition of o-notation, we can see that these two definitions of little-oh are equivalent.</span></p>
<p><span style="font-size: 19px;">This second definition allows us to perform some quick analysis. For example, 2n = o(n<sup>2</sup>), but 2n<sup>2</sup>&nbsp;≠o(n<sup>2</sup>).</span></p>
<h3><span style="font-size: 19px;">5. ω-Notation</span></h3>
<p><span style="font-size: 19px;">First, f(n) = ω(g(n)) if and only if g(n) = o(f(n)).</span></p>
<p><span style="font-size: 19px;">Formal definition: ω(g(n)) = {f(n): for any positive constant c, there exists a constant n<sub>0</sub>, s.t. 0 &lt;= c g(n) &lt; f(n) for all n &gt;= n<sub>0</sub>}</span></p>
<p><span style="font-size: 19px;">Another definition using limits: f(n) = ω(g(n)) if</span></p>
<pre><span style="font-size: 19px;">lim  f(n)/g(n) = ∞
n→∞ 
</span></pre>
<h3><span style="font-size: 19px;">6. Properties</span></h3>
<p><span style="font-size: 19px;">Here I give some properties of order notation, which are not difficult to prove using definitions. Think about their correctness when reviewing these properties.</span></p>
<ol>
<li><span style="font-size: 19px;">Transitivity: If f(n) = Θ(g(n)) and g(n) = Θ(h(n)), then f(n) = Θ(h(n)). This also applies to O, Ω, o and ω.</span></li>
<li><span style="font-size: 19px;">Reflexivity: f(n) = Θ(f(n)); f(n) = O(f(n)); f(n) = Ω(f(n)). This is however not true for o or ω.</span></li>
<li><span style="font-size: 19px;">Symmetry: f(n) = Θ(g(n)) if and only if g(n) = Θ(f(n)).</span></li>
<li><span style="font-size: 19px;">Transpose Symmetry: f(n) = O(g(n)) if and only if g(n) = Ω(f(n)); f(n) = o(g(n)) if and only g(n) = ω(f(n)).</span></li>
</ol>
<h3><span style="font-size: 19px;">7. Tricks</span></h3>
<p><span style="font-size: 19px;">Here I give some tricks that are useful for comparing functions using order notation.</span></p>
<p><span style="font-size: 19px;">The first trick is that we can determine order notation by computing</span></p>
<pre><span style="font-size: 19px;">lim  f(n)/g(n) = c
n→∞ 
</span></pre>
<p><span style="font-size: 19px;">If the result, c, is 0, then f(n) = o(g(n)). If c = ∞, then f(n) = ω(g(n)). If 0 &lt; c &lt; ∞, then f(n) = Θ(g(n)), which also implies that both f(n) = O(g(n)) and f(n) = Ω(g(n)) hold.</span></p>
<p><span style="font-size: 19px;">Let us use this to prove the following claim: Let d be a nonnegative constant integer and a<sub>0</sub>, a<sub>1</sub>, ..., a<sub>d</sub>&nbsp;be constants, in which a<sub>d</sub>&nbsp;&gt; 0. Let p(n) = Σ<sub>c = 0, 1, ..., d</sub>&nbsp;(a<sub>i&nbsp;</sub>n<sup>i</sup>). Then p(n) = Θ(n<sup>d</sup>).</span></p>
<p><span style="font-size: 19px;">Proof. To prove this, we compute</span></p>
<p><span style="font-size: 19px;">lim<sub>n → ∞</sub>&nbsp;p(n)/n<sup>d</sup>&nbsp;= lim<sub>n → ∞</sub>&nbsp;(a<sub>0</sub>/n<sup>d</sup>&nbsp;+ a<sub>1</sub>/n<sup>d-1</sup>&nbsp;+ ... + a<sub>d-1</sub>/n + a<sub>d</sub>) = a<sub>d</sub>.</span></p>
<p><span style="font-size: 19px;">Since a<sub>d</sub>&nbsp;is a positive constant, we claim that p(n) = Θ(n<sup>d</sup>). [end of proof]</span></p>
<p><span style="font-size: 19px;"><strong>8. Useful rules</strong></span></p>
<p><span style="font-size: 19px;">There are some useful theorems for limits. Here I present them in a form based on the particular task of comparing two functions of complexity (running time, space, etc).</span></p>
<p><span style="font-size: 19px;">First, L'Hopital's rule:</span></p>
<pre><span style="font-size: 19px;">lim  (f(n)/g(n)) = lim  (f'(n)/g'(n))
n→∞              n→∞
</span></pre>
<p><span style="font-size: 19px;">For example,</span></p>
<pre><span style="font-size: 19px;">lim (ln n / n) = lim ((1/n) / 1) = 0
n→∞             n→∞</span></pre>
<p><span style="font-size: 19px;">Thus ln n = o(n).</span></p>
<p><span style="font-size: 19px;">A more complicated example: Find out the relationship between f(n) = n<sup>n</sup>&nbsp;and g(n) = n!</span></p>
<p><span style="font-size: 19px;">Solution: By Stirling's approximation</span></p>
<p><img src="Pictures/math1.png" alt="math" title="math" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></p>
<p><span style="font-size: 19px;">we have</span></p>
<p><img src="Pictures/factorial.jpg" alt="factorial" title="factorial" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></p>
<p><span style="font-size: 19px;">Therefore, f(n) = ω(g(n)).</span></p>
<p><span style="font-size: 19px;">&nbsp;</span></p>
<p><span style="font-size: 19px;">Two other useful theorems:</span></p>
<p><span style="font-size: 19px;">1. If f(n) &lt;= g(n) for all n &gt; 0, then</span></p>
<pre><span style="font-size: 19px;">lim  f(n) &lt;= lim g(n)
n→∞         n→∞
</span></pre>
<p><span style="font-size: 19px;">2. Squeeze Theorem: If h(n) &lt;= f(n) &lt;= g(n) for all n &gt; 0, and</span></p>
<pre><span style="font-size: 19px;">lim  h(n) = lim g(n)
n→∞        n→∞
</span></pre>
<p><span style="font-size: 19px;">then</span></p>
<pre><span style="font-size: 19px;">lim  f(n) = lim g(n)
n→∞        n→∞
</span></pre>
<p><br><span style="font-size: 19px;">Using the squeeze theorem, we have an alternative, simpler approach that can prove n! = o(n<sup>n</sup>): Since</span></p>
<p><span style="font-size: 19px;">0 &lt;= n! / n<sup>n</sup>&nbsp;= (1/n) (2/n) ... (n/n) &lt;= 1/n</span></p>
<p><span style="font-size: 19px;">lim<sub>n → ∞</sub>0 = lim<sub>n → ∞</sub>(1/n) = 0</span></p>
<p><span style="font-size: 19px;">We have</span></p>
<p><span style="font-size: 19px;">lim<sub>n → ∞</sub>(n! / n<sup>n</sup>) = 0</span></p>

<p><span style="font-size: 19px;">Finally, we can use the following property to break more complicated functions into simpler parts:</span></p>
<p><span style="font-size: 19px;">If f<sub>1</sub>&nbsp;(n) = O(g<sub>1</sub>(n)) and f<sub>2</sub>&nbsp;(n) = O(g<sub>2</sub>(n)), then</span></p>
<p><span style="font-size: 19px;">f<sub>1</sub>&nbsp;(n) + f<sub>2</sub>&nbsp;(n) = O(g<sub>1</sub>(n) + g<sub>2</sub>(n))</span></p>
<p><span style="font-size: 19px;">f<sub>1</sub>&nbsp;(n) f<sub>2</sub>&nbsp;(n) = O(g<sub>1</sub>(n) g<sub>2</sub>(n))</span></p>
<p><span style="font-size: 19px;">Let us use this property to show the relationship between (n<sup>3</sup>) (lg n)<sup>3</sup>&nbsp;(lg lg n) and (n<sup>4</sup>) (lg n). First we have lg lg n = o(lg n). Thus, (lg n)<sup>3</sup>&nbsp;(lg lg n) = o(lg<sup>4</sup>&nbsp;n) = o(n). Therefore, n<sup>3</sup>&nbsp;(lg n)<sup>3</sup>&nbsp;(lg lg n) = n<sup>3</sup>&nbsp;(o(lg<sup>4</sup>&nbsp;n)) = o(n<sup>4</sup>) = o(n<sup>4</sup>&nbsp;lg n).</span></p>
<p><span style="font-size: 19px;"></span></p>

<h2><span style="font-size: 30px;color: rgb(250, 8, 8)">Chapter4</span></h2>
<h2><span style="font-size: 24px;">Algorithm Design and Maximum Subrange Sum</span></h2>
<p><span style="font-size: 19px;">I will now start talking about algorithm design. Before discussing each algorithm design paradigm, I will show different algorithmic solutions to the maximum subrange sum problem, to show good algorithm design is essential.</span></p>
<h3><span style="font-size: 19px;">1. Maximum Subrange Sum</span></h3>
<p><span style="font-size: 19px;">In this problem, the input is an array x of n possibly negative integers. The output of the algorithm for this problem is the maximum sum found in any subarray (possibly empty) of the input.</span></p>
<p><span style="font-size: 19px;">In the following example, n = 10 and the content of x is: 31, -41, 59, 26, -53, 58, 97, -93, -23, 84. The output is the sum of the subarray x[3..7], which is 187.</span></p>
<p><span style="font-size: 19px;">To understand this problem better, let us consider some special cases. First, if all the entries are positive, then the subarray with maximum sum is the entire array x. Second, if all the entries are negative, then the answer is 0, which corresponds to an empty subarray.</span></p>
<p><span style="font-size: 19px;">I described in class that this abstract problem is from practice.</span></p>
<h3><span style="font-size: 19px;">Solution 1 (Brute-force)</span></h3>
<p><span style="font-size: 19px;">In a brute-force solution to this problem we enumerate all possible subarrays, compute the sum of each of them and return the maximum.</span></p>
<p><span style="font-size: 19px;">In the pseudocode below, make sure to distinguish the letter l and the digit 1:</span></p>
<pre><span style="font-size: 19px;">maxsubrangesum1(x[1..n])</span><br><span style="font-size: 19px;">  max ← 0
  for l ← 1 to n do
    for u ← l to n do</span><br><span style="font-size: 19px;">      sum ← 0</span><br><span style="font-size: 19px;">      for i ← l to u do
        sum ← sum + x[i]
        if sum &gt; max then
          max ← sum
  return max</span></pre>
<p><span style="font-size: 19px;">To analyze the running time, we need find out the number of times the statement in the innermost loop is executed. An upper bound would be n<sup>3</sup>. With this it is not difficult to show that the running time is O(n<sup>3</sup>). With greater care, we can show the running time is Θ(n<sup>3</sup>).</span></p>
<h3><span style="font-size: 19px;">Solution 2</span></h3>
<p><span style="font-size: 19px;">To improve the running time, we observe that in the previous solution, we perform a lot of recomputation in the innermost loop. For the example given before, to computer the sum of the subarray x[3..5], we do a loop to get the result 32. Then, in the next iteration, we compute the sum of x[3..6], again using a loop. We can however easily compute x[3..6] as x[3..5] + x[6].</span></p>
<p><span style="font-size: 19px;">We can formalize this observation using the following equation: sum of x[l..u] = sum of x[l..u-1] + x[u].</span></p>
<pre><span style="font-size: 19px;">maxsubrangesum2(x[1..n])</span><br><span style="font-size: 19px;">  max ← 0
  for l ← 1 to n do
    sum ← 0
    for u ← l to n do</span><br><span style="font-size: 19px;">      sum ← sum + x[u]
      if sum &gt; max then
        max ← sum
  return max</span></pre>
<p><span style="font-size: 19px;">The running time is Θ(n<sup>2</sup>).</span></p>
<h3><span style="font-size: 19px;">Solution 3: Another Θ(n<sup>2</sup>)-Time Solution</span></h3>
<p><span style="font-size: 19px;">We describe another Θ(n<sup>2</sup>)-Time Solution, not because it is more efficient than solution 2, but because it makes use of a standard trick which is good to know.</span></p>
<p><span style="font-size: 19px;">In this solution, we first precompute the prefix sum array p[0..n], in which p[i] = x[1] + x[2] + ... + x[i], and p[0] = 0 (sum of the empty prefix). We can perform one loop to compute all the entries of p in O(n) time. With this, the sum of x[l..u] can be computed using x[l..u] = p[u] - p[l-1].</span></p>
<pre><span style="font-size: 19px;">maxsubrangesum3(x[1..n])
  p[0] ← 0
  for i ← 1 to n do
    p[i] ← p[i-1] + x[i]
</span><br><span style="font-size: 19px;">  max ← 0
  for l ← 1 to n do
    for u ← l to n do</span><br><span style="font-size: 19px;">      sum ← p[u] - p[l-1]
      if sum &gt; max then
        max ← sum
  return max</span></pre>
<h3><span style="font-size: 19px;">Solution 4: Applying an Algorithmic Paradigm: Divide-and-Conquer</span></h3>
<p><span style="font-size: 19px;">We learned divide-and-conquer when we learned mergesort in a previous course. Now let's apply this idea here.</span></p>
<p><span style="font-size: 19px;">Our algorithm breaks x into two halves. We can then find the maximum subrange sum in each half in recursive manner. After this, we still need consider subranges that straddle the midpoint. A useful observation is that among these ranges, the one with the maximum sum consists of a maximum-sum suffix of the left half of x, and a maximum-sum prefix of the right half of x. We can construct a proof by contradiction for this.</span></p>
<p><span style="font-size: 19px;">This gives us the following solution; note that we first need a recursive algorithm which solves this problem for array x[l..u].</span></p>
<pre><span style="font-size: 19px;">maxsubrangesum4(x, l, u)
  if l &gt; u then
    return 0
  if l = u then
    return max(0, x[l])

  m ← ⌊(l+u)/2⌋   // midpoint

  suml ← 0  // keep track of the sum of the suffixes of x[l..m]
  maxleft ← 0  
  for i ← m downto l do
    suml ← suml + x[i]
    maxleft ← max(maxleft, suml)

  sumr ← 0  // keep track of the sum of the prefixes of x[m..u]
  maxlright ← 0
  for i ← m+1 to u do
    sumr ← sumr + x[i]
    maxright ← max(maxright, sumr)

  maxa ← maxsubrangesum4(x, l, m)
  maxb ← maxsubrangesum4(x, m+1, u)

  return max(maxa, maxb, maxleft+maxright)</span></pre>
<p><span style="font-size: 19px;">To use this recursive algorithm to solve the maximum subrange sum problem, call</span></p>
<pre><span style="font-size: 19px;">maxsubrangesum4(x, l, n)<br></span></pre>
<p><span style="font-size: 19px;">Let T(n) be the running time. Then we can express T(n) using the following recurrence:</span></p>
<p><span style="font-size: 19px;">T(n) = T(⌈n/2⌉) + T(⌊n/2⌋) + n if n &gt; 1</span><br><span style="font-size: 19px;">T(n) = 1 if n = 1</span></p>
<p><span style="font-size: 19px;">To solve it, </span><span style="font-size: 19px;">we can assume n is a power of 2, which guarantees that the left and right half arrays are of the same size. This is a typical and reasonable assumption. Then, it is not difficult to see the running time satisfies T(n) = 1 if n = 1 and T(n) = 2 T(n/2) + n if n &gt; 1. This recurrence is the same as the recurrence for the running time of mergesort, so the running time of our algorithm is also O(n lg n) (later we will learn how to analyze running time given recurrences).</span></p>


<h3><span style="font-size: 19px;">Solution 5</span></h3>
<p><span style="font-size: 19px;">Suppose that we have found the maximum subrange sum for x[1..j-1]. To find it for x[1..j], there are two cases.</span></p>
<p><span style="font-size: 19px;">In the first case, the subrange with the maximum sum lies entirely within x[1..j-1]. In this case, we already know the answer.</span></p>
<p><span style="font-size: 19px;">In the second case, this subrange ends at x[j]. This means, for this case, we have to find the maximum suffix sum of x[1..j]. If we already know the maximum suffix sum of x[1..j-1], we can add x[j] to that sum. If the result is positive, then it is the maximum suffix sum of x[1..j]. Otherwise, we take 0 as the maximum suffix sum of x[1..j], which corresponds to the empty suffix.</span></p>
<p><span style="font-size: 19px;">The above reasoning gives us a solution that is based on a linear scan of the array, keeping track of the maximum subrange sum and maximum suffix sum in the portion of the array seen so far.</span></p>
<pre><span style="font-size: 19px;">masubrangesum5(x[1..n])</span><br><span style="font-size: 19px;">  maxsofar ← 0 //keep track of the maximum subrange sum in the portion of the array seen so far
  maxsuffixsum ← 0 //keep track of the maximum suffix sum in the portion of the array seen so far
</span><br><span style="font-size: 19px;">  for i ← 1 to n do
    maxsuffixsum ← max(maxsuffixsum + x[i], 0)</span><br><span style="font-size: 19px;">    maxsofar ← max(maxsofar, maxsuffixsum)</span><br><br><span style="font-size: 19px;">  return maxsofar</span></pre>
<p><span style="font-size: 19px;">This example requires only O(n) time.</span></p>
<br>
<p><span style="font-size: 21px;">From these five solutions, we can see that good algorithm design techniques are essential. The following are eight algorithm design paradigms</span></p>
<br>
<h3><span style="font-size: 25px; color: red;"><strong>Algorithm Design Paradigms</strong></span></h3>
<ol>
<li><span style="font-size: 21px;">reduce to know problem</span></li>
<li><span style="font-size: 21px;">recursion</span></li>
<li><span style="font-size: 21px;">divide-and-conquer</span></li>
<li><span style="font-size: 21px;">invent (or augment) a data structure</span></li>
<li><span style="font-size: 21px;">greedy algorithm</span></li>
<li><span style="font-size: 21px;">dynamic programming</span></li>
<li><span style="font-size: 21px;">exploit problem structure (algebraic, geometric, etc.)</span></li>
<li><span style="font-size: 21px;">probabilistic and randomized solution</span></li>
</ol>
<p><span style="font-size: 19px;">&nbsp;</span></p>
<h2><span style="font-size: 24px; color: red;">1-Reduce to Known Problem</span></h2>
<p><span style="font-size: 19px;"> The first algorithm design paradigm that we will discuss is "reduce to known problem". This is to develop an algorithm for a problem by treating this problem as a special case of a problem that we already know how to solve efficiently. For most of the examples that we will see here, we reduce the problems to sorting, which we know how to solve in O(n lg n) time (e.g., mergesort, heapsort).</span></p>
<p><span style="font-size: 19px;">Let's apply this paradigm to solve some problems.</span></p>
<h3><span style="font-size: 19px;">1. Element Distinctness (Uniqueness) Problem</span></h3>
<p><span style="font-size: 19px;">This problem asks us to determine whether the elements in a given array of n numbers are distinct.</span></p>
<p><span style="font-size: 19px;">Solution 1: A brute-force solution is to compare each element to every other element, and this solution requires a double loop. There are n choose 2 pairs of elements to compare. The running time is O(n<sup>2</sup>).</span></p>
<p><span style="font-size: 19px;">Solution 2: Sort the n numbers, and then we walk through the sorted array, comparing each element with the one immediately to its right. If we find any element that is equal to the element immediately to its right, then we have found a repeated element. If not, then the numbers are distinct. Since sorting requires O(n lg n) time and the computation after sorting requires O(n) time, this solution uses O(n lg n) time.</span></p>
<p><span style="font-size: 19px;">It is worth mentioning that the lower bound of the element distinctness problem is Ω(n) in the comparison-based model. What does this mean? To understand this, let's first learn what a comparison-based model is, and then what is the lower bound of a problem. Under the comparison-based model, algorithms can only perform arithmetic calculations and comparisons. They are forbidden to use the value of the input as addresses of memory cells. The above claim about lower bound means that any algorithm that solves this problem in the comparison-based model requires Ω(n lg n) time in the worst case. How to prove this is out of the scope of an introductory course to algorithms, but it is worth knowing the result.</span></p>
<p><span style="font-size: 19px;">Solution 4: This solution is for a special case in which we know that the input values are relatively small integers between 1 and M. In this case, we can make an array B[1..M], and initialize all its entries to 0. Then we scan x, writing a 1 in B[i] if we encounter i during the scan. Before writing, we check if B[i] has a 1 in it already, and if it does, then we have found a repeated element.</span></p>
<p><span style="font-size: 19px;">This requires O(M+n) time and O(M+n) space, and it is good if the numbers are small, e.g., M = O(n).</span></p>
<p><span style="font-size: 19px;">Exercise: how to modify our approach so that we need not initialize the entries of B to 0?</span></p>
<h3><span style="font-size: 19px;">2. Set Intersection</span></h3>
<p><span style="font-size: 19px;">In this problem, we are given two sets of integers (represented as two arrays), and each set is of size n, and we compute the intersection of these two sets.</span></p>
<p><span style="font-size: 19px;">Solution 1: A naive approach is to loop through one array, checking to see if any of its elements is contained in the other. Again, this requires O(n<sup>2</sup>) time.</span></p>
<p><span style="font-size: 19px;">Solution 2: Sort the first array. Then we loop through the second array, and for each element in the second array, we find out whether it exists in the first array using binary search. This requires O(n lg n) time.</span></p>
<h3><span style="font-size: 19px;">3. Collinear Points</span></h3>
<p><span style="font-size: 19px;">In this problem, we are given n points in the plane, and we are to find out whether there are three points that are collinear, i.e., lie along the same line.</span></p>
<p><span style="font-size: 19px;">Solution 1: For each triple of points, say P<sub>1</sub>&nbsp;= (x<sub>1</sub>, y<sub>1</sub>), P<sub>2</sub>&nbsp;= (x<sub>2</sub>, y<sub>2</sub>) and P<sub>3</sub>&nbsp;= (x<sub>3</sub>, y<sub>3</sub>), compute the slopes of the line connection P<sub>1</sub>&nbsp;with P<sub>2</sub>, and the line connecting P<sub>1</sub>&nbsp;with P<sub>3</sub>. If they are equal, then these three points are collinear. We can easily write a loop of three levels to check each triple exactly once. There are n choose 3 triples, and this require Θ(n<sup>3</sup>) time.</span></p>
<p><span style="font-size: 19px;">Solution 2: For each point P, compute the slope of each line defined by P and another point in the given set of points. If there is a duplicate element among all these slopes, then there are collinear points. This way we have reduced the problem of finding whether there is a triple of collinear points involving P to the element distinctness problem. Thus this algorithm runs in O(n<sup>2</sup>&nbsp;lg n) time.</span></p>
<p><span style="font-size: 19px;">Note that no one knows whether the problem of collinear points can be solved in O(n<sup>2</sup>) time, and thus our second solution is quite efficient.</span></p>



<h3><span style="font-size: 19px;">4. Convex Hull</span></h3>
<p><span style="font-size: 19px;">In this problem, we have a set, Q, of points in the plane, given by the x- and y-coordinates of these points. We would like to compute the convex hull of this point set, i.e., the smallest convex polygon P for which each point in Q is either on the boundary of P or in its interior.</span></p>
<p><span style="font-size: 19px;">Since the points in Q could possibly lie along the boundary of P, it is not difficult to see that all the vertices of P are in Q.</span></p>
<p><span style="font-size: 19px;">One way of "computing" the convex hull is to bang nails into a board at the positions where the points are located, put an elastic rubber band around all the nails, and then let go. The law of physics will do the computation and define the convex hull using the rubber band.</span></p>
<p><span style="font-size: 19px;">This problem has many applications in computer graphics and CAD. Many programs compute the convex hull of a complicated shape, and use it to determine whether the user's mouse clicked on it.</span></p>
<p><span style="font-size: 19px;">To study the solutions to convex hull, let's first learn some background knowledge in geometry.</span></p>
<p><span style="font-size: 19px;">Polar angle: The polar angel of a point p with respect to the origin point p<sub>0</sub>&nbsp;is the angle from the x-axis through the origin to the point p<sub>0</sub>, rotating counter clockwise.</span></p>
<p><span style="font-size: 19px;"><img src="Pictures/polar.jpg" alt="polar angle" title="polar angle" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span></p>
<p><span style="font-size: 19px;">In the above example, θ is the polar angle of p with respect to p<sub>0</sub>.</span></p>
<p><span style="font-size: 19px;">Compute the angle between two directed line segments: Let u and v be the vectors representing two directed line segments whose starting endpoints are the the origin point p<sub>0</sub>. How do we compute the angel, θ, between u and v?</span></p>
<p><span style="font-size: 19px;"><img src="Pictures/angle.jpg" alt="angle" title="angle" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span></p>
<p><span style="font-size: 19px;">We can use the following equation to compute θ: u⋅v = ||u|| ||v|| cos θ. Here the dot is the dot product of two vectors. Say, u = (x<sub>1</sub>, y<sub>1</sub>) and v = (x<sub>2</sub>, y<sub>2</sub>), then u⋅v = x<sub>1</sub>x<sub>2</sub>&nbsp;+ y<sub>1</sub>y<sub>2</sub>, and ||u|| is the length of u.</span></p>
<p><span style="font-size: 19px;">Note that we cannot assume arc cos or square root to be a constant-time operation supported by RAM. However, in many algorithms, we just have to compare two angles without computing their actual degrees. Thus, since this equation allows us to compute (cos θ)<sup>2</sup>&nbsp;in constant time, we can make use of it to compare two angles in constant time.</span></p>
<p><span style="font-size: 19px;">Before introducing our algorithms for convex hull, let's make a simplifying assumption that no three points in the input point set are colinear. This is a common assumption; it is not difficult to modify our algorithms to handle the special cases in which there are colinear points (and yes, these special cases should be taken care of in actual implementation), and we make this assumption so that we can focus on explaining algorithmic ideas.</span></p>
<p><span style="font-size: 19px;">A naive approach is to consider the line joined by each pair of points, p<sub>1</sub>&nbsp;and p<sub>2</sub>. Then, if all the other points are on the same side of this line, then the line segment between p<sub>1</sub>&nbsp;and p<sub>2</sub>&nbsp;is an edge of the convex hull.</span></p>
<p><span style="font-size: 19px;">How do we test if all the other points are on the same side of this line? One way of doing this is to first determine the equation of the line, say, Ax+By+C = 0. Then, substitute the coordinates of each of the other points into the left hand side. If the signs of the results are the same, then all the other points are on the same side.</span></p>
<p><span style="font-size: 19px;">It is not difficult to see the correctness of the algorithm, but the running time is O(n<sup>3</sup>). Can we do better?</span></p>
<p><span style="font-size: 19px;"><strong>5. Jarvis's March</strong>&nbsp;(Gift Wrapping): A better algorithm of computing convex hull is Jarvis' march. It is also called gift wrapping as its computational process is similar to the process of wrapping a gift: start from an extreme point and wrap it tightly using a piece of paper.</span></p>
<p><span style="font-size: 19px;">The algorithm is</span></p>
<ol>
<li><span style="font-size: 19px;">Start with p<sub>0</sub>, the point with the minimum y-coordinate, breaking ties arbitrarily. p<sub>0</sub>&nbsp;must be on CH(Q), the convex hull of the input point set Q.</span></li>
<li><span style="font-size: 19px;">The next vertex p<sub>1</sub>&nbsp;on CH(Q) has the smallest polar angel with respect to p<sub>0</sub>, and thus we can find it in linear time. To find the point, p<sub>2</sub>, after p<sub>1</sub>&nbsp;in CH(Q), we further find the point with the smallest polar angel with respect to p<sub>1</sub>, and so on.</span></li>
<li><span style="font-size: 19px;">When we reach the highest point p<sub>k</sub>, we have constructed the right chain of CH(Q).</span></li>
<li><span style="font-size: 19px;">The left chain can be computed in a symmetric way. We start from p<sub>k</sub>&nbsp;and choose p<sub>k+1</sub>&nbsp;as the point with the smallest polar angel with respect to p<sub>k</sub>, but from the negative x-axis, and so on.</span></li>
</ol>
<p><span style="font-size: 19px;"><img src="Pictures/jarvis.jpg" alt="Jarvis's March" title="Jarvis's March" width="799" height="479"></span></p>
<p><span style="font-size: 19px;">If the convex hull has h vertices, then the running time of this algorithm is O(nh), since it uses O(n) time to "march" from one vertex of CH(Q) to the next and there are h vertices. This is O(n<sup>2</sup>) in the worst case.</span></p>
<p><span style="font-size: 19px;"><strong>6. Graham's Scan</strong>: During Jarvis's march, we repeatedly look for the smallest polar angles all the way, so can we just reduce the problem into sorting by polar angle? It is not as simple as this, since we keep changing origin points. The Graham's scan algorithm still manages to reduce it to sorting, in a subtle way. It has the following steps:</span></p>
<ol>
<li><span style="font-size: 19px;">Locate p<sub>0</sub>, the lowest point in Q.</span></li>
<li><span style="font-size: 19px;">Sort all the other points by their polar angle with respect to p<sub>0</sub>. Let p<sub>1</sub>, p<sub>2</sub>, ..., p<sub>n-1</sub>&nbsp;denote the result of sorting.</span></li>
<li><span style="font-size: 19px;">Process the points starting with the one with the smallest polar angle. Add each point in turn with the next higher angle. If doing so causes us to make a left turn, continue. Otherwise, we made a right turn, and the point in the middle of the turn is not in CH(Q). Discard it and join the previous point to the new point. Backtrack, discarding points as you go until you end up making a left turn again.</span></li>
</ol>
<p><span style="font-size: 19px;"><img src="Pictures/graham.jpg" alt="Graham's Scan" title="Graham's Scan" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span></p>
<p><span style="font-size: 19px;">The pseudocode for Step 3 is</span></p>
<pre><span style="font-size: 19px;">s ← an empty stack
PUSH(p<sub>0</sub>, S)
PUSH(p<sub>1</sub>, S)
PUSH(p<sub>2</sub>, S)
for i ← 3 to n-1 do
  while the angle formed by points NEXT-TO-TOP(S), TOP(S), and p<sub>i</sub> makes a nonleft turn then
    POP(S)
  PUSH(p<sub>i</sub>, S)
return S<br><br></span></pre>
<p><span style="font-size: 19px;">In the next lecture, I will talk more about this piece of pseudocode and then analyze the running time.</span></p>
<p>&nbsp;</p>

<p><span style="font-size: 19px;">To analyze the running time of Graham's scan, it is easy to see that Step 1 requires O(n) time and Step 2 requires O(n lg n) time. Step 3 is not obvious. In Step 3, all the lines excluding the while loop requires O(n) time in total. To bound the cost of the while loop, it suffices to find out how many times the POP operation is executed. To find this out, observe that each point is visited at most twice in the pseudocode for Step 3: once when we add it to CH(Q) using PUSH, and possibly the second time when we discard it using POP. Thus the number of times that POP is executed is at most the number of points. Therefore, Step 3 requires O(n) time. The overall running time of Graham's scan is thus O(n lg n).</span></p>
<h1><span style="font-size: 24px; color: red;">3-Divide-and-Conquer</span></h1>
<p><span style="font-size: 19px;">We learned divide-and-conquer in previous courses, and we will learn more about it in this course. The general steps of divide-and-conquer are</span></p>
<ul>
<li><span style="font-size: 19px;">DIVIDE the given problem into a number of subproblems</span></li>
<li><span style="font-size: 19px;">CONQUER the subproblems by solving them recursively</span></li>
<li><span style="font-size: 19px;">COMBINE the solutions to the subproblems into the solution to the original problem</span></li>
</ul>
<p><span style="font-size: 19px;">In this course, we have already seen an example: the 4th solution to the maximum subarray problem.&nbsp;Let T(n) be the running time of that algorithm. Then we can express T(n) using the following recursive formula:</span></p>
<p><span style="font-size: 19px;">T(n) = T(⌈n/2⌉) + T(⌊n/2⌋) + n if n &gt; 1</span><br><span style="font-size: 19px;">T(n) = 1 if n = 1</span></p>
<p><span style="font-size: 19px;">To analyze the running time, assume, for simplicity, that n is a power of 2 (I will explain why this is a reasonable assumption). Then T(n) = 2T(n/2) + n if n &gt; 1.</span></p>
<p><span style="font-size: 19px;">The first approach that I will use to solve the recurrence is called iteration. The idea is to plug the right-hand side of the equation repeatedly into the left-hand side.</span></p>
<p><span style="font-size: 19px;">Assume that n = 2<sup>k</sup>. Then we have</span></p>
<p><span style="font-size: 19px;">T(2<sup>k</sup>) = 2T(2<sup>k-1</sup>) + 2<sup>k</sup></span><br><span style="font-size: 19px;">2T(2<sup>k-1</sup>) = 4T(2<sup>k-2</sup>) + 2<sup>k</sup></span><br><span style="font-size: 19px;">...</span><br><span style="font-size: 19px;">2<sup>k-1</sup>T(2) = 2<sup>k</sup>T(1) + 2<sup>k</sup></span></p>
<p><span style="font-size: 19px;">After summing all these equations, we have T(2<sup>k</sup>) = 2<sup>k</sup>&nbsp;+ k 2<sup>k</sup>&nbsp;= 2<sup>k</sup>(k+1)</span></p>
<p><span style="font-size: 19px;">Therefore, T(n) = n(lg n + 1) = O(n lg n).</span></p>
<p><span style="font-size: 19px;">Now, let's discuss why it is OK, in the analysis of the running time of mergesort, to make the simplifying assumption that n is a power of 2. Even if n is not necessarily a power of 2, we still have the following equation:</span></p>
<p><span style="font-size: 19px;">T(n) = T(2<sup>lg n</sup>) &lt;= T(2<sup>⌈lg n⌉</sup>) = 2<sup>⌈lg n⌉</sup>(⌈lg n⌉ + 1) &lt; 2<sup>lg n + 1</sup>&nbsp;(lg n + 1 + 1) = 2n (lg n + 2) = O(n lg n)</span></p>
<p><span style="font-size: 19px;">Therefore, in such recurrences, we normally ignore floor and ceiling when we solve them to compute the running time.</span></p>
<p><span style="font-size: 19px;">Next, let's using a different approach, namely the substitution method, to analyze running time. Study the following table</span></p>
<pre><span style="font-size: 19px;">n      1    2    4    8   16   32
T(n)   1    4   12   32   80  192
T(n)/n 1    2    3    4    5    6</span></pre>
<p><span style="font-size: 19px;">We can guess that T(n)/n = k + 1, where n = 2<sup>k</sup>.</span></p>
<p><span style="font-size: 19px;">Thus, our guess is T(2<sup>k</sup>) = (k+1)2<sup>k</sup>.</span></p>
<p><span style="font-size: 19px;">Now let's prove it by induction. Clearly it is true for k = 0. Assume it is true for k; we will prove it for k+1. We have</span></p>
<p><span style="font-size: 19px;">T(2<sup>k+1</sup>) = 2T(2<sup>k</sup>) + 2<sup>k+1</sup>&nbsp;= 2(k+1)2<sup>k</sup>&nbsp;+ 2<sup>k+1</sup>&nbsp;= (k+2)2<sup>k+1</sup></span></p>
<h3><span style="font-size: 19px;">&nbsp;</span></h3>
<h3><span style="font-size: 19px;">1. Multiply two n-bit nonnegative numbers</span></h3>
<p><span style="font-size: 19px;">In this problem, we are given two n-bit numbers, and we are to compute the product of these two. Naturally, this makes sense if n is so large that these numbers do not fit in a word. Thus, an array of n entries are used to store the n bits in each of these numbers.</span></p>
<p><span style="font-size: 19px;">If we design an algorithm based on the pencil-and-paper method, it would require O(n<sup>2</sup>) time. Now let's see if we can make any improvement using divide-and-conquer.</span></p>
<p><span style="font-size: 19px;">Let X and Y be the two input n-bit numbers. Assume that the n bits representing X is the concatenation of A and B, and the n bits representing Y is the concatenation of C and D, where A, B, C and D are n/2-bit numbers. If we are to write down pseudocode, we can use arrays of 0/1 digits to represent X, Y, A, B, C and D.</span></p>
<p><span style="font-size: 19px;"><strong>Solution 1</strong>: This solution is based on the following equation:</span></p>
<p><span style="font-size: 19px;">XY = (A 2<sup>n/2</sup>&nbsp;+ B) (C 2<sup>n/2</sup>&nbsp;+ D) = (AC) 2<sup>n</sup>&nbsp;+ (AD + BC) 2<sup>n/2</sup>&nbsp;+ BD</span></p>
<p><span style="font-size: 19px;">This immediately suggests the following idea of doing divide-and-conquer: First, perform four multiplications to compute AC, AD, BC, and BD. That is, solve 4 subproblems of size n/2. Then, use the solutions to these subproblems to compute XY; to do this, we simply perform additions and multiplications by 2<sup>n</sup>&nbsp;(or 2<sup>n/2</sup>), and these multiplications can be performed by appending 0 bits. Thus, we have the following recurrence on running times:</span></p>
<p><span style="font-size: 19px;">T(n) = 4T(n/2) + cn if n &gt; 1, and T(n) = c if n = 1. c is a constant here.</span></p>
<p><span style="font-size: 19px;">We use the iteration approach again.</span></p>
<p><span style="font-size: 19px;">T(2<sup>k</sup>) = 4T(2<sup>k-1</sup>) + c 2<sup>k</sup></span><br><span style="font-size: 19px;">4T(2<sup>k-1</sup>) = 16T(2<sup>k-2</sup>) + 4 c 2<sup>k-1</sup></span><br><span style="font-size: 19px;">...</span><br><span style="font-size: 19px;">4<sup>k-1</sup>T(2) = 4<sup>k</sup>T(1) + 4<sup>k-1</sup>&nbsp;c 2</span></p>
<p><span style="font-size: 19px;">When we sum up, we get T(2<sup>k</sup>) = c 4<sup>k</sup>&nbsp;+ c (2<sup>k</sup>&nbsp;+ 2<sup>k+1</sup>&nbsp;+ ... + 2<sup>2k-1</sup>) = c 4<sup>k</sup>&nbsp;+ c (4<sup>k</sup>&nbsp;- 2<sup>k</sup>)</span></p>
<p><span style="font-size: 19px;">Thus T(n) = (c + c) n<sup>2</sup>&nbsp;- cn = O(n<sup>2</sup>).</span></p>
<p><span style="font-size: 19px;">Thus this solution does not achieve any improvement compared with the pencil-and-paper solution.</span></p>
<p><span style="font-size: 19px;"><strong>Solution 2: Karatsuba's algorithm</strong></span></p>
<p><span style="font-size: 19px;">The second solution is based on the following equation</span></p>
<p><span style="font-size: 19px;">XY = (2<sup>n</sup>&nbsp;+ 2<sup>n/2</sup>) AC + 2<sup>n/2</sup>&nbsp;(A-B)(D-C) + (2<sup>n/2</sup>&nbsp;+ 1) BD</span></p>
<p><span style="font-size: 19px;">You can easily verify the correctness of this equation by expanding it.</span></p>
<p><span style="font-size: 19px;">If we design a divide-and-conquer approach based on this equation, we just have to solve 3 subproblems: the computation of AC, (A-B)(D-C) and BD. Thus the running time is</span></p>
<p><span style="font-size: 19px;">T(n) = 3T(n/2) + cn if n &gt; 1</span><br><span style="font-size: 19px;">T(n) = c if n = 1</span></p>
<p><span style="font-size: 19px;">We can prove this by induction (left as an exercise): T(2<sup>k</sup>) &lt;= c(3<sup>k+1</sup>&nbsp;- 2<sup>k+1</sup>).&nbsp;</span></p>



<p><span style="font-size: 19px;">First I talked about the <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=230447&amp;type=content&amp;rcode=dalhousie-2509488" target="_self">midterm exam</a>.</span></p>
<p><span style="font-size: 19px;">Then, we have</span></p>
<p><span style="font-size: 19px;">T(n) &lt;= c(3<sup>lg n + 1</sup>&nbsp;- 2<sup>lg n +1</sup>) = c(3⋅3<sup>lg n</sup>&nbsp;- 2⋅2<sup>lg n</sup>) = c(3 n<sup>lg 3</sup>&nbsp;- 2n) = O(n<sup>lg 3</sup>).</span></p>
<p><span style="font-size: 19px;">Since lg 3 = 1.58496...., we have T(n) = O(n<sup>1.59</sup>)</span></p>
<p><span style="font-size: 19px;">Some experimental studies showed that Karatsuba's algorithm outperforms the pencil-and-paper algorithm when n exceeds a certain threshold between 320 and 640. Thus it is good for very large integers, which are often used in math, stats, cryptography and astronomy.</span></p>
<br>
<h3><span style="font-size: 24px;color: red;">Master Theorem</span></h3>
<p><span style="font-size: 19px;">So far we have learned a few methods to solve recurrences, including iteration and substitution. Now let's learn a general theorem that will cover many cases of recurrences.</span></p>
<p><span style="font-size: 19px;">Master theorem can be stated as follows:</span></p>
<p><span style="font-size: 19px;">Let a &gt;=1 and b &gt; 1 be constants. Let f(n) be a function. Let T(n) be defined on nonnegative integers by the recurrence T(n) = a T(n/b) + f(n), where we interpret n/b to mean either ⌊n/b⌋ or ⌈n/b⌉. Then</span></p>
<ol>
<li><span style="font-size: 19px;">If f(n) = O(n<sup>log<sub>b</sub>a-ε</sup>) for some constant ε &gt; 0, then T(n) = Θ(n<sup>log<sub>b</sub>a</sup>).</span></li>
<li><span style="font-size: 19px;">If f(n) = Θ(n<sup>log<sub>b</sub>a</sup>lg<sup>k </sup>n) for some constant k &gt;= 0, then T(n) = Θ(n<sup>log<sub>b</sub>a</sup>&nbsp;lg<sup>k+1</sup>n).</span></li>
<li><span style="font-size: 19px;">If f(n) = Ω(n<sup>log<sub>b</sub>a+ε</sup>) for some constant ε &gt; 0, and (regularity condition) if a f(n/b) &lt;= c f(n) for some constant c &lt; 1 and all sufficiently large n, then T(n) = Θ(f(n)).</span></li>
</ol>
<p><span style="font-size: 19px;">Memorize this. Note that there are gaps between different cases, so this theorem does not address all possible cases of recursion. Let's study some examples.</span></p>
<p><span style="font-size: 19px;">1) Karatsuba's algorithm: T(n) = 3T(n/2) + cn. For this a = 3, b = 2, f(n) = cn, and n<sup>log<sub>b</sub>a</sup>&nbsp;= n<sup>lg 3</sup>. Then if we choose some constant ε in (0, lg 3 - 1], we have f(n) = O(n<sup>log<sub>b</sub>a-ε</sup>) for all n. Hence case 1 applies and T(n) = Θ(n<sup>lg 3</sup>).</span></p>
<p><span style="font-size: 19px;">2) The divide-and-conquer solution to maximum subrange sum problem and Mergesort: T(n) = 2T(n/2) + n. It is easy to verify that case 2 applies, and T(n) = Θ(n lg n).</span></p>
<p><span style="font-size: 19px;">3) T(n) = 3T(n/2) + n<sup>2</sup>. Here f(n) = n<sup>2</sup>, and n<sup>log<sub>b</sub>a</sup>&nbsp;= n<sup>lg 3</sup>. It is easy to see that the first conditio of case 3 is satisfied. To verify that case 3 applies, we still have to verify the regularity condition. We have 3f(n/2) = 3(n/2)<sup>2</sup>&nbsp;= (3/4)n<sup>2&nbsp;</sup>&lt;= cn<sup>2</sup>, for c = 3/4. Now we can claim that case 3 applies and T(n) = Θ(n<sup>2</sup>).</span></p>
<p><span style="font-size: 19px;">4) T(n) = 2T(n/2) + Θ(n). The solution would be the same as that to the solution to example 2). It is easy to see why by thinking about the definition of order notation.</span></p>
<p><span style="font-size: 19px;">The solution to T(n) = T(⌈n/2⌉) + T(⌊n/2⌋) + Θ(n) would also be the same, as indicated in the theorem, n/b can be interpreted as its floor/ceiling.</span></p>
<p><span style="font-size: 19px;">The solution to T(n) = 2T(n/2) + O(n) would be O(n lg n) instead of Θ(n lg n) (think about why).</span></p>
<p><span style="font-size: 19px;">5) T(n) = 2T(n/2) + n lg n. Here f(n) = n lg n, and n<sup>log<sub>b</sub>a</sup> = n. Since f(n) = &nbsp;Θ(n<sup>log<sub>b</sub>a</sup>lg<sup>k </sup>n) for k = 1, case 2 applies. Then T(n) = Θ(n lg<sup>2</sup> n).</span></p>
<p><span style="font-size: 19px;">6) T(n) = 2T(n/2) + n / lg n. Here f(n) = n / lg n, and n<sup>log<sub>b</sub>a</sup> = n. Case 3 does not apply since f(n) grows more slowly than n<sup>log<sub>b</sub>a</sup>. Case 2 does not apply because f(n) = &nbsp;Θ(n<sup>log<sub>b</sub>a</sup>lg<sup>k </sup>b) for k = -1, but k cannot be negative in case 2. Could case 1 apply? Apparently it does, but in fact it does not. For any ε &gt; 0, we have lim<sub>n→∞</sub>&nbsp;(f(n)/(n<sup>log<sub>b</sub>a+ε</sup>)) = lim<sub>n→∞</sub> (n<sup>ε </sup>/ lg n) = lim<sub>n→∞</sub> (ε ln 2 n<sup>ε</sup>) = ∞.</span></p>
<p><span style="font-size: 19px;">Thus f(n) = ω(n<sup>log<sub>b</sub>a-ε</sup>) and case 1 cannot apply.</span><span style="font-size: 19px;"></span></p>
<p><span style="font-size: 19px;">Therefore, master theorem does not apply to this recurrence.</span></p>
<p><span style="font-size: 19px;"></span></p>
<br>
<h3><span style="font-size: 23px;">Recursion Tree</span></h3>
<p><span style="font-size: 19px;">How did people prove the master theorem? They did this with the help of recursion tree, which is another approach of solving recurrences.</span></p>
<p><span style="font-size: 19px;">In a recursion tree, each node represents the cost of a single subproblem somewhere in the set of recursive function invocation.</span></p>
<p><span style="font-size: 19px;">Here I will show how to make use of a recursion tree to make a good guess on the running time. Let's take the following recurrence for example:</span></p>
<p><span style="font-size: 19px;">T(n) = 3T(⌊n/4⌋) + c n<sup>2</sup>&nbsp;if n &gt; 1</span><br><span style="font-size: 19px;">T(n) = c if n = 1</span></p>
<p><span style="font-size: 19px;">For simplicity, assume that n is a power of 4. Previously, we showed why similar simplifying assumptions are reasonable. Then the recurrence becomes T(n) = 3T(n/4) + c n<sup>2</sup>.</span></p>
<p><span style="font-size: 19px;">By assigning cost to each node and summing up the costs of the nodes at each level, we have the following figure</span></p>
<p><span style="font-size: 19px;"><img src="Pictures/recursiontree.jpg" alt="Recursion Tree" title="Recursion Tree" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span></p>
<p><span style="font-size: 19px;">We next compute the number, h, of the levels of this tree. The problem size of the root is n, and the problem size of a node at each successive level is 1/4 of that of a node at the level above, and at the leaf level (level h), the problem size is 1. Thus n (1/4)<sup>h-1</sup>&nbsp;= 1, and we have h = log<sub>4</sub>&nbsp;n + 1.</span></p>
<p><span style="font-size: 19px;">The number of leaves is thus 3<sup>h-1</sup>&nbsp;= 3<sup>log<sub>4</sub>n</sup>&nbsp;= n<sup>log<sub>4</sub>3</sup>.</span></p>
<p><span style="font-size: 19px;">Summing up the cost of all levels, we have</span></p>
<p><span style="font-size: 19px;">T(n) = cn<sup>2</sup>&nbsp;+ (3/16)c n<sup>2</sup>&nbsp;+ (3/16)<sup>2</sup>c n<sup>2</sup>&nbsp;+ ... + (3/16)<sup>h-2</sup>c n<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span><br><span style="font-size: 19px;">= Σ<sub>i=0, 1, ..., h-2</sub>(3/16)<sup>i</sup>c n<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span><br><span style="font-size: 19px;">&lt;= Σ<sub>i=0, 1, ..., ∞</sub>(3/16)<sup>i</sup>c n<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span></p>
<p><span style="font-size: 19px;">....We will finish the summation in the next lecture.</span></p>

<p><span style="font-size: 19px;">Summing up the cost of all levels, we have</span></p>
<p><span style="font-size: 19px;">T(n) = cn<sup>2</sup>&nbsp;+ (3/16)c n<sup>2</sup>&nbsp;+ (3/16)<sup>2</sup>c n<sup>2</sup>&nbsp;+ ... + (3/16)<sup>h-2</sup>c n<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span><br><span style="font-size: 19px;">= Σ<sub>i=0, 1, ..., h-2</sub>(3/16)<sup>i</sup>c n<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span><br><span style="font-size: 19px;">&lt;= Σ<sub>i=0, 1, ..., ∞</sub>(3/16)<sup>i</sup>c n<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span><br><span style="font-size: 19px;">= (16/13)cn<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span><br><span style="font-size: 19px;">= O(n<sup>2</sup>)</span></p>
<p><span style="font-size: 19px; color: #ffba59;">Students in Section 2: This was when we lost power.</span></p>
<p><span style="font-size: 19px;">We often make use of recursion tree to make a good guess, and then prove the guess using substitution method. For this example, we can easily prove by induction that T(n) &lt;= 2cn<sup>2</sup>. In fact, we can even prove that T(n) &lt;= (16/13)cn<sup>2</sup>.</span></p>
<h3><span style="font-size: 19px;">1. Closest Pair of Points</span></h3>
<p><span style="font-size: 19px;">In this problem, we look for the closest pair of points in a set, P, of n points.</span></p>
<p><span style="font-size: 19px;">The distance between two points (x<sub>1</sub>, y<sub>1</sub>) and (x<sub>2</sub>, y<sub>2</sub>) can be computed as d = √(x<sub>1</sub>-x<sub>2</sub>)<sup>2</sup>+(y<sub>1</sub>-y<sub>2</sub>)<sup>2</sup>. To compare the distance between one pair of points with the distance between another pair, we can compare the square of the distance to avoid the square root operation which cannot be performed in constant time. The brute-force approach will check all the pairs of points and thus will require Θ(n<sup>2</sup>) time.</span></p>
<p><span style="font-size: 19px;">To design a better solution, we use divide and conquer.</span></p>
<p><span style="font-size: 19px;"><strong>DIVIDE</strong>: We split P into two subsets P<sub>L</sub>&nbsp;and P<sub>R</sub>&nbsp;of approximately equal size. That is, |P<sub>L</sub>| = ⌈|P|/2⌉, and |P<sub>R</sub>| = ⌊|P|/2⌋. This can be done by sorting points by x-coordinate, and split P in two with a vertical line L through the median of the x-coordinates. Thus P<sub>L</sub>&nbsp;has points on or to the left of L, and P<sub>R</sub>&nbsp;has points on or to the right of R.</span></p>
<p><span style="font-size: 19px;"><strong>CONQUER</strong>: Find the closest pair of points in P<sub>L</sub>&nbsp;and P<sub>R</sub>&nbsp;recursively. Let δ<sub>L</sub>&nbsp;and δ<sub>R</sub>&nbsp;denote the distances between the closest pairs in P<sub>L</sub>&nbsp;and P<sub>R</sub>&nbsp;respectively. Let δ = min(δ<sub>L</sub>, δ<sub>R</sub>).</span></p>
<p><span style="font-size: 19px;"><strong>COMBINE</strong>: The closest pair is either the pair with distance δ found recursively during the CONQUER step, or a pair of points, p<sub>L</sub>&nbsp;(coordinates (x<sub>1</sub>, y<sub>1</sub>)) and p<sub>R</sub>&nbsp;(coordinates (x<sub>2</sub>, y<sub>2</sub>)), with p<sub>L</sub>&nbsp;in P<sub>L</sub>&nbsp;and p<sub>R</sub>&nbsp;in P<sub>R</sub>, whose distance is less than δ.</span></p>
<p><span style="font-size: 19px;">If the latter exists, we make the following observations:</span></p>
<ol>
  <li><span style="font-size: 19px;">(x<sub>1</sub>-x<sub>2</sub>)<sup>2</sup>&nbsp;+ (y<sub>1</sub>-y<sub>2</sub>)<sup>2</sup>&nbsp;&lt; δ<sup>2</sup>.</span></li>
  <li><span style="font-size: 19px;">|x<sub>1</sub>-x<sub>2</sub>| &lt; δ and |y<sub>1</sub>-y<sub>2</sub>| &lt; δ.</span></li>
  <li><span style="font-size: 19px;">There exists a 2δ ×δ rectangle E centered at line L that contains p<sub>L</sub>&nbsp;and p<sub>R</sub>. See the figure below. The correctness follows from item 2 above: these two points are at most δ units of distance apart vertically, and the distance of neither of them from L could be more than δ.</span><br><span style="font-size: 19px;"><img src="Pictures/recE.jpg" alt="rectangle" title="rectangle" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span></li>
</ol>
<ol start="4">
  <li><span style="font-size: 19px;">The left half of E has at most 4 points in P<sub>L</sub>. These four points are at the four corners of the left half of E. This is true because the distance between any two points in P<sub>L</sub>&nbsp;cannot be less than δ. The right half of E has at most 4 points in P<sub>R</sub>&nbsp;for the same reason. See the figure below.</span><br><span style="font-size: 19px;"><img src="Pictures/recE2.jpg" alt="Rectangle, again" title="Rectangle, again" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span><br><span style="font-size: 19px;">Thus the total number of points of P that reside in E cannot be more than 8. In the figure above, two points are duplicate points (those two on L), but here we are interested in an upper bound which does not have to be tight.</span></li>
</ol>
<p><span style="font-size: 19px;">From these observations, we have the following efficient process of finding p<sub>L</sub>&nbsp;and p<sub>R</sub>:</span></p>
<ol>
  <li><span style="font-size: 19px;">Sort the points in the strip of width 2δ centered at L by y-coordinate. We need not consider points that are not in this strip.</span></li>
  <li><span style="font-size: 19px;">For these points, compare each of them with 7 other points that follow it. This is sufficient for the purpose of finding p<sub>L</sub>&nbsp;and p<sub>R</sub>, since in the worst case, there are at most 6 points between them by observation 4.</span></li>
</ol>
<p><span style="font-size: 19px;">To analyze the running time, we have the following recurrence T(n) = 2T(n/2) + O(n lg n). The O(n lg n) is due to the sorting in both the divide (by x-coordinate) and the combine (by y-coordinate) steps. We solved this recurrence using the master theorem previously and found that T(n) = O(n lg<sup>2</sup>&nbsp;n) time.</span></p>
<p><span style="font-size: 19px;"><strong>Speed-up</strong>: To avoid sorting for each recursive call, at the beginning of the algorithm, we sort all the points by x-coordinate and store them in an array X. We then sort all the points by y-coordinate and store them in an array Y. Then, with care, we will make use of these two arrays to avoid sorting in recursive calls.</span></p>
<p><span style="font-size: 19px;">The content of array X will not change during the recursive calls. In the divide step, we divide X[p..r] (the set P) into X[p..q] (the set P<sub>L</sub>) and X[q+1..r] (the set P<sub>R</sub>), where q = ⌊(p+r)/2⌋.</span></p>
<p><span style="font-size: 19px;">When dividing P into P<sub>L</sub>&nbsp;and P<sub>R</sub>, we split Y into Y<sub>L</sub>&nbsp;and Y<sub>R</sub>&nbsp;by performing a linear scan in P. For each point encountered during this scan, we check whether it is in P<sub>L</sub>&nbsp;or P<sub>R</sub>, and copy it to either Y<sub>L</sub>&nbsp;or Y<sub>R</sub>&nbsp;accordingly. We then use Y<sub>L</sub>&nbsp;and Y<sub>R</sub>&nbsp;as arguments for recursive calls.</span></p>
<p><span style="font-size: 19px;">In the combine step, to sort the points in the strip of width 2δ centered at L, we perform a linear scan in Y and extract these points.</span></p>
<p><span style="font-size: 19px;">The above strategy allows us to perform sorting by x-coordinate once and by y-coordinate once during the entire execution of the algorithm.</span></p>
<p><span style="font-size: 19px;">The running time now has two parts: the part for sorting at the beginning (O(n lg n) time), and the part for the recursive algorithm (T(n) time). We now have T(n) = 2T(n/2) + O(n), and thus T(n) = O(n lg n). The total running time is thus O(n lg n) + T(n) = O(n lg n).</span></p>
<br>
<h2><span style="font-size: 24px;color: red">4-Invent (or Augment) a Data Structure</span></h2>
<p><span style="font-size: 19px;">The 4th algorithm design paradigm is to invent or augment a data structure. Data structures is one filed of algorithms, and we often use data structures as essential building blocks of algorithms. Previously we already saw one example of using data structures to design an algorithm. This is the maxsubrange3 algorithm, discussed in a previous lecture. In this algorithm, we created a prefix sum array p and used it to compute a subrange sum. Now let's see more examples.</span></p>
<h3><span style="font-size: 19px;">1. Abelian Square Detection in Strings</span></h3>
<p><span style="font-size: 19px;">In a string, an Abelian square is a substring of the form xx', where |x| = |x'| (|x| is the length of x), and x is a permutation of x'. For example, the English word reappear is an abelian square, in which x = reap and x' = pear, and so is the DNA fragment AACGTCAGAT.</span></p>
<p><span style="font-size: 19px;">The problem we consider is: Given string S[1..n], determine if it has an abelian square. For example, if S = preappearance, then the answer is true.</span></p>
<p><span style="font-size: 19px;">How do we determine whether x is a permutation of x'? One way of doing this is to sort both strings, and compare the sorted strings symbol by symbol. If the sorted strings are equal, then x is a permutation of x'.&nbsp;</span></p>


<p><span style="font-size: 19px;">How do we determine whether x is a permutation of x'? One way of doing this is to sort both strings, and compare the sorted strings symbol by symbol. If the sorted strings are equal, then x is a permutation of x'. Based on this, we have the following solution:</span></p>
<pre><span style="font-size: 19px;">for len ← 1 to ⌊n/2⌋ do</span><br><span style="font-size: 19px;">  for start ← 1 to n - 2 len + 1 do</span><br><span style="font-size: 19px;">    x ← sort(S[start..start+len-1]</span><br><span style="font-size: 19px;">    y ← sort(S[start+len..start+2len-1]</span><br><span style="font-size: 19px;">    if x = y then</span><br><span style="font-size: 19px;">      return true

return false</span></pre>
<p><span style="font-size: 19px;">The running time is O(n<sup>3</sup>lg n).</span></p>
<p><span style="font-size: 19px;">Can we do better? Let's think of a different way of determining whether x is a permutation of x'. If strings are defined over a finite alphabet Σ of k symbols, then we can tell whether x is a permutation of x' by counting the number of occurrences of each symbol in either string. If each symbol occurs the same number of times in both strings, then x is a permutation of x'. This means we need find a way to compute the number of occurrences of each symbol in any substring fast.</span></p>
<p><span style="font-size: 19px;">To do this, we augment the prefix sum array and define it as a 2D array p[1..k, 0..n], in which p[j, m] stores the number of occurrences of letter j in S[1..m]. We can compute p by a single scan of S from left to right in O(kn) time. This can be used to compute the number of occurrences of any letter in any substring in constant time.</span></p>
<p><span style="font-size: 19px;">Thus, x is a permutation of x' if and only if, for all 1&lt;=j&lt;=k, p[j, start+len-1] - p[j, start-1] = p[j, start+2len-1] - p[j, start+len-1].</span></p>
<p><span style="font-size: 19px;">Based on this, we have the following solution:</span></p>
<pre><span style="font-size: 19px;">for j ← 1 to k do
  p[j, 0] ← 0

for i ← 1 to n do
  for j ← 1 to k do
    if S[i] is the j-th letter of the alphabet then
      p[j, i] ← p[j, i-1] + 1
    else
      p[j, i] ← p[j, i-1]

for len ← 1 to ⌊n/2⌋ do
  for start ← 1 to n - 2 len + 1 do
    j ← 1
    while (j &lt;= k) and (p[j, start+len-1] - p[j, start-1] 
                        = p[j, start+2len-1] - p[j, start+len-1])
      j ← j + 1

    if j = k + 1
      return true

return false</span></pre>
<p><span style="font-size: 19px;">The running time of this solution is O(kn<sup>2</sup>). If k is not too large, this is much better than the previous solution.</span></p>
<h3><span style="font-size: 19px;">2. Supporting Subrange Sum Under Updates</span></h3>
<p><span style="font-size: 19px;">Now let's invent a data structure to solve the following problem: We would like to maintain an array A[1..n] to support</span></p>
<p><span style="font-size: 19px;">Query a: Set A[i] ← A[i] + δ</span><br><span style="font-size: 19px;">Query b: Subrange sum (c, d): Computing A[c] + A[c+1] + ... + A[d]</span></p>
<p><span style="font-size: 19px;">If we simply store the data in the array A, then query a can be supported in O(1) time, but query b requires O(n) time. Alternatively, if we store the data in the prefix array p, then query b can be supported in O(1) time, but query a requires O(n) time. Now let's design a better data structure.</span></p>
<p><span style="font-size: 19px;">In this data structure, each node stores the sum of a subarray. Each leaf corresponds to a subarray of length 1, and the level above corresponds to subarrays of length 2, and so on. The following figure illustrates the data structure for n = 8:</span></p>
<p><img src="Pictures/sumupdate.jpg" alt="sum" title="sum" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></p>
<p><span style="font-size: 19px;">To support query a, we simply update A[i] and all its ancestors, which uses O(lg n) time.</span></p>
<p><span style="font-size: 19px;">To compute the sum of A[c..d], we compute it as the difference between the sums of A[1..d] and A[1..c]. To compute the sum of A[1..d], first initialize a variable named sum to 0. We start from the root, and descend to the highest node whose range ends with d, and add its value to sum. During the top-down traversal, each time we choose the right child, we add the value of the left child (i.e., the left sibling of this child) to sum.</span></p>
<p><span style="font-size: 19px;">For example, using this algorithm, A[1..7] is computed as A[1..4] + A[5..6] + A[7]. A[1..6] is computed as A[1..4] + A[5..6]. A[1..3] is computed as A[1..2] + A[3].</span></p>
<p><span style="font-size: 19px;">As the height of this tree is O(lg n), query b can be answered in O(lg n) time.</span></p>
<p><span style="font-size: 19px;">Therefore, our data structure can support both queries in logarithmic time.</span></p>
<p><span style="font-size: 19px;">An exercise: how to store the tree as an array? Hint: think of heap.</span></p>
<br>
<h1><span style="font-size: 24px;color: red">Greedy Algorithms</span></h1>
<p><span style="font-size: 19px;">The 5th algorithm design paradigm discussed in this course is greedy algorithms. They are used to solve optimization problems. For an optimization problem, there exist many solutions that satisfy the constraints given in the problem. Each solution has a value, and we would like to find a solution with the optimal (minimum or maximum) value.</span></p>
<p><span style="font-size: 19px;">The idea of greedy algorithms is to make the choice that looks the best at the moment. More formally, we make a locally optimal choice (i.e., a choice optimal for a subproblem) hoping that this will lead to globally optimal solution. The use of the word "hoping" here implies that a locally optimal choice will not always lead to a globally optimal solution, and thus to master greedy algorithms, we need learn to prove or disprove that a certain greedy strategy will yield an optimal solution.</span></p>
<h3><span style="font-size: 19px;">1. Activity-Selection Problem</span></h3>
<p><span style="font-size: 19px;">In this problem, we are given a set of activities competing for time intervals on a given resource. Here n denotes the number of activities. Activity i has start time s<sub>i</sub>&nbsp;and finish time f<sub>i</sub>. If selected activity i takes up the half open interval [s<sub>i</sub>, f<sub>i</sub>).</span></p>
<p><span style="font-size: 19px;">The goal is to produce a schedule containing the maximum number of compatible activities. We say that activity i is compatible with activity j if either s<sub>i</sub>&nbsp;&gt;= f<sub>j</sub>&nbsp;or s<sub>j</sub>&nbsp;&gt;= f<sub>i</sub>&nbsp;holds.</span></p>
<p><span style="font-size: 19px;">The following figure is an example that I gave in class, in which each interval corresponds to an activity. By choosing the activities circled in green, we get one optimal solution.</span></p>
<p><span style="font-size: 19px;"><img src="Pictures/interval.jpg" alt="activities" title="activities" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span></p>
<p><span style="font-size: 19px;">Let's first see some greedy strategies that do not work.</span></p>
<p><span style="font-size: 19px;">One greedy strategy is to select activity that starts earliest and is compatible with previous choices. To show that it does not always yield an optimal solution, we construct the following counterexample:</span></p>
<pre><span style="font-size: 19px;">i     s<sub>i</sub>    f<sub>i</sub>
1     0     10
2     2     3
3     4     5
4     2     8
5     7     9</span></pre>
<p><span style="font-size: 19px;">The greedy algorithm will only schedule one activity: activity 1. However, it is easy to verify that activities 2, 3 and 5 are mutually compatible and the solution that selects these three activities is even better than the solution given by the greedy algorithm. Therefore, this greedy strategy does not always yield an optimal solution.</span></p>
<p><span style="font-size: 19px;">The following two greedy strategies will not always yield optimal solutions, either, and I asked you to come up with counterexamples as exercise:</span></p>
<ul>
<li><span style="font-size: 19px;">Select activity of shortest duration among remaining ones</span></li>
<li><span style="font-size: 19px;">Select activity with smallest number of conflicts among remaining ones</span></li>
</ul>

<pre><span style="font-size: 19px;">&nbsp;</span></pre>



<p><span style="font-size: 19px;">Now let's come up with a strategy that would work. From the counterexample above, we can see that if an activity ends really late, then choosing it would prevent us from scheduling many activities. This gives us some idea.</span></p>
<p><span style="font-size: 19px;">The following greedy algorithm will compute an optimal solution:</span></p>
<ul>
<li><span style="font-size: 19px;">Sort activities by finish time in increasing order</span></li>
<li><span style="font-size: 19px;">Start with the first, and choose the next possible activity compatible with previous ones</span></li>
</ul>
<p><span style="font-size: 19px;">I obtained the solution to the example in the figure using this strategy.</span></p>
<p><span style="font-size: 19px;">How to prove that this algorithm works? Let's look at a larger example and try to make some observations.</span></p>
<pre><span style="font-size: 19px;">i     s<sub>i</sub>    f<sub>i</sub>
1     1     4
2     3     5
3     0     6
4     5     7
5     3     9
6     5     9
7     6     10
8     8     11
9     8     12
10    2     14
11    12    16</span></pre>
<p><span style="font-size: 19px;">The greedy algorithm will select activities 1, 4, 8, 11.</span></p>
<p><span style="font-size: 19px;">First, let's see some intuition about why the solution computed by the greedy algorithm is optimal. For the above example, let activities (a<sub>1</sub>, a<sub>2</sub>, ..., a<sub>k</sub>) be an optimal answer. If we replace a<sub>1</sub>&nbsp;by activity 1, then (1, a<sub>2</sub>, ..., a<sub>k</sub>) is still an optimal solution, since activity 1 finishes no later than a<sub>1</sub>does, so&nbsp;a<sub>2</sub>, ..., a<sub>k</sub>&nbsp;are compatible with activity 1. We can continue and replace a<sub>2</sub>, ..., a<sub>k</sub>&nbsp;by 4, 8, 11 in the same fashion, and this way we have transformed the optimal answer to the answer given by the greedy algorithm, and they must both have the same number of activities.</span></p>
<p><span style="font-size: 19px;">Now let's formalize the above idea and come up with a correctness proof.</span></p>
<p><span style="font-size: 19px;">Proof. Let (a<sub>1</sub>, a<sub>2</sub>, ..., a<sub>k</sub>) be the answer given by the greedy algorithm, in which a<sub>i</sub>&nbsp;takes place before a<sub>i+1</sub>&nbsp;for all 1 &lt;= i &lt;= k-1. Let (b<sub>1</sub>, b<sub>2</sub>, ..., b<sub>m</sub>) be an optimal answer, in which b<sub>i</sub>&nbsp;takes place before b<sub>i+1</sub>&nbsp;for all 1 &lt;= i &lt;= m-1. We prove that k = m.</span></p>
<p><span style="font-size: 19px;">To prove this, we prove by induction on i that a<sub>i</sub>&nbsp;finishes no later than b<sub>i</sub>. This is sufficient because, with this claim, if k &lt; m, then b<sub>k+1</sub>&nbsp;starts after a<sub>k</sub>&nbsp;finishes, and is thus compatible with a<sub>1</sub>, a<sub>2</sub>, ..., a<sub>k</sub>. The greedy algorithm could have scheduled b<sub>k+1</sub>&nbsp;after a<sub>k</sub>, since b<sub>k+1</sub>&nbsp;finishes after a<sub>k</sub>&nbsp;does and is compatible with previously selected activities, but it didn't. This is a contradiction, so we cannot have k &lt; m.</span></p>
<p><span style="font-size: 19px;">Now let's prove the above claim by induction. The base case holds since a<sub>1</sub>&nbsp;ends no later than any other activity.</span></p>
<p><span style="font-size: 19px;">Assume that the claim holds for i, and we prove it for i+1. Let S be the set of activities compatible with (a<sub>1</sub>, a<sub>2</sub>, ..., a<sub>i</sub>), and S' be the set of activities compatible with (b<sub>1</sub>, b<sub>2</sub>, ..., b<sub>i</sub>). Then S contains all the activities whose starting time is greater than equal to the finish time of a<sub>i</sub>, and S' contains all the activities whose starting time is greater than equal to the finish time of b<sub>i</sub>. By the induction hypothesis, a<sub>i</sub>&nbsp;finishes no later than b<sub>i</sub>. Therefore, S' is a subset of S. Since b<sub>k+1</sub>&nbsp;is in S', it is also in S. By the greedy strategy, a<sub>i+1</sub>&nbsp;has the earliest finish time among activities in S, and thus it finishes no later than b<sub>i+1</sub>.</span></p>
<p><span style="font-size: 19px;">Pseudocode of the greedy algorithm:</span></p>
<pre><span style="font-size: 19px;">Activity_Selection(s, f, n)
sort both s and f by f[i] in increasing order
A ← {1}
k ← 1
for i ← 2 to n do
  if s[i] &gt;= f[k] then
    A ← A ∪ {i}
    k ← i
return A</span></pre>
<p><span style="font-size: 19px;">The time complexity is O(n lg n).</span></p>
<h3><span style="font-size: 19px;">2. Knapsack Problem</span></h3>
<p><span style="font-size: 19px;">In the input, we have n items, and each item i has a weight w<sub>i</sub>&nbsp;(in kg) and a value v<sub>i</sub>&nbsp;(in dollar amount). We also have a knapsack, which has a maximum weight, W, that it can hold.</span></p>
<p><span style="font-size: 19px;">The problem is which items we should choose to maximize the value we can hold in our knapsack.</span></p>
<p><span style="font-size: 19px;">(Think of the example of the thief mentioned in class.)</span></p>
<p><span style="font-size: 19px;">The following is an example (the value/weight ratio is not given as input, but we can compute it easily):</span></p>
<pre><span style="font-size: 19px;">W = 100, n = 6</span><br><span style="font-size: 19px;">item                 1     2     3     4     5     6</span><br><span style="font-size: 19px;">value                80    70    85    40    75    65
weight               25    40    70    15    20    5
value/weight ratio   3.20  1.75  1.21  2.67  3.75  13</span></pre>
<p><span style="font-size: 19px;">The following are four greedy strategies:</span></p>
<ol>
<li><span style="font-size: 19px;">Most valuable item first</span></li>
<li><span style="font-size: 19px;">Heaviest item first</span></li>
<li><span style="font-size: 19px;">Lightest item first</span></li>
<li><span style="font-size: 19px;">Item with the highest value to weight ratio first</span></li>
</ol>
<p><span style="font-size: 19px;">Unfortunately, none of these four strategies works. The above example can be used as a counterexample to show that these strategies do not work.</span></p>
<p><span style="font-size: 19px;">An optimal solution to the above sample input is to select items 1, 2, 5, and 6. The total weight would be 90, and the total value would be 290.</span></p>
<p><span style="font-size: 19px;">If greedy strategy 1 is used, then items 3, 1 and 6 are selected. The total weight is 100, and the value is 230.</span></p>
<p><span style="font-size: 19px;">If greedy strategy 2 is used, it happens that items 3, 1 and 6 are selected again, which is not an optimal solution.</span></p>
<p><span style="font-size: 19px;">If greedy strategy 3 is used, items 6, 4, 5 and 1 are selected. The total weight is 65, and the value is 260.</span></p>
<p><span style="font-size: 19px;">If greedy strategy 4 is used, items 6, 5, 1 and 4 are selected. Again, this is not optimal.</span></p>
<h3><span style="font-size: 19px;">3. Fractional Knapsack Problem</span></h3>
<p><span style="font-size: 19px;">One variant of the knapsack problem can be solved using a greedy algorithm. In this variant, instead of having each item in the input representing a single physical item, each item represents a sack of some substance which can be arbitrarily divided. Examples: a sack of sugar, a sack of salt and a sack of gold dust.</span></p>
<p><span style="font-size: 19px;">Thus, we can take a fraction of an item and put it into our knapsack.</span></p>
<p><span style="font-size: 19px;">Under these assumptions, greedy strategy 4 works! Using this strategy, we first choose items 6, 5, 1, 4 and we can still put 35kg's items into our knapsack. The next item with the highest value to weight ratio is item 2, and its weight is 40 kg. Then we choose 35/40 (=7/8) of item 2, which is worth 7/8 * 70 = 61.25. With this choice, the total weight is 100, and the total value is 321.25.</span></p>
<p><span style="font-size: 19px;">The following is the pseudocode:</span></p>
<pre><span style="font-size: 19px;">greedy(n, v, w, W)</span><br><span style="font-size: 19px;">sort both v and w by the ratio v[i]/w[i] in decreasing order
free ← W
sum ← 0
for i ← 1 to n do
  x ← min(w[i], free)
  sum ← sum + v[i] * (x/w[i])
  free ← free - x
return sum</span></pre>
</body></html>