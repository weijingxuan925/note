 <!DOCTYPE html>
<html><head>
<meta charset="utf-8">
<title>讲义</title>
</head>
<body style="font-family: verdana, sans-serif;font-size: 12px;color: #202122;">
<p><span style="font-size: 19px; ">Today I first gave an <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=230447&amp;type=content&amp;rcode=dalhousie-2471119" target="_self">introduction to this course</a>.</span></p>
<h2><span style="font-size: 24px;">RAM, Algorithm Analysis and Order Notation</span></h2>
<p><span style="font-size: 19px;">At the beginning of this course, let us learn some basic stuff about algorithm analysis.</span></p>
<h2><span style="font-size: 30px; color: rgb(250, 8, 8);" >Chapter1&2</span> </h2>
<h3><span style="font-size: 19px;">1. Pseudocode</span></h3>
<p><span style="font-size: 19px;">Before we analyze algorithms, let us learn how to describe an algorithm precisely. Instead of using "real" code, we use pseudocode to specify an algorithm. In a piece of pseudocode, we use the most clean and concise expressive method to describe a given algorithm. Sometimes, the pseudocode of an algorithm could even use an English sentence to describe a step of this algorithm. For example, if a complicated algorithm requires us to first sort the given input array, then we could write something like "sort the array A in ascending order using mergesort" as its first step. When writing pseudocode, we are not concerned with software engineering issues, such as data abstraction, modularity, and exception &amp; error handling. Thus, pseudocode focuses on conveying the essence of an algorithm.</span></p>
<p><span style="font-size: 19px;">In this course, you are allowed to follow any convention of pseudocode that you prefer, as long as your pseudocode is readable and consistent. If you do not have any preference yourself, feel free to follow the way in which I write pseudocode, or the convention described in the textbook (Chapter 2).</span></p>
<h3><span style="font-size: 19px;">2. Insertion Sort</span></h3>
<p><span style="font-size: 19px;">Now, let's write down the pseudocode of insertion sort, which is a sorting algorithm taught in a previous course, before we analyze its running time.</span></p>
<pre><span style="font-size: 19px;">INSERTION-SORT(A[1..N])
  for j ← 2 to n
    key ← A[j]</span><br><span style="font-size: 19px;">    i ← j - 1
    while i &gt; 0 and A[i] &gt; key</span><br><span style="font-size: 19px;">      A[i+1] ← A[i]</span><br><span style="font-size: 19px;">      i ← i - 1</span><br><span style="font-size: 19px;">    A[i+1] ← key</span></pre>
<p><span style="font-size: 19px;">How to measure the running time of an algorithm? We may be tempted to use wall-clock time, but this is a bad measurement as it highly depends on the computer on which the algorithm is run. Hence, to analyze an algorithm, we need remove computer details such as processor speed, disk and memory, and one way of achieving this is to count the number of elementary operations only. The goal of algorithm analysis is to predict the resources required by an algorithm, and to do this, we need a machine model under which we express our algorithms, and some way of assigning costs of the operations under this model.</span></p>
<h3><span style="font-size: 19px;">3. Random Access Machine (RAM)</span></h3>
<p><span style="font-size: 19px;">We analyze our algorithms under the random-access machine (RAM) model of computation. Algorithms are implimented as computer programs in a RAM.</span></p>
<p><span style="font-size: 19px;">A figure illustrating RAM:</span></p>
<p><span style="font-size: 19px;"><img src="Pictures/ram.png" alt="RAM" title="RAM" width="1426" height="751"></span></p>
<p><span style="font-size: 19px;">In this machine model, there are unbounded number of local memory cells (words). An integer or a floating-point number can be stored in a memory cell. Program instructions are executed sequentially, i.e., one after another starting from the first to the last; there is no concurrency. The location counter indicates the instruction of the program that is currently being executed.</span></p>
<p><span style="font-size: 19px;">In this model, the time complexity (running time) is defined as the number of instructions executed, and the space complexity is the number of memory cells accessed.</span></p>
<p><span style="font-size: 19px;">The instruction set of RAM contains instructions commonly found in real computers, such as arithmetic operations (add, subtract, multiply, divide, remainder, floor, ceiling), data movement (load, store, copy) and control (conditional &amp; unconditional branch). Each instruction uses 1 unit of time.</span></p>
<p><span style="font-size: 19px;">[Note: I didn't have time to talk about the following paragraph in Section 2 today, but I will talk about it in Section 2 on Friday] There are instructions in real computers that are not listed above. If your algorithm requires them, then make sure to make reasonable assumptions. For example, the exponentiation operation, i.e., computing x to the power of y, would require a numerical algorithm that is costly, and we would NOT assume that there is an instruction in the RAM that can perform this operation. On the other hand, when k is an integer &lt;= the number of bits in a word, then 2<sup>k</sup>&nbsp;can be computed using a left shift. Thus we would assume that 2<sup>k</sup>&nbsp;is a constant-time operation.</span></p>
<p><span style="font-size: 19px;">&nbsp;</span></p>
<p><span style="font-size: 19px;">[section 2 students need to finish reading the last paragraph of lecture 1 notes]</span></p>
<p><span style="font-size: 19px;">There are some limitations of the RAM, and more advanced models have been proposed to address these limitations. For example, to introduce concurrency to the RAM model, the PRAM model of computation was proposed. RAM does not consider memory hierarchy; to address this, models such as the external memory model were proposed. These are typically covered in graduate-level courses. To understand these models, a solid background of analyzing algorithms in the RAM is required. Furthermore, in many cases, algorithm analysis under RAM is sufficient. Thus, in this course, we mainly focus on RAM.</span></p>
<h3><span style="font-size: 19px;">Analysis of Insertion Sort</span></h3>
<p><span style="font-size: 19px;"><strong>1. </strong>To analyze algorithms in RAM, we first need understand that the running time of an algorithm is expressed as a function of its input size. This is because the running time of an algorithm normally grows when the size of input grows. We choose reasonable parameters associated with the problem and use them as input size. For example, in sorting, the array size is the input size. In graph algorithms, we often choose the number of vertices and the number of edges as parameters.</span></p>
<p><span style="font-size: 19px;"><strong>2.</strong> The general steps of analyzing pseudocode are</span></p>
<ol>
<li><span style="font-size: 19px;">For each line of pseudocode, count the number of primitive operations in it. Pay attention to the word "primitive" here; sorting an array is not a primitive operation.</span></li>
<li><span style="font-size: 19px;">Multiply this count with the number of times this line is executed.</span></li>
<li><span style="font-size: 19px;">Sum up over all lines.</span></li>
</ol>
<p><span style="font-size: 19px;"><strong>3.</strong> To use this to analyze insertion sort, we first observe that each line of pseudocode can be implemented using a constant number of RAM instructions. Thus, we let c<sub>i</sub>&nbsp;be the cost of line i, which is a constant. Then, for each value of j in the outer loop, we let t<sub>j</sub>&nbsp;be the number of times that the while loop test in line 4 is executed. With these definitions, we have</span></p>
<pre><span style="font-size: 19px;"> INSERTION-SORT(A[1..N])              cost                times
1   for j ← 2 to n                    c<sub>1</sub>                  n
2     key ← A[j]                      c<sub>2</sub>                  n-1</span><br><span style="font-size: 19px;">3     i ← j - 1                       c<sub>3</sub>                  n-1
4     while i &gt; 0 and A[i] &gt; key      c<sub>4</sub>                  ∑<sub>j=2,3,...,n</sub> t<sub>j</sub></span><br><span style="font-size: 19px;">5       A[i+1] ← A[i]                 c<sub>5</sub>                  ∑<sub>j=2,3,...,n</sub> (t<sub>j</sub>-1)</span><br><span style="font-size: 19px;">6       i ← i - 1                     c<sub>6</sub>                  ∑<sub>j=2,3,...,n</sub> (t<sub>j</sub>-1)</span><br><span style="font-size: 19px;">7     A[i+1] ← key                    c<sub>7</sub>                  n-1</span></pre>
<p><span style="font-size: 19px;">Now let's calculate the running time as a function of n:</span></p>
<p><img src="Pictures/tn.jpg" alt="T(n)" title="T(n)" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></p>
<p><span style="font-size: 19px;">This does not tell us much about the time complexity, as t<sub>j</sub>&nbsp;is not known and it depends on the actual input. The only thing that we can say about t<sub>j</sub>&nbsp;is that its value is between 1 and j (inclusive). To make sense of this function, let us see how the input affects the running time.</span></p>
<p><span style="font-size: 19px;"><strong>4. Best Case</strong>: In the best case, the input array A is already sorted, as this guarantees that t<sub>j</sub>&nbsp;is equal to 1. In this case,</span></p>
<p><span style="font-size: 19px;">T(n) = c<sub>1</sub>n + c<sub>2</sub>(n-1) + c<sub>3</sub>(n-1) + c<sub>4</sub>(n-1) + c<sub>7</sub>(n-1) = (c<sub>1</sub>&nbsp;+ c<sub>2</sub>&nbsp;+ c<sub>3&nbsp;</sub>+ c<sub>4</sub>&nbsp;+ c<sub>7</sub>) n - (c<sub>2</sub>&nbsp;+ c<sub>3&nbsp;</sub>+ c<sub>4</sub>&nbsp;+ c<sub>7</sub>)</span></p>
<p><span style="font-size: 19px;">This is a linear function of n, and thus we say the running time is Θ(n).</span></p>
<p><span style="font-size: 19px;"><strong>5. Worst Case</strong>: The case in which the input array A is reverse sorted is the worst case, as this guarantees that t<sub>j</sub>&nbsp;is equal to j. In this case,</span></p>
<p><img src="Pictures/tn2.jpg" alt="worst case" title="worst case" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></p>
<p><span style="font-size: 19px;">This is a quadratic function of n, and thus we say the running time is Θ(n<sup>2</sup>).</span></p>
<p><span style="font-size: 19px;"><strong> 6. Average Case</strong>: To perform the average-case analysis, we need assume that each of the n! permutations of A is equally likely. This is much more complicated, and if you are interested, you could read the following book (available in our library) to find out (not required):</span></p>
<p><span style="font-size: 19px;">The Art of Computer Programming, Volume 3: Sorting and Searching (2nd Edition), Donald E. Knuth.</span></p>
<p><span style="font-size: 19px;">If you intend to read this to enrich your knowledge, you could start by reading the discussions on inversions in the input array (5.1.1) and other properties of permutations in the same chapter, Before reading 5.2.1: Sorting by Insertion.</span></p>
<p><span style="font-size: 19px;">In this course, we mainly (though not always) concentrate on worst-case analysis. As pointed out by the textbook, there are many good reasons: Worst-case analysis gives an upper bound on the running time of the algorithm, guaranteeing the algorithm will not be more costly than the result of the analysis. In many practical applications, the worst-case happens very frequently. For example, when searching for records in a real database application, it is very often that the query would look for some key value that is not in the database. Finally, for many algorithms, the average-case running time is roughly as bad the their worst-case running time.</span></p>
<p><span style="font-size: 19px;">To appreciate the importance of good algorithm design, take the problem of sorting an array of SIN numbers for example. According to the data published by Statistics Canada, at the end of 2015, the population in Canada is roughly 34,880,000. We use this number as the value of n. Then, n<sup>2</sup>&nbsp;is roughly 10<sup>15</sup>. Let's first throw away the constants in the analysis of insertion sort, and say this is the number of instructions required. If we use faster sorting algorithms whose cost is proportional to n lg n (in algorithms, lg n is log<sub>2</sub>&nbsp;n), then for this input, lg n is roughly 25 and n lg n is roughly 10<sup>9</sup>. If the CPU performs 10<sup>11</sup>&nbsp;(100 billion) instructions per second, then insertion sort would require 10<sup>4</sup>&nbsp;seconds which are roughly 3 hours, while mergesort and heapsort would require 1/100 second. There is a big difference. In fact, the number of instructions of insertion sort should be a constant times n<sup>2</sup>, and if the constant is about 20, then it would require a long weekend. If you use mergesort/heapsort, then, even if the constant is 200, the sorting would be done in a matter of seconds.</span></p>

<h2><span style="font-size: 30px;color: rgb(250, 8, 8)">Chapter3</span> </h2>
<h3><span style="font-size: 19px;">Order (Rate) of Growth</span></h3>
<p><span style="font-size: 19px;">Another thing to note is that, when presenting the result of analysis, we used Θ to throw away the constant factors. This is because we care about the order (rate) of the growth of the function that represents the running time, and we often care less about the coefficients. Asymptotic notation allows us to make more simplifying abstraction.</span></p>
<h3><span style="font-size: 19px;">1. Θ-Notation</span></h3>
<p><span style="font-size: 19px;">First, the definition:</span></p>
<p><span style="font-size: 19px;">Θ(g(n)) = {f(n): there exist positive constants c<sub>1</sub>, c<sub>2</sub>&nbsp;and n<sub>0</sub>, s.t. 0 &lt;= c<sub>1</sub>g(n) &lt;= f(n) &lt;= c<sub>2</sub>g(n) for all n &gt;= n<sub>0</sub>}</span></p>
<p><span style="font-size: 19px;">Here we can say that g(n) is an asymptotically tight bound for f(n).</span></p>
<p><span style="font-size: 19px;">Observe that in the definition, Θ(g(n)) is defined as a set of functions. We however often abuse notation (acceptable here) and say f(n) = Θ(g(n)), and this means that f(n) ∈ Θ(g(n)). Sometimes this allows us to write shorter identities. For example, 20n<sup>2</sup>&nbsp;+ 13n + 1 = 20n<sup>2</sup>&nbsp;+ Θ(n) means 20n<sup>2</sup>&nbsp;+ 13n + 1 = 20n<sup>2</sup>&nbsp;+ f(n) and f(n) ∈ Θ(n) </span></p>
<p><span style="font-size: 19px;">Now, let's use the definition to prove that n<sup>2</sup>/2 + lg n = Θ(n<sup>2</sup>).</span></p>
<p><span style="font-size: 19px;">Proof. To prove this claim, we must determine positive constants c<sub>1</sub>, c<sub>2</sub>&nbsp;and n<sub>0</sub>, s.t.</span></p>
<p><span style="font-size: 19px;">c<sub>1</sub>&nbsp;n<sup>2</sup>&lt;= n<sup>2</sup>/2 + lg n &lt;= c<sub>2</sub>&nbsp;n<sup>2</sup></span></p>
<p><span style="font-size: 19px;">This is equivalent to c<sub>1</sub>&nbsp;&lt;= 1/2 + (lg n) / n<sup>2</sup>&nbsp;&lt;= c<sub>2</sub></span></p>
<p><span style="font-size: 19px;">Setting c<sub>1</sub>&nbsp;= 1/4, c<sub>2</sub>&nbsp;= 3/4 and n<sub>0</sub>&nbsp;= 2 would make this inequality hold (make sure that you do verify this).</span></p>
<p><span style="font-size: 19px;">There are also other choices of values of these constants that would guarantee the inequality, and in your proof, you just have to show the existence of one set, according to the definition.</span></p>
<p><span style="font-size: 19px;">Note that asymptotic notation applies to asymptotically positive functions only, which are functions whose values are positive for all sufficiently large n.</span></p>
<h3><span style="font-size: 19px;">2. O-Notation (Asymptotic Upper Bound)</span></h3>
<p><span style="font-size: 19px;">Definition: O(g(n)) = {f(n): there exist positive constants c and n<sub>0</sub>, s.t. 0 &lt;= f(n) &lt;= c g(n) for all n &gt;= n<sub>0</sub>}</span></p>
<p><span style="font-size: 19px;">Following this definition, we can determine that n, n<sup>2</sup>, 3n<sup>2</sup>&nbsp;+ 4n + 5 are all O(n<sup>2</sup>), while n<sup>3</sup>&nbsp;is not.</span></p>
<p><span style="font-size: 19px;">When we say that the running time of insertion sort is O(n<sup>2</sup>), we mean that the worst-case running time of insertion sort is O(n<sup>2</sup>). When we say that an algorithm runs in polynomial time, we mean that there exists a constant k, s.t. the worst-case running time of this algorithm is O(n<sup>k</sup>).</span></p>
<h3><span style="font-size: 19px;">3. Ω-Notation (Asymptotic Lower Bound)</span></h3>
<p><span style="font-size: 19px;">Definition: Ω(g(n)) = {f(n): there exist positive constants c and n<sub>0</sub>, s.t. 0 &lt;= c g(n) &lt;= f(n) for all n &gt;= n<sub>0</sub>}</span></p>
<p><span style="font-size: 19px;">Based on the definitions, we have the following theorem: f(n) = Θ(g(n)) if and only if f(n) = O(g(n)) and f(n) = Ω(g(n)). For example, the statement n<sup>2</sup>/2 + lg n = Θ(n<sup>2</sup>) is equivalent to n<sup>2</sup>/2 + lg n = O(n<sup>2</sup>) and n<sup>2</sup>/2 + lg n = Ω(n<sup>2</sup>).</span></p>
<p><span style="font-size: 19px;">Some quick examples: n<sup>2</sup>, (lg n) n<sup>2</sup>, 4n<sup>2</sup>+5 are all in Ω(g(n)), while n is not.</span></p>
<h3><span style="font-size: 19px;">4. o-Notation</span></h3>
<p><span style="font-size: 19px;">Definition: o(g(n)) = {f(n): for any positive constant c, there exists a constant n<sub>0</sub>, s.t. 0 &lt;= f(n) &lt; c g(n) for all n &gt;= n<sub>0</sub>}</span></p>
<p><span style="font-size: 19px;">This definition shows that f(n) grows more slowly than g(n).</span></p>
<p><span style="font-size: 19px;">Another definition of o-Notation is that f(n) = o(g(n)) if</span></p>
<pre><span style="font-size: 19px;">lim  f(n)/g(n) = 0</span><br><span style="font-size: 19px;">n→∞
</span></pre>
<p><span style="font-size: 19px;">By the definition of limits in calculus and the first definition of o-notation, we can see that these two definitions of little-oh are equivalent.</span></p>
<p><span style="font-size: 19px;">This second definition allows us to perform some quick analysis. For example, 2n = o(n<sup>2</sup>), but 2n<sup>2</sup>&nbsp;≠o(n<sup>2</sup>).</span></p>
<h3><span style="font-size: 19px;">5. ω-Notation</span></h3>
<p><span style="font-size: 19px;">First, f(n) = ω(g(n)) if and only if g(n) = o(f(n)).</span></p>
<p><span style="font-size: 19px;">Formal definition: ω(g(n)) = {f(n): for any positive constant c, there exists a constant n<sub>0</sub>, s.t. 0 &lt;= c g(n) &lt; f(n) for all n &gt;= n<sub>0</sub>}</span></p>
<p><span style="font-size: 19px;">Another definition using limits: f(n) = ω(g(n)) if</span></p>
<pre><span style="font-size: 19px;">lim  f(n)/g(n) = ∞
n→∞ 
</span></pre>
<h3><span style="font-size: 19px;">6. Properties</span></h3>
<p><span style="font-size: 19px;">Here I give some properties of order notation, which are not difficult to prove using definitions. Think about their correctness when reviewing these properties.</span></p>
<ol>
<li><span style="font-size: 19px;">Transitivity: If f(n) = Θ(g(n)) and g(n) = Θ(h(n)), then f(n) = Θ(h(n)). This also applies to O, Ω, o and ω.</span></li>
<li><span style="font-size: 19px;">Reflexivity: f(n) = Θ(f(n)); f(n) = O(f(n)); f(n) = Ω(f(n)). This is however not true for o or ω.</span></li>
<li><span style="font-size: 19px;">Symmetry: f(n) = Θ(g(n)) if and only if g(n) = Θ(f(n)).</span></li>
<li><span style="font-size: 19px;">Transpose Symmetry: f(n) = O(g(n)) if and only if g(n) = Ω(f(n)); f(n) = o(g(n)) if and only g(n) = ω(f(n)).</span></li>
</ol>
<h3><span style="font-size: 19px;">7. Tricks</span></h3>
<p><span style="font-size: 19px;">Here I give some tricks that are useful for comparing functions using order notation.</span></p>
<p><span style="font-size: 19px;">The first trick is that we can determine order notation by computing</span></p>
<pre><span style="font-size: 19px;">lim  f(n)/g(n) = c
n→∞ 
</span></pre>
<p><span style="font-size: 19px;">If the result, c, is 0, then f(n) = o(g(n)). If c = ∞, then f(n) = ω(g(n)). If 0 &lt; c &lt; ∞, then f(n) = Θ(g(n)), which also implies that both f(n) = O(g(n)) and f(n) = Ω(g(n)) hold.</span></p>
<p><span style="font-size: 19px;">Let us use this to prove the following claim: Let d be a nonnegative constant integer and a<sub>0</sub>, a<sub>1</sub>, ..., a<sub>d</sub>&nbsp;be constants, in which a<sub>d</sub>&nbsp;&gt; 0. Let p(n) = Σ<sub>c = 0, 1, ..., d</sub>&nbsp;(a<sub>i&nbsp;</sub>n<sup>i</sup>). Then p(n) = Θ(n<sup>d</sup>).</span></p>
<p><span style="font-size: 19px;">Proof. To prove this, we compute</span></p>
<p><span style="font-size: 19px;">lim<sub>n → ∞</sub>&nbsp;p(n)/n<sup>d</sup>&nbsp;= lim<sub>n → ∞</sub>&nbsp;(a<sub>0</sub>/n<sup>d</sup>&nbsp;+ a<sub>1</sub>/n<sup>d-1</sup>&nbsp;+ ... + a<sub>d-1</sub>/n + a<sub>d</sub>) = a<sub>d</sub>.</span></p>
<p><span style="font-size: 19px;">Since a<sub>d</sub>&nbsp;is a positive constant, we claim that p(n) = Θ(n<sup>d</sup>). [end of proof]</span></p>
<p><span style="font-size: 19px;"><strong>8. Useful rules</strong></span></p>
<p><span style="font-size: 19px;">There are some useful theorems for limits. Here I present them in a form based on the particular task of comparing two functions of complexity (running time, space, etc).</span></p>
<p><span style="font-size: 19px;">First, L'Hopital's rule:</span></p>
<pre><span style="font-size: 19px;">lim  (f(n)/g(n)) = lim  (f'(n)/g'(n))
n→∞              n→∞
</span></pre>
<p><span style="font-size: 19px;">For example,</span></p>
<pre><span style="font-size: 19px;">lim (ln n / n) = lim ((1/n) / 1) = 0
n→∞             n→∞</span></pre>
<p><span style="font-size: 19px;">Thus ln n = o(n).</span></p>
<p><span style="font-size: 19px;">A more complicated example: Find out the relationship between f(n) = n<sup>n</sup>&nbsp;and g(n) = n!</span></p>
<p><span style="font-size: 19px;">Solution: By Stirling's approximation</span></p>
<p><img src="Pictures/math1.png" alt="math" title="math" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></p>
<p><span style="font-size: 19px;">we have</span></p>
<p><img src="Pictures/factorial.jpg" alt="factorial" title="factorial" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></p>
<p><span style="font-size: 19px;">Therefore, f(n) = ω(g(n)).</span></p>
<p><span style="font-size: 19px;">&nbsp;</span></p>
<p><span style="font-size: 19px;">Two other useful theorems:</span></p>
<p><span style="font-size: 19px;">1. If f(n) &lt;= g(n) for all n &gt; 0, then</span></p>
<pre><span style="font-size: 19px;">lim  f(n) &lt;= lim g(n)
n→∞         n→∞
</span></pre>
<p><span style="font-size: 19px;">2. Squeeze Theorem: If h(n) &lt;= f(n) &lt;= g(n) for all n &gt; 0, and</span></p>
<pre><span style="font-size: 19px;">lim  h(n) = lim g(n)
n→∞        n→∞
</span></pre>
<p><span style="font-size: 19px;">then</span></p>
<pre><span style="font-size: 19px;">lim  f(n) = lim g(n)
n→∞        n→∞
</span></pre>
<p><br><span style="font-size: 19px;">Using the squeeze theorem, we have an alternative, simpler approach that can prove n! = o(n<sup>n</sup>): Since</span></p>
<p><span style="font-size: 19px;">0 &lt;= n! / n<sup>n</sup>&nbsp;= (1/n) (2/n) ... (n/n) &lt;= 1/n</span></p>
<p><span style="font-size: 19px;">lim<sub>n → ∞</sub>0 = lim<sub>n → ∞</sub>(1/n) = 0</span></p>
<p><span style="font-size: 19px;">We have</span></p>
<p><span style="font-size: 19px;">lim<sub>n → ∞</sub>(n! / n<sup>n</sup>) = 0</span></p>

<p><span style="font-size: 19px;">Finally, we can use the following property to break more complicated functions into simpler parts:</span></p>
<p><span style="font-size: 19px;">If f<sub>1</sub>&nbsp;(n) = O(g<sub>1</sub>(n)) and f<sub>2</sub>&nbsp;(n) = O(g<sub>2</sub>(n)), then</span></p>
<p><span style="font-size: 19px;">f<sub>1</sub>&nbsp;(n) + f<sub>2</sub>&nbsp;(n) = O(g<sub>1</sub>(n) + g<sub>2</sub>(n))</span></p>
<p><span style="font-size: 19px;">f<sub>1</sub>&nbsp;(n) f<sub>2</sub>&nbsp;(n) = O(g<sub>1</sub>(n) g<sub>2</sub>(n))</span></p>
<p><span style="font-size: 19px;">Let us use this property to show the relationship between (n<sup>3</sup>) (lg n)<sup>3</sup>&nbsp;(lg lg n) and (n<sup>4</sup>) (lg n). First we have lg lg n = o(lg n). Thus, (lg n)<sup>3</sup>&nbsp;(lg lg n) = o(lg<sup>4</sup>&nbsp;n) = o(n). Therefore, n<sup>3</sup>&nbsp;(lg n)<sup>3</sup>&nbsp;(lg lg n) = n<sup>3</sup>&nbsp;(o(lg<sup>4</sup>&nbsp;n)) = o(n<sup>4</sup>) = o(n<sup>4</sup>&nbsp;lg n).</span></p>
<p><span style="font-size: 19px;"></span></p>

<h2><span style="font-size: 30px;color: rgb(250, 8, 8)">Chapter4</span></h2>
<h2><span style="font-size: 24px;">Algorithm Design and Maximum Subrange Sum</span></h2>
<p><span style="font-size: 19px;">I will now start talking about algorithm design. Before discussing each algorithm design paradigm, I will show different algorithmic solutions to the maximum subrange sum problem, to show good algorithm design is essential.</span></p>
<h3><span style="font-size: 19px;">1. Maximum Subrange Sum</span></h3>
<p><span style="font-size: 19px;">In this problem, the input is an array x of n possibly negative integers. The output of the algorithm for this problem is the maximum sum found in any subarray (possibly empty) of the input.</span></p>
<p><span style="font-size: 19px;">In the following example, n = 10 and the content of x is: 31, -41, 59, 26, -53, 58, 97, -93, -23, 84. The output is the sum of the subarray x[3..7], which is 187.</span></p>
<p><span style="font-size: 19px;">To understand this problem better, let us consider some special cases. First, if all the entries are positive, then the subarray with maximum sum is the entire array x. Second, if all the entries are negative, then the answer is 0, which corresponds to an empty subarray.</span></p>
<p><span style="font-size: 19px;">I described in class that this abstract problem is from practice.</span></p>
<h3><span style="font-size: 19px;">Solution 1 (Brute-force)</span></h3>
<p><span style="font-size: 19px;">In a brute-force solution to this problem we enumerate all possible subarrays, compute the sum of each of them and return the maximum.</span></p>
<p><span style="font-size: 19px;">In the pseudocode below, make sure to distinguish the letter l and the digit 1:</span></p>
<pre><span style="font-size: 19px;">maxsubrangesum1(x[1..n])</span><br><span style="font-size: 19px;">  max ← 0
  for l ← 1 to n do
    for u ← l to n do</span><br><span style="font-size: 19px;">      sum ← 0</span><br><span style="font-size: 19px;">      for i ← l to u do
        sum ← sum + x[i]
        if sum &gt; max then
          max ← sum
  return max</span></pre>
<p><span style="font-size: 19px;">To analyze the running time, we need find out the number of times the statement in the innermost loop is executed. An upper bound would be n<sup>3</sup>. With this it is not difficult to show that the running time is O(n<sup>3</sup>). With greater care, we can show the running time is Θ(n<sup>3</sup>).</span></p>
<h3><span style="font-size: 19px;">Solution 2</span></h3>
<p><span style="font-size: 19px;">To improve the running time, we observe that in the previous solution, we perform a lot of recomputation in the innermost loop. For the example given before, to computer the sum of the subarray x[3..5], we do a loop to get the result 32. Then, in the next iteration, we compute the sum of x[3..6], again using a loop. We can however easily compute x[3..6] as x[3..5] + x[6].</span></p>
<p><span style="font-size: 19px;">We can formalize this observation using the following equation: sum of x[l..u] = sum of x[l..u-1] + x[u].</span></p>
<pre><span style="font-size: 19px;">maxsubrangesum2(x[1..n])</span><br><span style="font-size: 19px;">  max ← 0
  for l ← 1 to n do
    sum ← 0
    for u ← l to n do</span><br><span style="font-size: 19px;">      sum ← sum + x[u]
      if sum &gt; max then
        max ← sum
  return max</span></pre>
<p><span style="font-size: 19px;">The running time is Θ(n<sup>2</sup>).</span></p>
<h3><span style="font-size: 19px;">Solution 3: Another Θ(n<sup>2</sup>)-Time Solution</span></h3>
<p><span style="font-size: 19px;">We describe another Θ(n<sup>2</sup>)-Time Solution, not because it is more efficient than solution 2, but because it makes use of a standard trick which is good to know.</span></p>
<p><span style="font-size: 19px;">In this solution, we first precompute the prefix sum array p[0..n], in which p[i] = x[1] + x[2] + ... + x[i], and p[0] = 0 (sum of the empty prefix). We can perform one loop to compute all the entries of p in O(n) time. With this, the sum of x[l..u] can be computed using x[l..u] = p[u] - p[l-1].</span></p>
<pre><span style="font-size: 19px;">maxsubrangesum3(x[1..n])
  p[0] ← 0
  for i ← 1 to n do
    p[i] ← p[i-1] + x[i]
</span><br><span style="font-size: 19px;">  max ← 0
  for l ← 1 to n do
    for u ← l to n do</span><br><span style="font-size: 19px;">      sum ← p[u] - p[l-1]
      if sum &gt; max then
        max ← sum
  return max</span></pre>
<h3><span style="font-size: 19px;">Solution 4: Applying an Algorithmic Paradigm: Divide-and-Conquer</span></h3>
<p><span style="font-size: 19px;">We learned divide-and-conquer when we learned mergesort in a previous course. Now let's apply this idea here.</span></p>
<p><span style="font-size: 19px;">Our algorithm breaks x into two halves. We can then find the maximum subrange sum in each half in recursive manner. After this, we still need consider subranges that straddle the midpoint. A useful observation is that among these ranges, the one with the maximum sum consists of a maximum-sum suffix of the left half of x, and a maximum-sum prefix of the right half of x. We can construct a proof by contradiction for this.</span></p>
<p><span style="font-size: 19px;">This gives us the following solution; note that we first need a recursive algorithm which solves this problem for array x[l..u].</span></p>
<pre><span style="font-size: 19px;">maxsubrangesum4(x, l, u)
  if l &gt; u then
    return 0
  if l = u then
    return max(0, x[l])

  m ← ⌊(l+u)/2⌋   // midpoint

  suml ← 0  // keep track of the sum of the suffixes of x[l..m]
  maxleft ← 0  
  for i ← m downto l do
    suml ← suml + x[i]
    maxleft ← max(maxleft, suml)

  sumr ← 0  // keep track of the sum of the prefixes of x[m..u]
  maxlright ← 0
  for i ← m+1 to u do
    sumr ← sumr + x[i]
    maxright ← max(maxright, sumr)

  maxa ← maxsubrangesum4(x, l, m)
  maxb ← maxsubrangesum4(x, m+1, u)

  return max(maxa, maxb, maxleft+maxright)</span></pre>
<p><span style="font-size: 19px;">To use this recursive algorithm to solve the maximum subrange sum problem, call</span></p>
<pre><span style="font-size: 19px;">maxsubrangesum4(x, l, n)<br></span></pre>
<p><span style="font-size: 19px;">Let T(n) be the running time. Then we can express T(n) using the following recurrence:</span></p>
<p><span style="font-size: 19px;">T(n) = T(⌈n/2⌉) + T(⌊n/2⌋) + n if n &gt; 1</span><br><span style="font-size: 19px;">T(n) = 1 if n = 1</span></p>
<p><span style="font-size: 19px;">To solve it, </span><span style="font-size: 19px;">we can assume n is a power of 2, which guarantees that the left and right half arrays are of the same size. This is a typical and reasonable assumption. Then, it is not difficult to see the running time satisfies T(n) = 1 if n = 1 and T(n) = 2 T(n/2) + n if n &gt; 1. This recurrence is the same as the recurrence for the running time of mergesort, so the running time of our algorithm is also O(n lg n) (later we will learn how to analyze running time given recurrences).</span></p>


<h3><span style="font-size: 19px;">Solution 5</span></h3>
<p><span style="font-size: 19px;">Suppose that we have found the maximum subrange sum for x[1..j-1]. To find it for x[1..j], there are two cases.</span></p>
<p><span style="font-size: 19px;">In the first case, the subrange with the maximum sum lies entirely within x[1..j-1]. In this case, we already know the answer.</span></p>
<p><span style="font-size: 19px;">In the second case, this subrange ends at x[j]. This means, for this case, we have to find the maximum suffix sum of x[1..j]. If we already know the maximum suffix sum of x[1..j-1], we can add x[j] to that sum. If the result is positive, then it is the maximum suffix sum of x[1..j]. Otherwise, we take 0 as the maximum suffix sum of x[1..j], which corresponds to the empty suffix.</span></p>
<p><span style="font-size: 19px;">The above reasoning gives us a solution that is based on a linear scan of the array, keeping track of the maximum subrange sum and maximum suffix sum in the portion of the array seen so far.</span></p>
<pre><span style="font-size: 19px;">masubrangesum5(x[1..n])</span><br><span style="font-size: 19px;">  maxsofar ← 0 //keep track of the maximum subrange sum in the portion of the array seen so far
  maxsuffixsum ← 0 //keep track of the maximum suffix sum in the portion of the array seen so far
</span><br><span style="font-size: 19px;">  for i ← 1 to n do
    maxsuffixsum ← max(maxsuffixsum + x[i], 0)</span><br><span style="font-size: 19px;">    maxsofar ← max(maxsofar, maxsuffixsum)</span><br><br><span style="font-size: 19px;">  return maxsofar</span></pre>
<p><span style="font-size: 19px;">This example requires only O(n) time.</span></p>
<br>
<p><span style="font-size: 21px;">From these five solutions, we can see that good algorithm design techniques are essential. The following are eight algorithm design paradigms</span></p>
<br>
<h3><span style="font-size: 25px; color: red;"><strong>Algorithm Design Paradigms</strong></span></h3>
<ol>
<li><span style="font-size: 21px;">reduce to know problem</span></li>
<li><span style="font-size: 21px;">recursion</span></li>
<li><span style="font-size: 21px;">divide-and-conquer</span></li>
<li><span style="font-size: 21px;">invent (or augment) a data structure</span></li>
<li><span style="font-size: 21px;">greedy algorithm</span></li>
<li><span style="font-size: 21px;">dynamic programming</span></li>
<li><span style="font-size: 21px;">exploit problem structure (algebraic, geometric, etc.)</span></li>
<li><span style="font-size: 21px;">probabilistic and randomized solution</span></li>
</ol>
<p><span style="font-size: 19px;">&nbsp;</span></p>
<h2><span style="font-size: 24px; color: red;">1-Reduce to Known Problem</span></h2>
<p><span style="font-size: 19px;"> The first algorithm design paradigm that we will discuss is "reduce to known problem". This is to develop an algorithm for a problem by treating this problem as a special case of a problem that we already know how to solve efficiently. For most of the examples that we will see here, we reduce the problems to sorting, which we know how to solve in O(n lg n) time (e.g., mergesort, heapsort).</span></p>
<p><span style="font-size: 19px;">Let's apply this paradigm to solve some problems.</span></p>
<h3><span style="font-size: 19px;">1. Element Distinctness (Uniqueness) Problem</span></h3>
<p><span style="font-size: 19px;">This problem asks us to determine whether the elements in a given array of n numbers are distinct.</span></p>
<p><span style="font-size: 19px;">Solution 1: A brute-force solution is to compare each element to every other element, and this solution requires a double loop. There are n choose 2 pairs of elements to compare. The running time is O(n<sup>2</sup>).</span></p>
<p><span style="font-size: 19px;">Solution 2: Sort the n numbers, and then we walk through the sorted array, comparing each element with the one immediately to its right. If we find any element that is equal to the element immediately to its right, then we have found a repeated element. If not, then the numbers are distinct. Since sorting requires O(n lg n) time and the computation after sorting requires O(n) time, this solution uses O(n lg n) time.</span></p>
<p><span style="font-size: 19px;">It is worth mentioning that the lower bound of the element distinctness problem is Ω(n) in the comparison-based model. What does this mean? To understand this, let's first learn what a comparison-based model is, and then what is the lower bound of a problem. Under the comparison-based model, algorithms can only perform arithmetic calculations and comparisons. They are forbidden to use the value of the input as addresses of memory cells. The above claim about lower bound means that any algorithm that solves this problem in the comparison-based model requires Ω(n lg n) time in the worst case. How to prove this is out of the scope of an introductory course to algorithms, but it is worth knowing the result.</span></p>
<p><span style="font-size: 19px;">Solution 4: This solution is for a special case in which we know that the input values are relatively small integers between 1 and M. In this case, we can make an array B[1..M], and initialize all its entries to 0. Then we scan x, writing a 1 in B[i] if we encounter i during the scan. Before writing, we check if B[i] has a 1 in it already, and if it does, then we have found a repeated element.</span></p>
<p><span style="font-size: 19px;">This requires O(M+n) time and O(M+n) space, and it is good if the numbers are small, e.g., M = O(n).</span></p>
<p><span style="font-size: 19px;">Exercise: how to modify our approach so that we need not initialize the entries of B to 0?</span></p>
<h3><span style="font-size: 19px;">2. Set Intersection</span></h3>
<p><span style="font-size: 19px;">In this problem, we are given two sets of integers (represented as two arrays), and each set is of size n, and we compute the intersection of these two sets.</span></p>
<p><span style="font-size: 19px;">Solution 1: A naive approach is to loop through one array, checking to see if any of its elements is contained in the other. Again, this requires O(n<sup>2</sup>) time.</span></p>
<p><span style="font-size: 19px;">Solution 2: Sort the first array. Then we loop through the second array, and for each element in the second array, we find out whether it exists in the first array using binary search. This requires O(n lg n) time.</span></p>
<h3><span style="font-size: 19px;">3. Collinear Points</span></h3>
<p><span style="font-size: 19px;">In this problem, we are given n points in the plane, and we are to find out whether there are three points that are collinear, i.e., lie along the same line.</span></p>
<p><span style="font-size: 19px;">Solution 1: For each triple of points, say P<sub>1</sub>&nbsp;= (x<sub>1</sub>, y<sub>1</sub>), P<sub>2</sub>&nbsp;= (x<sub>2</sub>, y<sub>2</sub>) and P<sub>3</sub>&nbsp;= (x<sub>3</sub>, y<sub>3</sub>), compute the slopes of the line connection P<sub>1</sub>&nbsp;with P<sub>2</sub>, and the line connecting P<sub>1</sub>&nbsp;with P<sub>3</sub>. If they are equal, then these three points are collinear. We can easily write a loop of three levels to check each triple exactly once. There are n choose 3 triples, and this require Θ(n<sup>3</sup>) time.</span></p>
<p><span style="font-size: 19px;">Solution 2: For each point P, compute the slope of each line defined by P and another point in the given set of points. If there is a duplicate element among all these slopes, then there are collinear points. This way we have reduced the problem of finding whether there is a triple of collinear points involving P to the element distinctness problem. Thus this algorithm runs in O(n<sup>2</sup>&nbsp;lg n) time.</span></p>
<p><span style="font-size: 19px;">Note that no one knows whether the problem of collinear points can be solved in O(n<sup>2</sup>) time, and thus our second solution is quite efficient.</span></p>



<h3><span style="font-size: 19px;">4. Convex Hull</span></h3>
<p><span style="font-size: 19px;">In this problem, we have a set, Q, of points in the plane, given by the x- and y-coordinates of these points. We would like to compute the convex hull of this point set, i.e., the smallest convex polygon P for which each point in Q is either on the boundary of P or in its interior.</span></p>
<p><span style="font-size: 19px;">Since the points in Q could possibly lie along the boundary of P, it is not difficult to see that all the vertices of P are in Q.</span></p>
<p><span style="font-size: 19px;">One way of "computing" the convex hull is to bang nails into a board at the positions where the points are located, put an elastic rubber band around all the nails, and then let go. The law of physics will do the computation and define the convex hull using the rubber band.</span></p>
<p><span style="font-size: 19px;">This problem has many applications in computer graphics and CAD. Many programs compute the convex hull of a complicated shape, and use it to determine whether the user's mouse clicked on it.</span></p>
<p><span style="font-size: 19px;">To study the solutions to convex hull, let's first learn some background knowledge in geometry.</span></p>
<p><span style="font-size: 19px;">Polar angle: The polar angel of a point p with respect to the origin point p<sub>0</sub>&nbsp;is the angle from the x-axis through the origin to the point p<sub>0</sub>, rotating counter clockwise.</span></p>
<p><span style="font-size: 19px;"><img src="Pictures/polar.jpg" alt="polar angle" title="polar angle" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span></p>
<p><span style="font-size: 19px;">In the above example, θ is the polar angle of p with respect to p<sub>0</sub>.</span></p>
<p><span style="font-size: 19px;">Compute the angle between two directed line segments: Let u and v be the vectors representing two directed line segments whose starting endpoints are the the origin point p<sub>0</sub>. How do we compute the angel, θ, between u and v?</span></p>
<p><span style="font-size: 19px;"><img src="Pictures/angle.jpg" alt="angle" title="angle" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span></p>
<p><span style="font-size: 19px;">We can use the following equation to compute θ: u⋅v = ||u|| ||v|| cos θ. Here the dot is the dot product of two vectors. Say, u = (x<sub>1</sub>, y<sub>1</sub>) and v = (x<sub>2</sub>, y<sub>2</sub>), then u⋅v = x<sub>1</sub>x<sub>2</sub>&nbsp;+ y<sub>1</sub>y<sub>2</sub>, and ||u|| is the length of u.</span></p>
<p><span style="font-size: 19px;">Note that we cannot assume arc cos or square root to be a constant-time operation supported by RAM. However, in many algorithms, we just have to compare two angles without computing their actual degrees. Thus, since this equation allows us to compute (cos θ)<sup>2</sup>&nbsp;in constant time, we can make use of it to compare two angles in constant time.</span></p>
<p><span style="font-size: 19px;">Before introducing our algorithms for convex hull, let's make a simplifying assumption that no three points in the input point set are colinear. This is a common assumption; it is not difficult to modify our algorithms to handle the special cases in which there are colinear points (and yes, these special cases should be taken care of in actual implementation), and we make this assumption so that we can focus on explaining algorithmic ideas.</span></p>
<p><span style="font-size: 19px;">A naive approach is to consider the line joined by each pair of points, p<sub>1</sub>&nbsp;and p<sub>2</sub>. Then, if all the other points are on the same side of this line, then the line segment between p<sub>1</sub>&nbsp;and p<sub>2</sub>&nbsp;is an edge of the convex hull.</span></p>
<p><span style="font-size: 19px;">How do we test if all the other points are on the same side of this line? One way of doing this is to first determine the equation of the line, say, Ax+By+C = 0. Then, substitute the coordinates of each of the other points into the left hand side. If the signs of the results are the same, then all the other points are on the same side.</span></p>
<p><span style="font-size: 19px;">It is not difficult to see the correctness of the algorithm, but the running time is O(n<sup>3</sup>). Can we do better?</span></p>
<p><span style="font-size: 19px;"><strong>5. Jarvis's March</strong>&nbsp;(Gift Wrapping): A better algorithm of computing convex hull is Jarvis' march. It is also called gift wrapping as its computational process is similar to the process of wrapping a gift: start from an extreme point and wrap it tightly using a piece of paper.</span></p>
<p><span style="font-size: 19px;">The algorithm is</span></p>
<ol>
<li><span style="font-size: 19px;">Start with p<sub>0</sub>, the point with the minimum y-coordinate, breaking ties arbitrarily. p<sub>0</sub>&nbsp;must be on CH(Q), the convex hull of the input point set Q.</span></li>
<li><span style="font-size: 19px;">The next vertex p<sub>1</sub>&nbsp;on CH(Q) has the smallest polar angel with respect to p<sub>0</sub>, and thus we can find it in linear time. To find the point, p<sub>2</sub>, after p<sub>1</sub>&nbsp;in CH(Q), we further find the point with the smallest polar angel with respect to p<sub>1</sub>, and so on.</span></li>
<li><span style="font-size: 19px;">When we reach the highest point p<sub>k</sub>, we have constructed the right chain of CH(Q).</span></li>
<li><span style="font-size: 19px;">The left chain can be computed in a symmetric way. We start from p<sub>k</sub>&nbsp;and choose p<sub>k+1</sub>&nbsp;as the point with the smallest polar angel with respect to p<sub>k</sub>, but from the negative x-axis, and so on.</span></li>
</ol>
<p><span style="font-size: 19px;"><img src="Pictures/jarvis.jpg" alt="Jarvis's March" title="Jarvis's March" width="799" height="479"></span></p>
<p><span style="font-size: 19px;">If the convex hull has h vertices, then the running time of this algorithm is O(nh), since it uses O(n) time to "march" from one vertex of CH(Q) to the next and there are h vertices. This is O(n<sup>2</sup>) in the worst case.</span></p>
<p><span style="font-size: 19px;"><strong>6. Graham's Scan</strong>: During Jarvis's march, we repeatedly look for the smallest polar angles all the way, so can we just reduce the problem into sorting by polar angle? It is not as simple as this, since we keep changing origin points. The Graham's scan algorithm still manages to reduce it to sorting, in a subtle way. It has the following steps:</span></p>
<ol>
<li><span style="font-size: 19px;">Locate p<sub>0</sub>, the lowest point in Q.</span></li>
<li><span style="font-size: 19px;">Sort all the other points by their polar angle with respect to p<sub>0</sub>. Let p<sub>1</sub>, p<sub>2</sub>, ..., p<sub>n-1</sub>&nbsp;denote the result of sorting.</span></li>
<li><span style="font-size: 19px;">Process the points starting with the one with the smallest polar angle. Add each point in turn with the next higher angle. If doing so causes us to make a left turn, continue. Otherwise, we made a right turn, and the point in the middle of the turn is not in CH(Q). Discard it and join the previous point to the new point. Backtrack, discarding points as you go until you end up making a left turn again.</span></li>
</ol>
<p><span style="font-size: 19px;"><img src="Pictures/graham.jpg" alt="Graham's Scan" title="Graham's Scan" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span></p>
<p><span style="font-size: 19px;">The pseudocode for Step 3 is</span></p>
<pre><span style="font-size: 19px;">s ← an empty stack
PUSH(p<sub>0</sub>, S)
PUSH(p<sub>1</sub>, S)
PUSH(p<sub>2</sub>, S)
for i ← 3 to n-1 do
  while the angle formed by points NEXT-TO-TOP(S), TOP(S), and p<sub>i</sub> makes a nonleft turn then
    POP(S)
  PUSH(p<sub>i</sub>, S)
return S<br><br></span></pre>
<p><span style="font-size: 19px;">In the next lecture, I will talk more about this piece of pseudocode and then analyze the running time.</span></p>
<p>&nbsp;</p>

<p><span style="font-size: 19px;">To analyze the running time of Graham's scan, it is easy to see that Step 1 requires O(n) time and Step 2 requires O(n lg n) time. Step 3 is not obvious. In Step 3, all the lines excluding the while loop requires O(n) time in total. To bound the cost of the while loop, it suffices to find out how many times the POP operation is executed. To find this out, observe that each point is visited at most twice in the pseudocode for Step 3: once when we add it to CH(Q) using PUSH, and possibly the second time when we discard it using POP. Thus the number of times that POP is executed is at most the number of points. Therefore, Step 3 requires O(n) time. The overall running time of Graham's scan is thus O(n lg n).</span></p>
<h1><span style="font-size: 24px; color: red;">3-Divide-and-Conquer</span></h1>
<p><span style="font-size: 19px;">We learned divide-and-conquer in previous courses, and we will learn more about it in this course. The general steps of divide-and-conquer are</span></p>
<ul>
<li><span style="font-size: 19px;">DIVIDE the given problem into a number of subproblems</span></li>
<li><span style="font-size: 19px;">CONQUER the subproblems by solving them recursively</span></li>
<li><span style="font-size: 19px;">COMBINE the solutions to the subproblems into the solution to the original problem</span></li>
</ul>
<p><span style="font-size: 19px;">In this course, we have already seen an example: the 4th solution to the maximum subarray problem.&nbsp;Let T(n) be the running time of that algorithm. Then we can express T(n) using the following recursive formula:</span></p>
<p><span style="font-size: 19px;">T(n) = T(⌈n/2⌉) + T(⌊n/2⌋) + n if n &gt; 1</span><br><span style="font-size: 19px;">T(n) = 1 if n = 1</span></p>
<p><span style="font-size: 19px;">To analyze the running time, assume, for simplicity, that n is a power of 2 (I will explain why this is a reasonable assumption). Then T(n) = 2T(n/2) + n if n &gt; 1.</span></p>
<p><span style="font-size: 19px;">The first approach that I will use to solve the recurrence is called iteration. The idea is to plug the right-hand side of the equation repeatedly into the left-hand side.</span></p>
<p><span style="font-size: 19px;">Assume that n = 2<sup>k</sup>. Then we have</span></p>
<p><span style="font-size: 19px;">T(2<sup>k</sup>) = 2T(2<sup>k-1</sup>) + 2<sup>k</sup></span><br><span style="font-size: 19px;">2T(2<sup>k-1</sup>) = 4T(2<sup>k-2</sup>) + 2<sup>k</sup></span><br><span style="font-size: 19px;">...</span><br><span style="font-size: 19px;">2<sup>k-1</sup>T(2) = 2<sup>k</sup>T(1) + 2<sup>k</sup></span></p>
<p><span style="font-size: 19px;">After summing all these equations, we have T(2<sup>k</sup>) = 2<sup>k</sup>&nbsp;+ k 2<sup>k</sup>&nbsp;= 2<sup>k</sup>(k+1)</span></p>
<p><span style="font-size: 19px;">Therefore, T(n) = n(lg n + 1) = O(n lg n).</span></p>
<p><span style="font-size: 19px;">Now, let's discuss why it is OK, in the analysis of the running time of mergesort, to make the simplifying assumption that n is a power of 2. Even if n is not necessarily a power of 2, we still have the following equation:</span></p>
<p><span style="font-size: 19px;">T(n) = T(2<sup>lg n</sup>) &lt;= T(2<sup>⌈lg n⌉</sup>) = 2<sup>⌈lg n⌉</sup>(⌈lg n⌉ + 1) &lt; 2<sup>lg n + 1</sup>&nbsp;(lg n + 1 + 1) = 2n (lg n + 2) = O(n lg n)</span></p>
<p><span style="font-size: 19px;">Therefore, in such recurrences, we normally ignore floor and ceiling when we solve them to compute the running time.</span></p>
<p><span style="font-size: 19px;">Next, let's using a different approach, namely the substitution method, to analyze running time. Study the following table</span></p>
<pre><span style="font-size: 19px;">n      1    2    4    8   16   32
T(n)   1    4   12   32   80  192
T(n)/n 1    2    3    4    5    6</span></pre>
<p><span style="font-size: 19px;">We can guess that T(n)/n = k + 1, where n = 2<sup>k</sup>.</span></p>
<p><span style="font-size: 19px;">Thus, our guess is T(2<sup>k</sup>) = (k+1)2<sup>k</sup>.</span></p>
<p><span style="font-size: 19px;">Now let's prove it by induction. Clearly it is true for k = 0. Assume it is true for k; we will prove it for k+1. We have</span></p>
<p><span style="font-size: 19px;">T(2<sup>k+1</sup>) = 2T(2<sup>k</sup>) + 2<sup>k+1</sup>&nbsp;= 2(k+1)2<sup>k</sup>&nbsp;+ 2<sup>k+1</sup>&nbsp;= (k+2)2<sup>k+1</sup></span></p>
<h3><span style="font-size: 19px;">&nbsp;</span></h3>
<h3><span style="font-size: 19px;">1. Multiply two n-bit nonnegative numbers</span></h3>
<p><span style="font-size: 19px;">In this problem, we are given two n-bit numbers, and we are to compute the product of these two. Naturally, this makes sense if n is so large that these numbers do not fit in a word. Thus, an array of n entries are used to store the n bits in each of these numbers.</span></p>
<p><span style="font-size: 19px;">If we design an algorithm based on the pencil-and-paper method, it would require O(n<sup>2</sup>) time. Now let's see if we can make any improvement using divide-and-conquer.</span></p>
<p><span style="font-size: 19px;">Let X and Y be the two input n-bit numbers. Assume that the n bits representing X is the concatenation of A and B, and the n bits representing Y is the concatenation of C and D, where A, B, C and D are n/2-bit numbers. If we are to write down pseudocode, we can use arrays of 0/1 digits to represent X, Y, A, B, C and D.</span></p>
<p><span style="font-size: 19px;"><strong>Solution 1</strong>: This solution is based on the following equation:</span></p>
<p><span style="font-size: 19px;">XY = (A 2<sup>n/2</sup>&nbsp;+ B) (C 2<sup>n/2</sup>&nbsp;+ D) = (AC) 2<sup>n</sup>&nbsp;+ (AD + BC) 2<sup>n/2</sup>&nbsp;+ BD</span></p>
<p><span style="font-size: 19px;">This immediately suggests the following idea of doing divide-and-conquer: First, perform four multiplications to compute AC, AD, BC, and BD. That is, solve 4 subproblems of size n/2. Then, use the solutions to these subproblems to compute XY; to do this, we simply perform additions and multiplications by 2<sup>n</sup>&nbsp;(or 2<sup>n/2</sup>), and these multiplications can be performed by appending 0 bits. Thus, we have the following recurrence on running times:</span></p>
<p><span style="font-size: 19px;">T(n) = 4T(n/2) + cn if n &gt; 1, and T(n) = c if n = 1. c is a constant here.</span></p>
<p><span style="font-size: 19px;">We use the iteration approach again.</span></p>
<p><span style="font-size: 19px;">T(2<sup>k</sup>) = 4T(2<sup>k-1</sup>) + c 2<sup>k</sup></span><br><span style="font-size: 19px;">4T(2<sup>k-1</sup>) = 16T(2<sup>k-2</sup>) + 4 c 2<sup>k-1</sup></span><br><span style="font-size: 19px;">...</span><br><span style="font-size: 19px;">4<sup>k-1</sup>T(2) = 4<sup>k</sup>T(1) + 4<sup>k-1</sup>&nbsp;c 2</span></p>
<p><span style="font-size: 19px;">When we sum up, we get T(2<sup>k</sup>) = c 4<sup>k</sup>&nbsp;+ c (2<sup>k</sup>&nbsp;+ 2<sup>k+1</sup>&nbsp;+ ... + 2<sup>2k-1</sup>) = c 4<sup>k</sup>&nbsp;+ c (4<sup>k</sup>&nbsp;- 2<sup>k</sup>)</span></p>
<p><span style="font-size: 19px;">Thus T(n) = (c + c) n<sup>2</sup>&nbsp;- cn = O(n<sup>2</sup>).</span></p>
<p><span style="font-size: 19px;">Thus this solution does not achieve any improvement compared with the pencil-and-paper solution.</span></p>
<p><span style="font-size: 19px;"><strong>Solution 2: Karatsuba's algorithm</strong></span></p>
<p><span style="font-size: 19px;">The second solution is based on the following equation</span></p>
<p><span style="font-size: 19px;">XY = (2<sup>n</sup>&nbsp;+ 2<sup>n/2</sup>) AC + 2<sup>n/2</sup>&nbsp;(A-B)(D-C) + (2<sup>n/2</sup>&nbsp;+ 1) BD</span></p>
<p><span style="font-size: 19px;">You can easily verify the correctness of this equation by expanding it.</span></p>
<p><span style="font-size: 19px;">If we design a divide-and-conquer approach based on this equation, we just have to solve 3 subproblems: the computation of AC, (A-B)(D-C) and BD. Thus the running time is</span></p>
<p><span style="font-size: 19px;">T(n) = 3T(n/2) + cn if n &gt; 1</span><br><span style="font-size: 19px;">T(n) = c if n = 1</span></p>
<p><span style="font-size: 19px;">We can prove this by induction (left as an exercise): T(2<sup>k</sup>) &lt;= c(3<sup>k+1</sup>&nbsp;- 2<sup>k+1</sup>).&nbsp;</span></p>



<p><span style="font-size: 19px;">First I talked about the <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=230447&amp;type=content&amp;rcode=dalhousie-2509488" target="_self">midterm exam</a>.</span></p>
<p><span style="font-size: 19px;">Then, we have</span></p>
<p><span style="font-size: 19px;">T(n) &lt;= c(3<sup>lg n + 1</sup>&nbsp;- 2<sup>lg n +1</sup>) = c(3⋅3<sup>lg n</sup>&nbsp;- 2⋅2<sup>lg n</sup>) = c(3 n<sup>lg 3</sup>&nbsp;- 2n) = O(n<sup>lg 3</sup>).</span></p>
<p><span style="font-size: 19px;">Since lg 3 = 1.58496...., we have T(n) = O(n<sup>1.59</sup>)</span></p>
<p><span style="font-size: 19px;">Some experimental studies showed that Karatsuba's algorithm outperforms the pencil-and-paper algorithm when n exceeds a certain threshold between 320 and 640. Thus it is good for very large integers, which are often used in math, stats, cryptography and astronomy.</span></p>
<br>
<h3><span style="font-size: 24px;color: red;">Master Theorem</span></h3>
<p><span style="font-size: 19px;">So far we have learned a few methods to solve recurrences, including iteration and substitution. Now let's learn a general theorem that will cover many cases of recurrences.</span></p>
<p><span style="font-size: 19px;">Master theorem can be stated as follows:</span></p>
<p><span style="font-size: 19px;">Let a &gt;=1 and b &gt; 1 be constants. Let f(n) be a function. Let T(n) be defined on nonnegative integers by the recurrence T(n) = a T(n/b) + f(n), where we interpret n/b to mean either ⌊n/b⌋ or ⌈n/b⌉. Then</span></p>
<ol>
<li><span style="font-size: 19px;">If f(n) = O(n<sup>log<sub>b</sub>a-ε</sup>) for some constant ε &gt; 0, then T(n) = Θ(n<sup>log<sub>b</sub>a</sup>).</span></li>
<li><span style="font-size: 19px;">If f(n) = Θ(n<sup>log<sub>b</sub>a</sup>lg<sup>k </sup>n) for some constant k &gt;= 0, then T(n) = Θ(n<sup>log<sub>b</sub>a</sup>&nbsp;lg<sup>k+1</sup>n).</span></li>
<li><span style="font-size: 19px;">If f(n) = Ω(n<sup>log<sub>b</sub>a+ε</sup>) for some constant ε &gt; 0, and (regularity condition) if a f(n/b) &lt;= c f(n) for some constant c &lt; 1 and all sufficiently large n, then T(n) = Θ(f(n)).</span></li>
</ol>
<p><span style="font-size: 19px;">Memorize this. Note that there are gaps between different cases, so this theorem does not address all possible cases of recursion. Let's study some examples.</span></p>
<p><span style="font-size: 19px;">1) Karatsuba's algorithm: T(n) = 3T(n/2) + cn. For this a = 3, b = 2, f(n) = cn, and n<sup>log<sub>b</sub>a</sup>&nbsp;= n<sup>lg 3</sup>. Then if we choose some constant ε in (0, lg 3 - 1], we have f(n) = O(n<sup>log<sub>b</sub>a-ε</sup>) for all n. Hence case 1 applies and T(n) = Θ(n<sup>lg 3</sup>).</span></p>
<p><span style="font-size: 19px;">2) The divide-and-conquer solution to maximum subrange sum problem and Mergesort: T(n) = 2T(n/2) + n. It is easy to verify that case 2 applies, and T(n) = Θ(n lg n).</span></p>
<p><span style="font-size: 19px;">3) T(n) = 3T(n/2) + n<sup>2</sup>. Here f(n) = n<sup>2</sup>, and n<sup>log<sub>b</sub>a</sup>&nbsp;= n<sup>lg 3</sup>. It is easy to see that the first conditio of case 3 is satisfied. To verify that case 3 applies, we still have to verify the regularity condition. We have 3f(n/2) = 3(n/2)<sup>2</sup>&nbsp;= (3/4)n<sup>2&nbsp;</sup>&lt;= cn<sup>2</sup>, for c = 3/4. Now we can claim that case 3 applies and T(n) = Θ(n<sup>2</sup>).</span></p>
<p><span style="font-size: 19px;">4) T(n) = 2T(n/2) + Θ(n). The solution would be the same as that to the solution to example 2). It is easy to see why by thinking about the definition of order notation.</span></p>
<p><span style="font-size: 19px;">The solution to T(n) = T(⌈n/2⌉) + T(⌊n/2⌋) + Θ(n) would also be the same, as indicated in the theorem, n/b can be interpreted as its floor/ceiling.</span></p>
<p><span style="font-size: 19px;">The solution to T(n) = 2T(n/2) + O(n) would be O(n lg n) instead of Θ(n lg n) (think about why).</span></p>
<p><span style="font-size: 19px;">5) T(n) = 2T(n/2) + n lg n. Here f(n) = n lg n, and n<sup>log<sub>b</sub>a</sup> = n. Since f(n) = &nbsp;Θ(n<sup>log<sub>b</sub>a</sup>lg<sup>k </sup>n) for k = 1, case 2 applies. Then T(n) = Θ(n lg<sup>2</sup> n).</span></p>
<p><span style="font-size: 19px;">6) T(n) = 2T(n/2) + n / lg n. Here f(n) = n / lg n, and n<sup>log<sub>b</sub>a</sup> = n. Case 3 does not apply since f(n) grows more slowly than n<sup>log<sub>b</sub>a</sup>. Case 2 does not apply because f(n) = &nbsp;Θ(n<sup>log<sub>b</sub>a</sup>lg<sup>k </sup>b) for k = -1, but k cannot be negative in case 2. Could case 1 apply? Apparently it does, but in fact it does not. For any ε &gt; 0, we have lim<sub>n→∞</sub>&nbsp;(f(n)/(n<sup>log<sub>b</sub>a+ε</sup>)) = lim<sub>n→∞</sub> (n<sup>ε </sup>/ lg n) = lim<sub>n→∞</sub> (ε ln 2 n<sup>ε</sup>) = ∞.</span></p>
<p><span style="font-size: 19px;">Thus f(n) = ω(n<sup>log<sub>b</sub>a-ε</sup>) and case 1 cannot apply.</span><span style="font-size: 19px;"></span></p>
<p><span style="font-size: 19px;">Therefore, master theorem does not apply to this recurrence.</span></p>
<p><span style="font-size: 19px;"></span></p>
<br>
<h3><span style="font-size: 23px;">Recursion Tree</span></h3>
<p><span style="font-size: 19px;">How did people prove the master theorem? They did this with the help of recursion tree, which is another approach of solving recurrences.</span></p>
<p><span style="font-size: 19px;">In a recursion tree, each node represents the cost of a single subproblem somewhere in the set of recursive function invocation.</span></p>
<p><span style="font-size: 19px;">Here I will show how to make use of a recursion tree to make a good guess on the running time. Let's take the following recurrence for example:</span></p>
<p><span style="font-size: 19px;">T(n) = 3T(⌊n/4⌋) + c n<sup>2</sup>&nbsp;if n &gt; 1</span><br><span style="font-size: 19px;">T(n) = c if n = 1</span></p>
<p><span style="font-size: 19px;">For simplicity, assume that n is a power of 4. Previously, we showed why similar simplifying assumptions are reasonable. Then the recurrence becomes T(n) = 3T(n/4) + c n<sup>2</sup>.</span></p>
<p><span style="font-size: 19px;">By assigning cost to each node and summing up the costs of the nodes at each level, we have the following figure</span></p>
<p><span style="font-size: 19px;"><img src="Pictures/recursiontree.jpg" alt="Recursion Tree" title="Recursion Tree" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span></p>
<p><span style="font-size: 19px;">We next compute the number, h, of the levels of this tree. The problem size of the root is n, and the problem size of a node at each successive level is 1/4 of that of a node at the level above, and at the leaf level (level h), the problem size is 1. Thus n (1/4)<sup>h-1</sup>&nbsp;= 1, and we have h = log<sub>4</sub>&nbsp;n + 1.</span></p>
<p><span style="font-size: 19px;">The number of leaves is thus 3<sup>h-1</sup>&nbsp;= 3<sup>log<sub>4</sub>n</sup>&nbsp;= n<sup>log<sub>4</sub>3</sup>.</span></p>
<p><span style="font-size: 19px;">Summing up the cost of all levels, we have</span></p>
<p><span style="font-size: 19px;">T(n) = cn<sup>2</sup>&nbsp;+ (3/16)c n<sup>2</sup>&nbsp;+ (3/16)<sup>2</sup>c n<sup>2</sup>&nbsp;+ ... + (3/16)<sup>h-2</sup>c n<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span><br><span style="font-size: 19px;">= Σ<sub>i=0, 1, ..., h-2</sub>(3/16)<sup>i</sup>c n<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span><br><span style="font-size: 19px;">&lt;= Σ<sub>i=0, 1, ..., ∞</sub>(3/16)<sup>i</sup>c n<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span></p>
<p><span style="font-size: 19px;">....We will finish the summation in the next lecture.</span></p>

<p><span style="font-size: 19px;">Summing up the cost of all levels, we have</span></p>
<p><span style="font-size: 19px;">T(n) = cn<sup>2</sup>&nbsp;+ (3/16)c n<sup>2</sup>&nbsp;+ (3/16)<sup>2</sup>c n<sup>2</sup>&nbsp;+ ... + (3/16)<sup>h-2</sup>c n<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span><br><span style="font-size: 19px;">= Σ<sub>i=0, 1, ..., h-2</sub>(3/16)<sup>i</sup>c n<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span><br><span style="font-size: 19px;">&lt;= Σ<sub>i=0, 1, ..., ∞</sub>(3/16)<sup>i</sup>c n<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span><br><span style="font-size: 19px;">= (16/13)cn<sup>2</sup>&nbsp;+ Θ(n<sup>log<sub>4</sub>3</sup>)</span><br><span style="font-size: 19px;">= O(n<sup>2</sup>)</span></p>
<p><span style="font-size: 19px; color: #ffba59;">Students in Section 2: This was when we lost power.</span></p>
<p><span style="font-size: 19px;">We often make use of recursion tree to make a good guess, and then prove the guess using substitution method. For this example, we can easily prove by induction that T(n) &lt;= 2cn<sup>2</sup>. In fact, we can even prove that T(n) &lt;= (16/13)cn<sup>2</sup>.</span></p>
<h3><span style="font-size: 19px;">1. Closest Pair of Points</span></h3>
<p><span style="font-size: 19px;">In this problem, we look for the closest pair of points in a set, P, of n points.</span></p>
<p><span style="font-size: 19px;">The distance between two points (x<sub>1</sub>, y<sub>1</sub>) and (x<sub>2</sub>, y<sub>2</sub>) can be computed as d = √(x<sub>1</sub>-x<sub>2</sub>)<sup>2</sup>+(y<sub>1</sub>-y<sub>2</sub>)<sup>2</sup>. To compare the distance between one pair of points with the distance between another pair, we can compare the square of the distance to avoid the square root operation which cannot be performed in constant time. The brute-force approach will check all the pairs of points and thus will require Θ(n<sup>2</sup>) time.</span></p>
<p><span style="font-size: 19px;">To design a better solution, we use divide and conquer.</span></p>
<p><span style="font-size: 19px;"><strong>DIVIDE</strong>: We split P into two subsets P<sub>L</sub>&nbsp;and P<sub>R</sub>&nbsp;of approximately equal size. That is, |P<sub>L</sub>| = ⌈|P|/2⌉, and |P<sub>R</sub>| = ⌊|P|/2⌋. This can be done by sorting points by x-coordinate, and split P in two with a vertical line L through the median of the x-coordinates. Thus P<sub>L</sub>&nbsp;has points on or to the left of L, and P<sub>R</sub>&nbsp;has points on or to the right of R.</span></p>
<p><span style="font-size: 19px;"><strong>CONQUER</strong>: Find the closest pair of points in P<sub>L</sub>&nbsp;and P<sub>R</sub>&nbsp;recursively. Let δ<sub>L</sub>&nbsp;and δ<sub>R</sub>&nbsp;denote the distances between the closest pairs in P<sub>L</sub>&nbsp;and P<sub>R</sub>&nbsp;respectively. Let δ = min(δ<sub>L</sub>, δ<sub>R</sub>).</span></p>
<p><span style="font-size: 19px;"><strong>COMBINE</strong>: The closest pair is either the pair with distance δ found recursively during the CONQUER step, or a pair of points, p<sub>L</sub>&nbsp;(coordinates (x<sub>1</sub>, y<sub>1</sub>)) and p<sub>R</sub>&nbsp;(coordinates (x<sub>2</sub>, y<sub>2</sub>)), with p<sub>L</sub>&nbsp;in P<sub>L</sub>&nbsp;and p<sub>R</sub>&nbsp;in P<sub>R</sub>, whose distance is less than δ.</span></p>
<p><span style="font-size: 19px;">If the latter exists, we make the following observations:</span></p>
<ol>
  <li><span style="font-size: 19px;">(x<sub>1</sub>-x<sub>2</sub>)<sup>2</sup>&nbsp;+ (y<sub>1</sub>-y<sub>2</sub>)<sup>2</sup>&nbsp;&lt; δ<sup>2</sup>.</span></li>
  <li><span style="font-size: 19px;">|x<sub>1</sub>-x<sub>2</sub>| &lt; δ and |y<sub>1</sub>-y<sub>2</sub>| &lt; δ.</span></li>
  <li><span style="font-size: 19px;">There exists a 2δ ×δ rectangle E centered at line L that contains p<sub>L</sub>&nbsp;and p<sub>R</sub>. See the figure below. The correctness follows from item 2 above: these two points are at most δ units of distance apart vertically, and the distance of neither of them from L could be more than δ.</span><br><span style="font-size: 19px;"><img src="Pictures/recE.jpg" alt="rectangle" title="rectangle" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span></li>
</ol>
<ol start="4">
  <li><span style="font-size: 19px;">The left half of E has at most 4 points in P<sub>L</sub>. These four points are at the four corners of the left half of E. This is true because the distance between any two points in P<sub>L</sub>&nbsp;cannot be less than δ. The right half of E has at most 4 points in P<sub>R</sub>&nbsp;for the same reason. See the figure below.</span><br><span style="font-size: 19px;"><img src="Pictures/recE2.jpg" alt="Rectangle, again" title="Rectangle, again" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span><br><span style="font-size: 19px;">Thus the total number of points of P that reside in E cannot be more than 8. In the figure above, two points are duplicate points (those two on L), but here we are interested in an upper bound which does not have to be tight.</span></li>
</ol>
<p><span style="font-size: 19px;">From these observations, we have the following efficient process of finding p<sub>L</sub>&nbsp;and p<sub>R</sub>:</span></p>
<ol>
  <li><span style="font-size: 19px;">Sort the points in the strip of width 2δ centered at L by y-coordinate. We need not consider points that are not in this strip.</span></li>
  <li><span style="font-size: 19px;">For these points, compare each of them with 7 other points that follow it. This is sufficient for the purpose of finding p<sub>L</sub>&nbsp;and p<sub>R</sub>, since in the worst case, there are at most 6 points between them by observation 4.</span></li>
</ol>
<p><span style="font-size: 19px;">To analyze the running time, we have the following recurrence T(n) = 2T(n/2) + O(n lg n). The O(n lg n) is due to the sorting in both the divide (by x-coordinate) and the combine (by y-coordinate) steps. We solved this recurrence using the master theorem previously and found that T(n) = O(n lg<sup>2</sup>&nbsp;n) time.</span></p>
<p><span style="font-size: 19px;"><strong>Speed-up</strong>: To avoid sorting for each recursive call, at the beginning of the algorithm, we sort all the points by x-coordinate and store them in an array X. We then sort all the points by y-coordinate and store them in an array Y. Then, with care, we will make use of these two arrays to avoid sorting in recursive calls.</span></p>
<p><span style="font-size: 19px;">The content of array X will not change during the recursive calls. In the divide step, we divide X[p..r] (the set P) into X[p..q] (the set P<sub>L</sub>) and X[q+1..r] (the set P<sub>R</sub>), where q = ⌊(p+r)/2⌋.</span></p>
<p><span style="font-size: 19px;">When dividing P into P<sub>L</sub>&nbsp;and P<sub>R</sub>, we split Y into Y<sub>L</sub>&nbsp;and Y<sub>R</sub>&nbsp;by performing a linear scan in P. For each point encountered during this scan, we check whether it is in P<sub>L</sub>&nbsp;or P<sub>R</sub>, and copy it to either Y<sub>L</sub>&nbsp;or Y<sub>R</sub>&nbsp;accordingly. We then use Y<sub>L</sub>&nbsp;and Y<sub>R</sub>&nbsp;as arguments for recursive calls.</span></p>
<p><span style="font-size: 19px;">In the combine step, to sort the points in the strip of width 2δ centered at L, we perform a linear scan in Y and extract these points.</span></p>
<p><span style="font-size: 19px;">The above strategy allows us to perform sorting by x-coordinate once and by y-coordinate once during the entire execution of the algorithm.</span></p>
<p><span style="font-size: 19px;">The running time now has two parts: the part for sorting at the beginning (O(n lg n) time), and the part for the recursive algorithm (T(n) time). We now have T(n) = 2T(n/2) + O(n), and thus T(n) = O(n lg n). The total running time is thus O(n lg n) + T(n) = O(n lg n).</span></p>
<br>
<h2><span style="font-size: 24px;color: red">4-Invent (or Augment) a Data Structure</span></h2>
<p><span style="font-size: 19px;">The 4th algorithm design paradigm is to invent or augment a data structure. Data structures is one filed of algorithms, and we often use data structures as essential building blocks of algorithms. Previously we already saw one example of using data structures to design an algorithm. This is the maxsubrange3 algorithm, discussed in a previous lecture. In this algorithm, we created a prefix sum array p and used it to compute a subrange sum. Now let's see more examples.</span></p>
<h3><span style="font-size: 19px;">1. Abelian Square Detection in Strings</span></h3>
<p><span style="font-size: 19px;">In a string, an Abelian square is a substring of the form xx', where |x| = |x'| (|x| is the length of x), and x is a permutation of x'. For example, the English word reappear is an abelian square, in which x = reap and x' = pear, and so is the DNA fragment AACGTCAGAT.</span></p>
<p><span style="font-size: 19px;">The problem we consider is: Given string S[1..n], determine if it has an abelian square. For example, if S = preappearance, then the answer is true.</span></p>
<p><span style="font-size: 19px;">How do we determine whether x is a permutation of x'? One way of doing this is to sort both strings, and compare the sorted strings symbol by symbol. If the sorted strings are equal, then x is a permutation of x'.&nbsp;</span></p>


<p><span style="font-size: 19px;">How do we determine whether x is a permutation of x'? One way of doing this is to sort both strings, and compare the sorted strings symbol by symbol. If the sorted strings are equal, then x is a permutation of x'. Based on this, we have the following solution:</span></p>
<pre><span style="font-size: 19px;">for len ← 1 to ⌊n/2⌋ do</span><br><span style="font-size: 19px;">  for start ← 1 to n - 2 len + 1 do</span><br><span style="font-size: 19px;">    x ← sort(S[start..start+len-1]</span><br><span style="font-size: 19px;">    y ← sort(S[start+len..start+2len-1]</span><br><span style="font-size: 19px;">    if x = y then</span><br><span style="font-size: 19px;">      return true

return false</span></pre>
<p><span style="font-size: 19px;">The running time is O(n<sup>3</sup>lg n).</span></p>
<p><span style="font-size: 19px;">Can we do better? Let's think of a different way of determining whether x is a permutation of x'. If strings are defined over a finite alphabet Σ of k symbols, then we can tell whether x is a permutation of x' by counting the number of occurrences of each symbol in either string. If each symbol occurs the same number of times in both strings, then x is a permutation of x'. This means we need find a way to compute the number of occurrences of each symbol in any substring fast.</span></p>
<p><span style="font-size: 19px;">To do this, we augment the prefix sum array and define it as a 2D array p[1..k, 0..n], in which p[j, m] stores the number of occurrences of letter j in S[1..m]. We can compute p by a single scan of S from left to right in O(kn) time. This can be used to compute the number of occurrences of any letter in any substring in constant time.</span></p>
<p><span style="font-size: 19px;">Thus, x is a permutation of x' if and only if, for all 1&lt;=j&lt;=k, p[j, start+len-1] - p[j, start-1] = p[j, start+2len-1] - p[j, start+len-1].</span></p>
<p><span style="font-size: 19px;">Based on this, we have the following solution:</span></p>
<pre><span style="font-size: 19px;">for j ← 1 to k do
  p[j, 0] ← 0

for i ← 1 to n do
  for j ← 1 to k do
    if S[i] is the j-th letter of the alphabet then
      p[j, i] ← p[j, i-1] + 1
    else
      p[j, i] ← p[j, i-1]

for len ← 1 to ⌊n/2⌋ do
  for start ← 1 to n - 2 len + 1 do
    j ← 1
    while (j &lt;= k) and (p[j, start+len-1] - p[j, start-1] 
                        = p[j, start+2len-1] - p[j, start+len-1])
      j ← j + 1

    if j = k + 1
      return true

return false</span></pre>
<p><span style="font-size: 19px;">The running time of this solution is O(kn<sup>2</sup>). If k is not too large, this is much better than the previous solution.</span></p>
<h3><span style="font-size: 19px;">2. Supporting Subrange Sum Under Updates</span></h3>
<p><span style="font-size: 19px;">Now let's invent a data structure to solve the following problem: We would like to maintain an array A[1..n] to support</span></p>
<p><span style="font-size: 19px;">Query a: Set A[i] ← A[i] + δ</span><br><span style="font-size: 19px;">Query b: Subrange sum (c, d): Computing A[c] + A[c+1] + ... + A[d]</span></p>
<p><span style="font-size: 19px;">If we simply store the data in the array A, then query a can be supported in O(1) time, but query b requires O(n) time. Alternatively, if we store the data in the prefix array p, then query b can be supported in O(1) time, but query a requires O(n) time. Now let's design a better data structure.</span></p>
<p><span style="font-size: 19px;">In this data structure, each node stores the sum of a subarray. Each leaf corresponds to a subarray of length 1, and the level above corresponds to subarrays of length 2, and so on. The following figure illustrates the data structure for n = 8:</span></p>
<p><img src="Pictures/sumupdate.jpg" alt="sum" title="sum" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></p>
<p><span style="font-size: 19px;">To support query a, we simply update A[i] and all its ancestors, which uses O(lg n) time.</span></p>
<p><span style="font-size: 19px;">To compute the sum of A[c..d], we compute it as the difference between the sums of A[1..d] and A[1..c]. To compute the sum of A[1..d], first initialize a variable named sum to 0. We start from the root, and descend to the highest node whose range ends with d, and add its value to sum. During the top-down traversal, each time we choose the right child, we add the value of the left child (i.e., the left sibling of this child) to sum.</span></p>
<p><span style="font-size: 19px;">For example, using this algorithm, A[1..7] is computed as A[1..4] + A[5..6] + A[7]. A[1..6] is computed as A[1..4] + A[5..6]. A[1..3] is computed as A[1..2] + A[3].</span></p>
<p><span style="font-size: 19px;">As the height of this tree is O(lg n), query b can be answered in O(lg n) time.</span></p>
<p><span style="font-size: 19px;">Therefore, our data structure can support both queries in logarithmic time.</span></p>
<p><span style="font-size: 19px;">An exercise: how to store the tree as an array? Hint: think of heap.</span></p>
<br>
<h1><span style="font-size: 24px;color: red">5-Greedy Algorithms</span></h1>
<p><span style="font-size: 19px;">The 5th algorithm design paradigm discussed in this course is greedy algorithms. They are used to solve optimization problems. For an optimization problem, there exist many solutions that satisfy the constraints given in the problem. Each solution has a value, and we would like to find a solution with the optimal (minimum or maximum) value.</span></p>
<p><span style="font-size: 19px;">The idea of greedy algorithms is to make the choice that looks the best at the moment. More formally, we make a locally optimal choice (i.e., a choice optimal for a subproblem) hoping that this will lead to globally optimal solution. The use of the word "hoping" here implies that a locally optimal choice will not always lead to a globally optimal solution, and thus to master greedy algorithms, we need learn to prove or disprove that a certain greedy strategy will yield an optimal solution.</span></p>
<h3><span style="font-size: 19px;">1. Activity-Selection Problem</span></h3>
<p><span style="font-size: 19px;">In this problem, we are given a set of activities competing for time intervals on a given resource. Here n denotes the number of activities. Activity i has start time s<sub>i</sub>&nbsp;and finish time f<sub>i</sub>. If selected activity i takes up the half open interval [s<sub>i</sub>, f<sub>i</sub>).</span></p>
<p><span style="font-size: 19px;">The goal is to produce a schedule containing the maximum number of compatible activities. We say that activity i is compatible with activity j if either s<sub>i</sub>&nbsp;&gt;= f<sub>j</sub>&nbsp;or s<sub>j</sub>&nbsp;&gt;= f<sub>i</sub>&nbsp;holds.</span></p>
<p><span style="font-size: 19px;">The following figure is an example that I gave in class, in which each interval corresponds to an activity. By choosing the activities circled in green, we get one optimal solution.</span></p>
<p><span style="font-size: 19px;"><img src="Pictures/interval.jpg" alt="activities" title="activities" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span></p>
<p><span style="font-size: 19px;">Let's first see some greedy strategies that do not work.</span></p>
<p><span style="font-size: 19px;">One greedy strategy is to select activity that starts earliest and is compatible with previous choices. To show that it does not always yield an optimal solution, we construct the following counterexample:</span></p>
<pre><span style="font-size: 19px;">i     s<sub>i</sub>    f<sub>i</sub>
1     0     10
2     2     3
3     4     5
4     2     8
5     7     9</span></pre>
<p><span style="font-size: 19px;">The greedy algorithm will only schedule one activity: activity 1. However, it is easy to verify that activities 2, 3 and 5 are mutually compatible and the solution that selects these three activities is even better than the solution given by the greedy algorithm. Therefore, this greedy strategy does not always yield an optimal solution.</span></p>
<p><span style="font-size: 19px;">The following two greedy strategies will not always yield optimal solutions, either, and I asked you to come up with counterexamples as exercise:</span></p>
<ul>
<li><span style="font-size: 19px;">Select activity of shortest duration among remaining ones</span></li>
<li><span style="font-size: 19px;">Select activity with smallest number of conflicts among remaining ones</span></li>
</ul>

<pre><span style="font-size: 19px;">&nbsp;</span></pre>



<p><span style="font-size: 19px;">Now let's come up with a strategy that would work. From the counterexample above, we can see that if an activity ends really late, then choosing it would prevent us from scheduling many activities. This gives us some idea.</span></p>
<p><span style="font-size: 19px;">The following greedy algorithm will compute an optimal solution:</span></p>
<ul>
<li><span style="font-size: 19px;">Sort activities by finish time in increasing order</span></li>
<li><span style="font-size: 19px;">Start with the first, and choose the next possible activity compatible with previous ones</span></li>
</ul>
<p><span style="font-size: 19px;">I obtained the solution to the example in the figure using this strategy.</span></p>
<p><span style="font-size: 19px;">How to prove that this algorithm works? Let's look at a larger example and try to make some observations.</span></p>
<pre><span style="font-size: 19px;">i     s<sub>i</sub>    f<sub>i</sub>
1     1     4
2     3     5
3     0     6
4     5     7
5     3     9
6     5     9
7     6     10
8     8     11
9     8     12
10    2     14
11    12    16</span></pre>
<p><span style="font-size: 19px;">The greedy algorithm will select activities 1, 4, 8, 11.</span></p>
<p><span style="font-size: 19px;">First, let's see some intuition about why the solution computed by the greedy algorithm is optimal. For the above example, let activities (a<sub>1</sub>, a<sub>2</sub>, ..., a<sub>k</sub>) be an optimal answer. If we replace a<sub>1</sub>&nbsp;by activity 1, then (1, a<sub>2</sub>, ..., a<sub>k</sub>) is still an optimal solution, since activity 1 finishes no later than a<sub>1</sub>does, so&nbsp;a<sub>2</sub>, ..., a<sub>k</sub>&nbsp;are compatible with activity 1. We can continue and replace a<sub>2</sub>, ..., a<sub>k</sub>&nbsp;by 4, 8, 11 in the same fashion, and this way we have transformed the optimal answer to the answer given by the greedy algorithm, and they must both have the same number of activities.</span></p>
<p><span style="font-size: 19px;">Now let's formalize the above idea and come up with a correctness proof.</span></p>
<p><span style="font-size: 19px;">Proof. Let (a<sub>1</sub>, a<sub>2</sub>, ..., a<sub>k</sub>) be the answer given by the greedy algorithm, in which a<sub>i</sub>&nbsp;takes place before a<sub>i+1</sub>&nbsp;for all 1 &lt;= i &lt;= k-1. Let (b<sub>1</sub>, b<sub>2</sub>, ..., b<sub>m</sub>) be an optimal answer, in which b<sub>i</sub>&nbsp;takes place before b<sub>i+1</sub>&nbsp;for all 1 &lt;= i &lt;= m-1. We prove that k = m.</span></p>
<p><span style="font-size: 19px;">To prove this, we prove by induction on i that a<sub>i</sub>&nbsp;finishes no later than b<sub>i</sub>. This is sufficient because, with this claim, if k &lt; m, then b<sub>k+1</sub>&nbsp;starts after a<sub>k</sub>&nbsp;finishes, and is thus compatible with a<sub>1</sub>, a<sub>2</sub>, ..., a<sub>k</sub>. The greedy algorithm could have scheduled b<sub>k+1</sub>&nbsp;after a<sub>k</sub>, since b<sub>k+1</sub>&nbsp;finishes after a<sub>k</sub>&nbsp;does and is compatible with previously selected activities, but it didn't. This is a contradiction, so we cannot have k &lt; m.</span></p>
<p><span style="font-size: 19px;">Now let's prove the above claim by induction. The base case holds since a<sub>1</sub>&nbsp;ends no later than any other activity.</span></p>
<p><span style="font-size: 19px;">Assume that the claim holds for i, and we prove it for i+1. Let S be the set of activities compatible with (a<sub>1</sub>, a<sub>2</sub>, ..., a<sub>i</sub>), and S' be the set of activities compatible with (b<sub>1</sub>, b<sub>2</sub>, ..., b<sub>i</sub>). Then S contains all the activities whose starting time is greater than equal to the finish time of a<sub>i</sub>, and S' contains all the activities whose starting time is greater than equal to the finish time of b<sub>i</sub>. By the induction hypothesis, a<sub>i</sub>&nbsp;finishes no later than b<sub>i</sub>. Therefore, S' is a subset of S. Since b<sub>k+1</sub>&nbsp;is in S', it is also in S. By the greedy strategy, a<sub>i+1</sub>&nbsp;has the earliest finish time among activities in S, and thus it finishes no later than b<sub>i+1</sub>.</span></p>
<p><span style="font-size: 19px;">Pseudocode of the greedy algorithm:</span></p>
<pre><span style="font-size: 19px;">Activity_Selection(s, f, n)
sort both s and f by f[i] in increasing order
A ← {1}
k ← 1
for i ← 2 to n do
  if s[i] &gt;= f[k] then
    A ← A ∪ {i}
    k ← i
return A</span></pre>
<p><span style="font-size: 19px;">The time complexity is O(n lg n).</span></p>
<h3><span style="font-size: 19px;">2. Knapsack Problem</span></h3>
<p><span style="font-size: 19px;">In the input, we have n items, and each item i has a weight w<sub>i</sub>&nbsp;(in kg) and a value v<sub>i</sub>&nbsp;(in dollar amount). We also have a knapsack, which has a maximum weight, W, that it can hold.</span></p>
<p><span style="font-size: 19px;">The problem is which items we should choose to maximize the value we can hold in our knapsack.</span></p>
<p><span style="font-size: 19px;">(Think of the example of the thief mentioned in class.)</span></p>
<p><span style="font-size: 19px;">The following is an example (the value/weight ratio is not given as input, but we can compute it easily):</span></p>
<pre><span style="font-size: 19px;">W = 100, n = 6</span><br><span style="font-size: 19px;">item                 1     2     3     4     5     6</span><br><span style="font-size: 19px;">value                80    70    85    40    75    65
weight               25    40    70    15    20    5
value/weight ratio   3.20  1.75  1.21  2.67  3.75  13</span></pre>
<p><span style="font-size: 19px;">The following are four greedy strategies:</span></p>
<ol>
<li><span style="font-size: 19px;">Most valuable item first</span></li>
<li><span style="font-size: 19px;">Heaviest item first</span></li>
<li><span style="font-size: 19px;">Lightest item first</span></li>
<li><span style="font-size: 19px;">Item with the highest value to weight ratio first</span></li>
</ol>
<p><span style="font-size: 19px;">Unfortunately, none of these four strategies works. The above example can be used as a counterexample to show that these strategies do not work.</span></p>
<p><span style="font-size: 19px;">An optimal solution to the above sample input is to select items 1, 2, 5, and 6. The total weight would be 90, and the total value would be 290.</span></p>
<p><span style="font-size: 19px;">If greedy strategy 1 is used, then items 3, 1 and 6 are selected. The total weight is 100, and the value is 230.</span></p>
<p><span style="font-size: 19px;">If greedy strategy 2 is used, it happens that items 3, 1 and 6 are selected again, which is not an optimal solution.</span></p>
<p><span style="font-size: 19px;">If greedy strategy 3 is used, items 6, 4, 5 and 1 are selected. The total weight is 65, and the value is 260.</span></p>
<p><span style="font-size: 19px;">If greedy strategy 4 is used, items 6, 5, 1 and 4 are selected. Again, this is not optimal.</span></p>
<h3><span style="font-size: 19px;">3. Fractional Knapsack Problem</span></h3>
<p><span style="font-size: 19px;">One variant of the knapsack problem can be solved using a greedy algorithm. In this variant, instead of having each item in the input representing a single physical item, each item represents a sack of some substance which can be arbitrarily divided. Examples: a sack of sugar, a sack of salt and a sack of gold dust.</span></p>
<p><span style="font-size: 19px;">Thus, we can take a fraction of an item and put it into our knapsack.</span></p>
<p><span style="font-size: 19px;">Under these assumptions, greedy strategy 4 works! Using this strategy, we first choose items 6, 5, 1, 4 and we can still put 35kg's items into our knapsack. The next item with the highest value to weight ratio is item 2, and its weight is 40 kg. Then we choose 35/40 (=7/8) of item 2, which is worth 7/8 * 70 = 61.25. With this choice, the total weight is 100, and the total value is 321.25.</span></p>
<p><span style="font-size: 19px;">The following is the pseudocode:</span></p>
<pre><span style="font-size: 19px;">greedy(n, v, w, W)</span><br><span style="font-size: 19px;">sort both v and w by the ratio v[i]/w[i] in decreasing order
free ← W
sum ← 0
for i ← 1 to n do
  x ← min(w[i], free)
  sum ← sum + v[i] * (x/w[i])
  free ← free - x
return sum</span></pre>

<p><span style="font-size: 19px;">Now let's give a correctness proof by contradiction. First we consider the case in which v[i]/w[i] are all distinct. Suppose the optimal solution S is better than our greedy solution G. Then S must agree for some number of items (perhaps 0) with G and then differ. Assume that the items have been ordered by v[i]/w[i] in decreasing order. Let item k be the first item of which S and G choose a different amount. Since G chooses the maximum amount of item k that can fit in the knapsack with the previously chosen items, then S must choose less amount of item k than G does. Say G chooses g kg of item k and S chooses s kg of item k. Then we can exchange (g-s) kg of later items that S chooses with (g-s) kg of item k. Doing so would only increase the total value of s, since later items have lower value/weight ratio. Therefore, S is not optimal, which is a contradiction.</span></p>
<p><span style="font-size: 19px;">For the more general case in which v[i]/w[i] are not necessarily distinct, we can group items of the same v[i]/w[i] ratio into the same group, and apply the reasoning above. It would be good exercise for you to work out all the details.</span></p>
<h3><span style="font-size: 19px;">4. Egyptian Fractions</span></h3>
<p><span style="font-size: 19px;">Egyptian fractions were developed in the Middle Kingdom of Egypt, almost 4000 years ago. It is an expression of the form 1/x<sub>1</sub>&nbsp;+ 1/x<sub>2</sub>&nbsp;+ ... + 1/x<sub>n</sub>, in which each x<sub>i</sub>&nbsp;is a distinct positive integer. Thus this expression is a sum of unit fractions. Some examples of expressing fractional numbers using Egyptian Fractions are:</span></p>
<p><span style="font-size: 19px;">5/6 = 1/2 + 1/3</span></p>
<p><span style="font-size: 19px;">47/72 = 1/2 + 1/9 + 1/24</span></p>
<p><span style="font-size: 19px;">Ancient Egyptians were concerned with unit fractions because these helped them to solve problems. For example, a farmer had to pay 72 farmhands with 47 sacks of grain. To distribute these evenly, he had to give each person 47/72 of a sack. Dividing 47 sacks into 72 shares each having 47/72 of a sack was too much work. The farmer however had a much better idea by making use of the equation above.</span></p>
<p><span style="font-size: 19px;">He first took out 36 sacks of grain and divided each sack in half. He gave each person 1/2 of a sack first.</span></p>
<p><span style="font-size: 19px;">Then there are 11 sacks left, and he took 8 sacks, and gave each person 1/9 of a sack.</span></p>
<p><span style="font-size: 19px;">Finally, he divided each of the remaining 3 sacks into 24 equal shares, and gave each person 1/24 of a sack.</span></p>
<p><span style="font-size: 19px;">By then, everyone had 1/2+1/9+1/24=47/72 of a sack, so the division was fair.</span></p>
<p><span style="font-size: 19px;">In 1202, Fibonacci came up with a greedy approach that can be used to express any rational number as the sum of unit fractions. The key idea is to make use of the following equation:</span></p>
<p><span style="font-size: 19px;">x/y = 1/⌈y/x⌉ + ((-y) mod x) / (y ⌈y/x⌉)</span></p>
<p><span style="font-size: 19px;">To verify the correctness of this equation, the following two identities are useful:</span></p>
<p><span style="font-size: 19px;">a mod n = a - n * ⌊a/n⌋</span></p>
<p><span style="font-size: 19px;">⌈x⌉ = -⌊-x⌋</span></p>
<p><span style="font-size: 19px;">Each time we apply this equation, we produce a unit fraction, and the numerator of the second term decreases. Therefore, this greedy algorithm will always terminate. If we use this algorithm to express 47/72, we have</span></p>
<p><span style="font-size: 19px;">47/72 = 1/2 + 1/7 + 1/101 + 1/50904</span></p>
<p><span style="font-size: 19px;">This example shows that the greedy expansion is not necessarily the shortest representation. It does not necessarily have the smallest denominator, either.</span></p>
<p><span style="font-size: 19px;">As an exercise, compute the greedy expansion of 31/311.</span></p>
<p><span style="font-size: 19px;">A related open problem is to prove or disprove the following claim: Every fraction of the form 4/n, in which n is an odd integer, has an Egyptian fraction representation with exactly 3 terms.</span></p>
<p><span style="font-size: 19px;">&nbsp;</span></p>
<h2><span style="font-size: 24px; color: red">6-Dynamic Programming</span></h2>
<p><span style="font-size: 19px;">Dynamic programming was popularized by Richard Bellman, who started to describe it in 1940s. It typically applies to optimization problems. It typically reduces the complexity from exponential, e.g. 2<sup>n</sup>&nbsp;to polynomial, e.g. O(n<sup>3</sup>), O(n<sup>2</sup>) or O(n). This is achieved by avoiding the enumeration of all possibilities.</span></p>
<h3><span style="font-size: 19px;">1. Matrix-Chain Multiplication</span></h3>
<p><span style="font-size: 19px;">To introduce the matrix-chain multiplication problem, I first briefly reviewed how to multiply two matrices (see Appendix of CLRS). From this we see that to multiple two matrices of dimensions m by n and n by p, the number of scalar multiplications is m*p*n.</span></p>
<p><span style="font-size: 19px;">Now let's look at the multiplication of the following three matrices:</span></p>
<pre><span style="font-size: 19px;">matrix dimension</span><br><span style="font-size: 19px;">M<sub>1</sub>     10 x 100
M<sub>2</sub>     100 x 5</span><br><span style="font-size: 19px;">M<sub>3</sub>     5 x 50
</span></pre>
<p><span style="font-size: 19px;">There are two ways of computing M<sub>1</sub>*M<sub>2</sub>*M<sub>3</sub>.</span></p>
<p><span style="font-size: 19px;">If we compute it as M<sub>1</sub>(M<sub>2</sub>M<sub>3</sub>), then the number of scalar multiplications is 100*5*50 + 10*100*50 = 75,000.</span></p>
<p><span style="font-size: 19px;">If we compute it as (M<sub>1</sub>M<sub>2</sub>)M<sub>3</sub>, then the number of scalar multiplications is 10*100*5 + 10*5*50 = 7,500.</span></p>
<p><span style="font-size: 19px;">The difference is a factor of 10!</span></p>
<p><span style="font-size: 19px;">This promotes us to think about the matrix-chain multiplication problem. In this problem, we are given A chain (M<sub>1</sub>, M<sub>2</sub>, ..., M<sub>n</sub>) of n matrices, in which matrix M<sub>i</sub>&nbsp;has dimension p<sub>i-1</sub>&nbsp;* p<sub>i</sub>. The task is to fully parenthesize the product M<sub>1</sub>M<sub>2</sub>...M<sub>n</sub>&nbsp;in a way that minimizes the number of scalar multiplications.</span></p>
<p><span style="font-size: 19px;"><strong>Solution1: Brute-force</strong>: We could use the brute-force approach. That is, we try all possibilities and take the minimum. Then the running time depends on the total number of different ways to multiply n matrices. Let W(n) denote this number.</span></p>
<p><span style="font-size: 19px;">To compute W(n), we make use of the observation that the last multiplication is either M<sub>1</sub>(M<sub>2</sub>...M<sub>n</sub>), or (M<sub>1</sub>M<sub>2</sub>)(M<sub>3</sub>...M<sub>n</sub>), or ..., or (M<sub>1</sub>M<sub>2</sub>...M<sub>n-1</sub>)M<sub>n</sub>.</span></p>
<p><span style="font-size: 19px;">From this observation, we can conclude that W(n) = 1 if n = 1, and W(n) = Σ<sub>1 &lt;= k &lt;= n-1</sub>W(k)W(n-k). </span></p>
<p><span style="font-size: 19px; color: #ffba59;">(Section 1 ended here)</span></p>
<p><span style="font-size: 19px;">Then we can prove by induction that</span></p>
<p><span style="font-size: 19px;">W(n) = (2n-2 choose n-1)/n</span></p>
<p><span style="font-size: 19px;">To see how big this value is, we make use of the fomula for bionmial coefficient (a choose b) = a! / (b! (a-b)!), and Stirling's approximation n! = n<sup>n</sup>e<sup>-n</sup>√2πn(1+Θ(1/n)).</span></p>

<p><span style="font-size: 19px;">To see how big this value is, we make use of the fomula for bionmial coefficient (a choose b) = a! / (b! (a-b)!), and Stirling's approximation n! = n<sup>n</sup>e<sup>-n</sup>√2πn(1+Θ(1/n)).</span></p>
<p><span style="font-size: 19px;">We then have (2n choose n) = (2n)! / (n!)<sup>2</sup>&nbsp;≈ (2n)<sup>2n</sup>e<sup>-2n</sup>√2π 2n&nbsp;/ (n<sup>2n</sup>e<sup>-2n</sup>2πn) = 2<sup>2n</sup>&nbsp;/ √πn.</span></p>
<p><span style="font-size: 19px;">Applying the above equation, we have W(n) ≈ 4<sup>n</sup>/n<sup>3/2</sup>, which is exponential! When n = 20, this is roughly as large as 10<sup>10</sup>.</span></p>
<p></p>
<p><span style="font-size: 19px;"><strong>Dynamic Programming</strong>: We now use dynamic programming to avoid all the recomputation of the recursive approach to acheive a polynomial time solution.</span></p>
<p><span style="font-size: 19px;">We make use of the following observatoin: Suppose the optimal method to compute the product (M<sub>1</sub>M<sub>2</sub>...M<sub>n</sub>) were to first compute the product of M<sub>1</sub>M<sub>2</sub>...M<sub>k</sub>&nbsp;(in some order), then compute M<sub>k+1</sub>M<sub>k+2</sub>...M<sub>n</sub>&nbsp;(in some order), and then multiply these two results together. Then the method used for the computation of M<sub>1</sub>M<sub>2</sub>...M<sub>k</sub>&nbsp;MUST BE OPTIMAL, for otherwise, we could substitute a superior method to compute M<sub>1</sub>M<sub>2</sub>...M<sub>k</sub>&nbsp;and improve the optimal method for the computation of M<sub>1</sub>M<sub>2</sub>...M<sub>n</sub>. Similary, the method used to compute M<sub>k+1</sub>M<sub>k+2</sub>...M<sub>n</sub>&nbsp;must also be optimal. What is left is to find the value of k, and there are n-1 choices.</span></p>
<p><span style="font-size: 19px;">Based on this, we further develop the idea for our algorithm. Let m[i, j] be the optimal cost for computing M<sub>i</sub>M<sub>i+1</sub>...M<sub>j</sub>. Then we have</span></p>
<pre><span style="font-size: 19px;">          / 0 if i = j</span><br><span style="font-size: 19px;">m[i,j] = |</span><br><span style="font-size: 19px;">          \ min<sub> i&lt;=k&lt;j</sub> m[i,k]+m[k+1,j]+p[i-1]p[k]p[j] otherwise
</span></pre>
<p><span style="font-size: 19px;">We can eailsy translate this into a recursive algorithm. However, before I write down its pseudocode, I would emphesize that this algorithm is BAD! It is BAD! Very BAD! No one should ever use such an algorithm. It is however a common mistake, so we need know about it in order to avoid coming up with a similar solution.</span></p>
<pre><span style="font-size: 19px;">Recursive_Multi(i, j)
if i = j then
  return 0
else
  minc ← ∞
  for k ← i to j-1 do
    a ← Recursive_Multi(i, k)
    b ← Recursive_Multi(k+1, j)
    minc ← min(minc, a+b+p[i-1]*p[k]*p[j])
return minc</span></pre>
<p><span style="font-size: 19px;">This algorithm is bad because it essentially checks all possibilities. It is as bad as brute-force. It requires exponential time!</span></p>
<p><span style="font-size: 19px;">To use dynamic programming correctly, we fix an order in which we compute m[i, j]. Here we first compute m[i, i] for all i. Then we compute m[i, i+1] for all i. Then m[i, i+2] for all i, and so on. That is, we compute answers to smaller subproblems first.</span></p>
<pre><span style="font-size: 19px;">Matrix_Multi_Order(p, n)</span><br><span style="font-size: 19px;">for i ← 1 to n do</span><br><span style="font-size: 19px;">  m[i,i] ← 0</span><br><br><span style="font-size: 19px;">for d ← 1 to n-1 do</span><br><span style="font-size: 19px;">  for i ← 1 to n-d do
    j ← i + d
    m[i,j] ← ∞
    for k ← i to j-1 do
      q ← m[i,k] + m[k+1,j] + p[i-1]*p[k]*p[j]
      if q &lt; m[i,j] then
        m[i,j] ← q
        s[i,j] ← k
return (m, s)</span></pre>
<p><span style="font-size: 19px;">From the above code we can see why we compute m[i, j] in the order specified above. This is to guarantee that the values of m[i,k] and m[k+1,j] are available when we compute m[i,j].</span></p>
<p><span style="font-size: 19px;">The running time is O(n<sup>3</sup>), and the optimal cost of multiplying these matrices is stored in m[1, n].</span></p>
<p><span style="font-size: 19px;">To fully understand the order in which the dynamic programming algorithm computes m[i, j], I used the example on pp. 380 (or pp.376 in the 3rd edition) in CLRS. When you review it, think about how entry m[1,4] is calculated.</span></p>
<p><span style="font-size: 19px;">Our previous algorithm computes the optimal cost of multiplying the given matrix chain. How do we multiply the matrices optimally? This can be done using recursion.</span></p>
<p><span style="font-size: 19px;">Say, we wish to multiply M<sub>i</sub>M<sub>i+1</sub>...M<sub>j</sub>&nbsp;optimally, if we know k, then we can compute M<sub>i</sub>M<sub>i+1</sub>...M<sub>k</sub>&nbsp;and M<sub>k+1</sub>M<sub>k+2</sub>...M<sub>j</sub>&nbsp;recursively, and then multiply the products together. How can we find out k? We can simply retrieve its value from s[i, j]. Thus, we can use the following algorithm to multiply matrices optimally after we compute the array s using the previous dynamic programming algorithm:</span></p>
<pre><span style="font-size: 19px;">Matrix_Mult(M, s, i, j)</span><br><span style="font-size: 19px;">if j &gt; i then
  X ← Matrix_Mult(M, s, i, s[i,j])
  Y ← Matrix_Mult(M, s, s[i,j]+1, j)
  return X*Y
else
  return M<sub>i</sub></span></pre>

  <p><span style="font-size: 19px;">We claim that the running time is O(n) + the time to do the actual n-1 matrix multiplications. The latter depends on the size of the matrices. To see that the additional cost (other than the actual matrix multiplication cost) is indeed O(n), we observe that O(n) calls are made and each call uses O(1) time.</span></p>
  <p><span style="font-size: 19px;">Another way of writing down a dynamic programming algorithm is to use memoization. In our solution, the idea is to compute m[1, n] recursively. Each time we compute m[i, j], we check if it has already been computed. If it has, then we retrieve its value. Otherwise, we compute it recursively.</span></p>
  <p><span style="font-size: 19px;">How do we find out if an entry of m has been computed? For this we simply fill all the entries of m using infinity in the beginning.</span></p>
  <pre><span style="font-size: 19px;">Memoized_Matrix_Multi(p, n)</span><br><span style="font-size: 19px;">for i ← 1 to n do
    for j ← 1 to n do
      m[i,j] ← ∞
  return Lookup(p, 1, n)
  
  Lookup(p, i, j)
  1  if m[i,j] &lt; ∞ then
  2    return m[i,j]
  
  3  if i = j then
  4    m[i,j] ← 0</span><br><span style="font-size: 19px;">
  5  else
  6    for k ← i to j-1 do
  7      q ← Lookup(p, i, k) + Lookup(p, k+1, j) + p[i-1]*p[k]*p[j]
  8      if q &lt; m[i,j] then
  9        m[i,j] ← q
  10       s[i,j] ← k
  
  11 return m[i,j]</span></pre>
  <p><span style="font-size: 19px;">To analyze the running time, the crucial part is to analyze the running time for computing Lookup[p, 1, n]. There are two types of calls to Lookup: type-1 call in which line 2 is executed, and type-2 call in which lines 3-11 are executed.</span></p>
  <p></p>
  <p><span style="font-size: 19px;">To analyze the running time, the crucial part is to analyze the running time for computing Lookup[p, 1, n]. There are two types of calls to Lookup: type-1 call in which line 2 is executed, and type-2 call in which lines 3-11 are executed.</span></p>
  <p><span style="font-size: 19px;">Each type-2 call corresponds to an infinity in a different entry of m, and thus the number of type-2 calls is O(n<sup>2</sup>). Since lines 3-10 require O(n) time each time they are executed, the total number of cost for lines 3-10 throughout the execution of the algorithm is O(n<sup>2</sup>*n) = O(n<sup>3</sup>).</span></p>
  <p><span style="font-size: 19px;">Type-1 calls are made as recursive calls by type-2 calls. Since there are O(n<sup>2</sup>) type-2 calls and each of them makes at most n type-1 calls, the total number of type-1 calls is O(n<sup>3</sup>). This means that line 2 is executed O(n<sup>3</sup>) times and this uses O(n<sup>3</sup>) time in total.</span></p>
  <p><span style="font-size: 19px;">The number of times line 1 is executed is equal to the total number of calls to Lookup, which is O(n<sup>3</sup>). This uses O(n<sup>3</sup>) time.</span></p>
  <p><span style="font-size: 19px;">Therefore, the total running time is O(n<sup>3</sup>).</span></p>
  <h3><span style="font-size: 19px;">2. Longest Common Subsequence</span></h3>
  <p><span style="font-size: 19px;">In this problem, we are given two strings: x[1..m] and y[1..n], and out task is to find the longest sequence that appears (not necessarily consecutively) in both strings.</span></p>
  <p><span style="font-size: 19px;">For example, if x = algorithm and y = exploration, then their LCS (longest common subsequence) is lort.</span></p>
  <p><span style="font-size: 19px;">Why do we study this problem? The length of the LCS between two strings is often used as a measurement of their similarity. Thus this has applications in text mining and bioinformatics.</span></p>
  <p><span style="font-size: 19px;">If we solve this problem using a brute-force approach, that is, we enumerate all possible subsequences of x and y and then compute their intersection, the running time would be exponential. The total number of subsequences of x alone is already 2<sup>m</sup>.</span></p>
  <p><span style="font-size: 19px;">Fortunately, we can use dynamic programming to design a polynomial-time solution.</span></p>
  <p><span style="font-size: 19px;">The first step of using dynamic programming to solve LCS is to make an observation to come up with a recursion for the length of the longest common subsequence of x and y.</span></p>
  <p><span style="font-size: 19px;">For this, we look at the last character of either string. There are three cases.</span></p>
  <ol>
  <li><span style="font-size: 19px;">If x[m] = y[n] = a, then any longest common subsequence, z[1..k], of x and y will end in z[k] = a (for otherwise, we could append a to z and get a longer common subsequence). We also claim that z[1..k-1] is a longest common subsequence of x[1..m-1] and y[1..n-1].</span></li>
  <li><span style="font-size: 19px;">If x[m] ≠ y[n], then if z[k] ≠ x[m], we claim that z[1..k] is a longest common subsequence of x[1..m-1] and y[1..n], for if there were a longer longest common subsequence of x[1..m-1] and y[1..n], it would also be a common subsequence of x[1..m] and y[1..n] and is longer than z.</span></li>
  <li><span style="font-size: 19px;">If x[m] ≠ y[n], then if z[k] ≠ y[n], we claim that z[1..k] is a longest common subsequence of x[1..m] and y[1..n-1], for similar reasons.</span></li>
  </ol>
  <p><span style="font-size: 19px;">The next step is to come up with a recursive formula.</span></p>
  <p><span style="font-size: 19px;">Let c[i, j] be the length of the LCS of these two prefixes: x[1..i] and y[1..j]. Then, based on the above observation, we have</span></p>
  <pre><span style="font-size: 19px;">         / 0  if i = 0 or j = 0</span><br><span style="font-size: 19px;">c[i,j] = | c[i-1,j-1] + 1     if x[i] = y[j]</span><br><span style="font-size: 19px;">         \ max(c[i-1,j], c[i,j-1])     if x[i] ≠ y[j]
  
  </span></pre>
  <p><span style="font-size: 19px;">Then we can use the following algorithm to compute all the entries of c, and return the length of LCS of x and y.</span></p>
  <pre><span style="font-size: 19px;">LCS_Length(x[1..m], y[1..n])</span><br><span style="font-size: 19px;">for i ← 1 to m do</span><br><span style="font-size: 19px;">  c[i,0] ← 0</span><br><span style="font-size: 19px;">
  for j ← 1 to n do
    c[0,j] ← 0
  
  for i ← 1 to m do
    for j ← 1 to n do
      if x[i] = y[j] then
        c[i,j] ← c[i-1, j-1] + 1
      else
        c[i,j] ← max(c[i-1,j], c[i,j-1])
  
  return c[m,n]</span></pre>
  <p><span style="font-size: 19px;">In the above algorithm, after filling in the entries for the base case, we compute other entries in increasing order of i. For each i, we compute in increasing order of j. This is important as this order guarantees that the entries of c needed to compute c[i,j] have been computed by the time we look at c[i,j].</span></p>
  <p><span style="font-size: 19px;">As an exercise, I asked you to fill in a 9x 11 table whose entries record the length of LCS between any pairs of prefixes of strings algorithm (string x) and exploration (string y). More precisely, the entry at the ith row and the jth column records the length of LCS between prefix x[1..i] and prefix y[1..j].</span></p>
  
  <p><span style="font-size: 19px;">To compute the LCS itself, the approach in CLRS stores some extra information (the array b on page 394) and then makes use of it to compute LCS recursively. Here we will use a more space-efficient approach which does not require additional data structures. Assume that the array c has already been computed using our dynamic programming algorithm. Then, we can deduce the LCS of x and y from the content of c as follows:</span></p>
  <pre><span style="font-size: 19px;">LCS(i, j)    // A recursive algorithm that computes the LCS of x[1..i] and y[1..j]
  if i = 0 or j = 0 then
    return empty string
  
  else if c[i,j] &gt; max(c[i-1,j], c[i,j-1]) then
         return (LCS(i-1,j-1), x[i])
       else if c[i-1,j] &gt;= c[i,j-1] then
              return LCS(i-1,j)
            else
              return LCS(i,j-1)</span></pre>
  <p><span style="font-size: 19px;">To see the running time of LCS(m, n), observe that each call decreases the values of i, j, or both by 1. Therefore, the number of calls is at most the sum of the initial values of i and j, which is m+n. The work in each call costs constant time, so that total running time is O(m+n).</span></p>
  <p><span style="font-size: 19px;">An exercise is to ask you to use this algorithm to compute the LCS between x = algorithm and y = exploration.</span></p>
  <h3><span style="font-size: 19px;">3. Making Change</span></h3>
  <p><span style="font-size: 19px;">In this problem, we are given an amount in cents. We are to make change using a system of denominations, using the smallest number of coins possible to make change.</span></p>
  <p><span style="font-size: 19px;"><strong>Canadian Coinage System (including pennies) and Greedy Algorithms.</strong>&nbsp;Let's first consider the Canadian coinage system, in which we have toonies, loonies, quarters, dimes, nickels and pennies. We actually "had" pennies up to 2012; here we still include pennies as we are studying the problem of making exact change.</span></p>
  <p><span style="font-size: 19px;">We can model this problem mathematically as follows: We are given an integer n, where 0 &lt;= n &lt; 500, and our goal is to express n = 200a + 100b+ 25c + 10d + 5e + f, where a, b, c, d, e, f are integers &gt;= 0, s.t. a+b+c+d+e+f is minimized.</span></p>
  <p><span style="font-size: 19px;">How do most cashiers make change? They use the following greedy algorithm: Choose as many toonies as possible (s.t. 200a &lt;= n); then for n-200a, choose as many loonies as possible, etc. For example, if the amount of change is $4.87, then a=2, b=0, c=3, d=1, e=0, f=2. In total, 8 coins are used.</span></p>
  <p><span style="font-size: 19px;">This greedy algorithm always computes an optimal solution for the Canadian coinage system. But, this is not necessarily true for any arbitrary coinage system.</span></p>
  <p><span style="font-size: 19px;">For example, if the face values of coins are 12, 5, 1, and the amount of change is 15, then the greedy algorithm will express 15 as 1*12+0*5+3*1, and 4 coins will be used. However, we can use 3 coins to make change, as 15 = 3*5.</span></p>
  <p><span style="font-size: 19px;">How to tell, for a given coinage, whether the greedy algorithm always gives an optimal solution? If you are interested, you can read the following optimal reading material to find out:</span></p>
  <p><span style="font-size: 19px;">A polynomial time algorithm for the change-making problem, Operations Research Letters, 33 (3) 2005, 231-234. <a href="http://www.sciencedirect.com/science/article/pii/S0167637704000823">http://www.sciencedirect.com/science/article/pii/S0167637704000823</a></span></p>
  <p><span style="font-size: 19px;">Another question is, can we design an algorithm that will always compute an optimal solution for any given coinage system? We can indeed design such an algorithm using dynamic programming.</span></p>
  <p><span style="font-size: 19px;"><strong>Dynamic Programming</strong>. To use dynamic programming for the problem of making change, we assume in the input we have the amount (in cents) n &gt; 0, and an array d[1..k] storing the denominations of the k coins, in which d[1] = 1 (so that we can always make the exact change) and d[i] &lt; d[i+1] for all i.</span></p>
  <p><span style="font-size: 19px;">We make the following observation: The optimal method must use some denomination d[i]. If we know which d[i] is used, then we can claim that the optimal solution can be computed by first computing the optimal solution for n-d[i], and then adding one coin of denomination d[i]. We do not know which coin is to be used, so we try all d[i] s.t. d[i] &lt;= n.</span></p>
  <p><span style="font-size: 19px;">This gives us the following recursion, in which opt[j] stores the optimal number of coins to make change for j cents.</span></p>
  <pre><span style="font-size: 19px;">         / 1, if j = d[i] for some i</span><br><span style="font-size: 19px;">opt[j] = |  
           \ 1 + min(opt[j-d[i]), where the min is over all i, s.t. d[i] &lt; j, otherwise.</span></pre>
  <p><span style="font-size: 19px;">We can then write an algorithm to compute opt[j] for j = 1, 2, ..., n.</span></p>
  <pre><span style="font-size: 19px;">Coins(n, d[1..k])</span><br><span style="font-size: 19px;">for j ← 1 to n do
    opt[j] ← ∞
    for i ← k downto 1 do
      if d[i] = j then
        opt[j] ← 1
        largest[j] ← d[i]
      else if d[i] &lt; j then
             a ← 1 + opt[j-d[i]]
             if a &lt; opt[j] then
               opt[j] ← a
               largest[j] ← d[i]
  return opt[n]</span></pre>
  <p><span style="font-size: 19px;">The running time is O(kn).</span></p>
  <p><span style="font-size: 19px;">Let's look at an example. In this example, d = {1, 4, 5} and n = 8. Then, the greedy algorithm would express 8 as 5+1+1+1, and uses 4 coins. If we use dynamic programming, we will compute the entries of opt and largest as follows:</span></p>
  <pre><span style="font-size: 19px;">j     opt[j]     largest[j]</span><br><span style="font-size: 19px;">1          1              1
  2          2              1
  3          3              1
  4          1              4
  5          1              5
  6          2              5
  7          3              5
  8          2              4</span></pre>
  <p><span style="font-size: 19px;">For example, the entry opt[8] is computed as 1 + the minimum of opt[8-5], opt[8-4] and opt[8-1].</span></p>
  <p><span style="font-size: 19px;">How to get the coins used in the optimal solution? Let's look at the above example again. In this example, 8 has a representation of size 2 (=opt[8]), with largest coin 4 (=largest[8]). Then, 8-4=4 has a representation of size 1 (=opt[4]), with largest coin 4 (=largest[4]). From this we can tell that the optimal solution expresses 8 as 4+4.</span></p>
  <p><span style="font-size: 19px;">Based on the above process, we can design an algorithm that will compute the set of coins used in the optimal solution, assuming that the array largest has already been computed using dynamic programming.</span></p>
  <pre><span style="font-size: 19px;">Make_Change(n, largest)</span><br><span style="font-size: 19px;">C ← Φ   // a multiset that stores the answer
  while n &gt; 0 do
    C ← C ∪ largest[n]
    n ← n - largest[n]
  return C
  </span></pre>
  <h2><span style="font-size: 24px; color: red">7-Exploiting the Problem's Structure</span></h2>
  <p><span style="font-size: 19px;">This is the 7th algorithm design paradigm that we study in this course.</span></p>
  <p><span style="font-size: 19px;">Many problems have some sort of structure (algebraic, number-theoretic, topological, geometric, etc.) that suggests a solution, so we design algorithms accordingly to make use of such structure.</span></p>
  <p><span style="font-size: 19px;">As an example, we consider the greatest common divisor problem: Given nonnegative integers u and v, compute their greatest common divisor, gcd(u, v).</span></p>
  <p><span style="font-size: 19px;">First, let us learn a few definitions. d | u means d is a divisor of u. We say d divides u. This means that there exists an integer k, s.t., u = kd. We say that d is a common divisor of u and v if d | u and d | v. We say that d is the largest common divisor of u and v if d is a common divisor of u and v, and no other common divisor is larger. For example, gcd(24, 18) = 6.</span></p>
  <p><span style="font-size: 19px; color: #ffba59;">Section 1 stopped here.</span></p>
  <p><span style="font-size: 19px;">This is useful in algebra. For example, when we reduce a fraction to lowest term, we need the greatest common divisor of the numerator and denominator. Math software would need an algorithm for this problem.</span></p>
  <p><span style="font-size: 19px;">We probably programmed a simple solution to this problem when we learned our first programming language The brute-force approach is to let t = min(u, v), and try numbers t, t-1, t-2, .... The running time is O(min(u, v)). Is this linear time?</span></p>
  
  <p><span style="font-size: 19px;">We probably programmed a simple solution to this problem when we learned our first programming language The brute-force approach is to let t = min(u, v), and try numbers t, t-1, t-2, .... The running time is O(min(u, v)). Is this linear time?</span></p>
  <p><span style="font-size: 19px;">Surprisingly, the answer is no. Recall that when we first started to study running time, we learned that we would study the running time as a function of input size. When determining the size of input for a problem, we use something reasonable. Here, we have two integers, u and v. A reasonable measurement is to use the number of bits required to encode these two integers in binary, which is roughly lg u + lg v (we ignore floor/ceiling here for simplicity). Thus, if the values of u and v are close to each other, then the input size is roughly 2 lg u, and the running time O(u) would be approximately O(2<sup>(input size)/2</sup>). This is exponential!</span></p>
  <p><span style="font-size: 19px;">In cryptography, we often deal with integers that is represented in 128, 256, 512, ... bits. Then, if an algorithm requires 2<sup>256</sup>&nbsp;steps, it would take forever.</span></p>
  <p><span style="font-size: 19px;">To come up with a better solution, let's make some observations. We develop the following simple idea: If d | u and d | v, then d | (au + bv) for all integers a and b. We then choose a and b to minimize au+bv. One way of doing this is to choose a = 1 and b = -⌊u/v⌋ and then au+bv = u - v⌊u/v⌋ = u mod v.</span></p>
  <p><span style="font-size: 19px;">We can prove that d | u &amp; d | v if and only if d | v and d | (u mod v) using the equation u mod v = u - v⌊u/v⌋.</span></p>
  <p><span style="font-size: 19px;">Euclid's algorithm is based on the above structure. It was discovered in 300 B.C.! The pseudocode is simple:</span></p>
  <pre><span style="font-size: 19px;">Euclid(u, v)</span><br><span style="font-size: 19px;">while (v ≠ 0) do
    (u, v) ← (v, u mod v)
  return u</span></pre>
  <p><span style="font-size: 19px;">Finck analyzed this algorithm in 1841. He proved the following theorem: Euclid's algorithm uses O(lg v) steps on input (u, v), where u &gt; v &gt; 0.</span></p>
  <p><span style="font-size: 19px;">Proof. Consider replacing (u, v) by (u, r), where r = u mod v. There are three cases.</span></p>
  <p><span style="font-size: 19px;">Case I: v &gt; u/2. Then r = u - v &lt; u/2.</span></p>
  <p><span style="font-size: 19px;">Case II: v = u/2. Then r = 0;</span></p>
  <p><span style="font-size: 19px;">Case III: v &lt; u/2. Then r &lt; v &lt; u/2.</span></p>
  <p><span style="font-size: 19px;">Hence r &lt; u/2.</span></p>
  <p><span style="font-size: 19px;">It follows that after two steps, say (u, v) ← (v, r) ← (r, s), we have s &lt; v/2, and r &lt; u/2. Therefore, after two steps, v decreases by a factor of 2. After O(lg v) steps, v will decrease to 0. This completes the proof.</span></p>
  <p><span style="font-size: 19px;">With greater care, we can prove that the number of steps &lt;= 2 lg v + 1.</span></p>
  <p><span style="font-size: 19px;">This means that the running time of Euclid's algorithm is linear.</span></p>
  <p><span style="font-size: 19px;">Can we do better? There is an algorithm called binary algorithm that avoids using integer division, and uses subtraction and shifts instead. Its running time does not beat that of Euclid's algorithm asymptotically. It however often uses less time in practice as subtraction/shift are faster than division. It was proposed by J. Stein in 1967, though an approach used in the 1st century of China is essentially equivalent. Here we use Stein's modern description.</span></p>
  <p><span style="font-size: 19px;">This algorithm is based on the following recurrence, in which u &gt;= v:</span></p>
  <pre><span style="font-size: 19px;">            / u, if v = 0
              | 2 * gcd(u/2, v/2), if u and v are both even
  gcd(u, v) = | gcd(u, v/2), if u is odd and v is even
              | gcd(u/2, v), if u is even and v is odd
              \ gcd((u-v)/2, v), if and v are both odd</span></pre>
  <p><span style="font-size: 19px;">This gives us the following pseudocode:</span></p>
  <pre><span style="font-size: 19px;">GCD(u, v)
  if u &lt; v then
    return GCD(v, u)
  if v = 0 then
    return u
  if u and v are both even then
    return 2*GCD(u/2, v/2)
  if u is odd and v is even then
    return GCD(u, v/2)
  if u is even and v is odd then
    return GCD(u/2, v)
  return GCD((u-v)/2, v)</span></pre>
  <p><span style="font-size: 19px;">To see the running time, we first analyze the total number of subtractions, s(u, v). We prove that s(u, v) &lt;= lg(u+v).</span></p>
  <p><span style="font-size: 19px;">Proof: We prove by induction on u+v.</span></p>
  <p><span style="font-size: 19px;">In the base case, u+v = 1. Then either u = 1, v = 0, or u = 0, v = 1. In either case, s(u, v) = 0 and the claim holds.</span></p>
  <p><span style="font-size: 19px;">Now assume the claim holds for u+v &lt; n; we prove it for u + v = n. There are five cases.</span></p>
  <p><span style="font-size: 19px;">If v = 0 or u = 0, s(u, v) = 0 and the claim holds.</span></p>
  <p><span style="font-size: 19px;">If u and v are both even, then s(u, v) = s(u/2, v/2) &lt;= lg(u/2 + v/2) = lg(u+v) - 1 &lt;= lg(u+v).</span></p>
  <p><span style="font-size: 19px;">If u is even and v is odd, then s(u, v) = s(u/2, v) &lt;= lg(u/2 + v) &lt;= lg(u+v).</span></p>
  <p><span style="font-size: 19px;">The case in which u is odd and v is even is similar to the case above.</span></p>
  <p><span style="font-size: 19px;">If u and v are both odd, then s(u, v) = s((u-v)/2, v) + 1 &lt;= lg((u-v)/2 + v) + 1 = lg((u+v)/2) + 1 = lg(u+v).</span></p>
  <p><span style="font-size: 19px;">This completes the proof.</span></p>
  <p><span style="font-size: 19px;">The total number of right shifts is less than or equal to the total umber of bits in both numbers, which is lg u + lg v.</span></p>
  <p><span style="font-size: 19px;">With the above reasonings, we can easily show that the total running time O(max(lg u, lg v)).</span></p>
  <h1><span style="font-size: 24px; color: red;">8-Probabilistic &amp; Randomized Techniques</span></h1>
  <p><span style="font-size: 19px;">The 8th algorithm design paradigm is probabilistic &amp; randomized techniques. To learn this, we first clear some concepts.</span></p>
  <p><span style="font-size: 19px;">In probabilistic analysis, we use knowledge of, or make assumptions about the distribution of input. In probabilistic analysis, we use knowledge of, or make assumptions about the distribution of input. We then average the running time over the distribution of possible inputs and get the average-case running time. One classic example is quicksort. In a previous course, we assumed that the input distribution is uniform, i.e., all input permutations are equally likely, and found out that the average-case running time is O(n lg n). Some of you might have also learned interpolation search, whose average-case running time is O(lg lg n).</span></p>
  <p><span style="font-size: 19px;">Another concept is randomized algorithms. The behavior of randomized algorithms is determined by input and by values produced by a random-number generator. Here "random" means "truly random". That is, each time we ask for a random integer between 1 and n, we get each integer equally likely. The expected running time of a randomized algorithm is the expectation of the running time over the distribution of values returned by the random number generator. Note: NOT over the distribution of input.</span></p>
  <p><span style="font-size: 19px;">Previously we have only learned deterministic algorithms which do not use random number generators. We have also seen how to perform probabilistic analysis, so in this lecture, we will focus on randomized algorithms only.</span></p>
  <h3><span style="font-size: 19px;">1. The Majority Element Problem</span></h3>
  <p><span style="font-size: 19px;">In this problem, we are given an array A[1..n] of integers, in which one integer (the majority element) occurs more than n/2 times, and our tasks is to find this element. This problem has applications in data mining.</span></p>
  <p><span style="font-size: 19px;">There indeed exists a deterministic algorithm that can solve this problem in O(n) time. This is the Boyer-Moore's voting algorithm. It is a subtle algorithm. Dr. Gagie taught it in class. Usually I gave the following link to students and asked them to read it if interested to learn this algorithm:</span></p>
  <p><span style="font-size: 19px;"><a href="http://www.cs.utexas.edu/~moore/best-ideas/mjrty/">http://www.cs.utexas.edu/~moore/best-ideas/mjrty/</a></span></p>
  <p><span style="font-size: 19px;">It was good that Travis taught it in class; this made it easier to understand. To be consistent with previous terms, I will not require that you understand this algorithm for assignments or exams, but it is good to know it. In the next lecture, the guest lecturers will show a simple randomized algorithm.</span></p>
  
  <p><span style="font-size: 19px;">The following is a simple randomized algorithm for this problem:</span></p>
  <pre><span style="font-size: 19px;">Find_Majority(A[1..n])</span><br><span style="font-size: 19px;">while true do
    i ← RANDOM(1, n)
    Get the number, j, of occurrences of A[i] in A[1..n]
    if j &gt; n/2 then
      return A[i]</span></pre>
  <p><span style="font-size: 19px;">To get the expected running time, we first observe that since there is a majority element, the probability of finding it in one try (i.e. one iteration of the while loop) is &gt; 1/2. Thus, the expected number of tries to find it is &lt; 2 (why? we will prove this). Each try uses O(n) time, and thus the expected running time is O(n).</span></p>
  <p><span style="font-size: 19px;">To analyze the expected number of tries, let's first brush up some knowledge in probability theory. The expectation of a random variable X that can take any of the values in a set S is Σ<sub>v in S</sub>P<sub>r</sub>(v) * v. For example, when we toss a die, if the die is fair, the the expected value of the die is 1*(1/6) + 2*(1/6) + ... + 6*(1/6) = 3.5.</span></p>
  <p><span style="font-size: 19px;">Now let's analyze the expected number of tries. In each try, we guess an index of the array, and we find the majority with probability p &gt; 1/2. (Here p is equal to the number of occurrences of the majority divided by n.) Let x be the expected number of tries. Then x = p*1 + (1-p)*p*2 + (1-p)<sup>2</sup>*p*3 + ... = Σ<sub>i&gt;=1</sub>(1-p)<sup>i-1</sup>*p*i.</span></p>
  <p><span style="font-size: 19px;">Let y = Σ<sub>i&gt;=1</sub>(1-p)<sup>i</sup>*p. Then y= (1-p)p/(1-(1-p)) = 1-p.</span></p>
  <p><span style="font-size: 19px;">We have x*(1-p) + y = Σ<sub>i&gt;=1</sub>(1-p)<sup>i</sup>*p*i + Σ<sub>i&gt;=1</sub>(1-p)<sup>i</sup>*p = Σ<sub>i&gt;=1</sub>(1-p)<sup>i</sup>*p*(i+1) = x - p.</span></p>
  <p><span style="font-size: 19px;">Plugging in the value of y, we have x*(1-p) + (1-p) = x - p. Solving this equation, we have x = 1/p. This is indeed less than 2.</span></p>
  <p><span style="font-size: 19px;">So far we have shown that the above randomized algorithm uses O(n) expected time, which is efficient. There are however some issues with this solution. First, this solution is not robust: If there is no majority element, then it will loop forever. For some applications, this issue can be resolved in the following way: Terminate the while loop if it has been iterated k times for a certain value k. If no majority element has been found yet, then we can claim that, if there is indeed a majority element, then the probabilities of our algorithm not finding it is less than 1/(2<sup>k</sup>). This is acceptable in many applications in data mining, but may not always be acceptable. Another issue is that it may take a long time, though this is rare.</span></p>
  <p><span style="font-size: 19px;">Despite these issues, randomized algorithms are often the most practical solutions. For example, if you learn more about computational geometry in the future, you will learn a randomized solution to the point location problem, which is the most practical one.</span></p>
  <h3><span style="font-size: 19px;">2. The Selection Problem</span></h3>
  <p><span style="font-size: 19px;">In the selection problem, we are given an array A[1..n] and an integer i with 1 &lt;= i &lt;= n, and our task is to determine the i-th smallest number in A.</span></p>
  <p><span style="font-size: 19px;">When i = 1, n, or ⌈n/2⌉, the problem becomes the problem of finding min, max, or median.</span></p>
  <p><span style="font-size: 19px;">One obvious approach is to sort the array and take the i-th element in the sorted array. This would require O(n lg n) time. Can we do better?</span></p>
  <p><span style="font-size: 19px;">Now, let's study the following randomized algorithm for selection. It consists of two parts. First, a randomized algorithm that partitions the array A[l..u] into two parts (note that this is character l, not digit 1).</span></p>
  <pre><span style="font-size: 19px;">Randomized_Partition(A, l, u)
  i) Choose a random index i' from [l..u]. Assign v ← A[i'].
  ii) Rearrange the elements of A[l..u] so that there exists an integer p s.t.
      elements in A[l..p-1] are &lt;= v, 
      A[p] = v (i.e. v is moved from A[i'] to A[p]), and
      elements in A[p+1..u] are &gt;= v.
  iii) return p</span></pre>
  <p><span style="font-size: 19px;">This is similar to the partition step for quicksort, though in the basic version of quicksort, we do not choose a random element to partition the array. Instead, we choose, say, the last element. Similar to the partition step in quicksort, this can be performed in time linear in the length of the subarray A[l..u].</span></p>
  <p><span style="font-size: 19px;">With this, we can introduce the randomized selection algorithm.</span></p>
  <pre><span style="font-size: 19px;">Randomized_Select(A, l, u, i) // find the ith smallest element from A[l..u]
  if l = r then
    return A[l]
  
  p ← Randomized_Partition(A, l, u)
  k ← p - l + 1 // note that a[p] is the k-th smallest element in A[l..u]
  
  if k = i then
    return A[p]
  else if i &lt; k then
         return Randomized_Select(A, l, p-1, i)
       else
         return Randomized_Select(A, p+1, u, i-k)</span></pre>
  <p><span style="font-size: 19px;">To analyze the expected running time T(n), we observe that Randomized_Select calls itself on either an array of size k-1, or an array of size n-k. In the worst case, we fall in the larger of these two partitions. Thus we have</span></p>
  <p><span style="font-size: 19px;">T(n) &lt;= ((1/n) * Σ<sub>k=1,2,...,n</sub>T(max(k-1, n-k))) + an, for some constant a.</span></p>
  <p><span style="font-size: 19px; color: #ffba59;">Section 2 ended here</span></p>
  <p><span style="font-size: 19px;">We prove that T(n) &lt;= cn for some constant c by induction on n.</span></p>
  <p><span style="font-size: 19px;">Assume it is true for some base case. We will figure out what the base case should be later.</span></p>
  <p><span style="font-size: 19px;">Assume it is true for n' &lt; n; we now prove it for n. We have</span></p>
  <p><span style="font-size: 19px;">T(n) &lt;= ((1/n) * Σ<sub>k=1,2,...,n</sub>T(max(k-1, n-k))) + an</span></p>
  <p><span style="font-size: 19px;">&lt;= ((2/n)* Σ<sub>k=⌊n/2⌋,⌊n/2⌋+1,...,n</sub>T(k)) + an</span><br><br><span style="font-size: 19px;">&lt;= (2/n)* Σ<sub>k=⌊n/2⌋,⌊n/2⌋+1,...,n</sub>ck + an</span></p>
  <p><span style="font-size: 19px;">= (2c/n)* Σ<sub>k=⌊n/2⌋,⌊n/2⌋+1,...,n</sub>k + an</span></p>
  <p><span style="font-size: 19px;">= (2c/n)* ( Σ<sub>k=1,2,...,n</sub>k - Σ<sub>k=1,2,...,⌊n/2⌋-1</sub>k) + an</span></p>
  <p><span style="font-size: 19px;">= (2c/n)*(n(n-1)/2 - ⌊n/2⌋(⌊n/2⌋-1)/2) + an</span></p>
  <p><span style="font-size: 19px;">&lt;= (2c/n)*(n(n-1)/2 - (n/2-1)(n/2-2)/2) + an</span></p>
  <p><span style="font-size: 19px;">= (c/n)((3/4)n<sup>2</sup>&nbsp;+ (1/2)n - 2) + an</span></p>
  <p><span style="font-size: 19px;">&lt;= 3cn/4 + c/2 + an</span></p>
  <p><span style="font-size: 19px;">We need choose c s.t. 3cn/4 + c/2 + an &lt;= cn</span></p>
  <p><span style="font-size: 19px;">Simplifying, we have T(n) &lt;= cn provided n &gt;= 2c/(c-4a).</span></p>
  <p><span style="font-size: 19px;">Choose c &gt;= 12a. Then 2c/(c-4a) = 2/(1-4a/c) &lt;= 3.</span></p>
  <p><span style="font-size: 19px;">Thus, T(n) &lt;= cn provided n &gt;= 3.</span></p>
  <p><span style="font-size: 19px;">To make the base case work, we need ensure that T(n) &lt;= cn for n = 1, 2. This can be done by choosing c = max(12a, T(1), T(2)/2).</span></p>
  <p><span style="font-size: 19px;">This completes the proof.</span></p>
  <p><span style="font-size: 19px;">Thus, the randomized selection algorithm uses expected linear time.</span></p>
  <p><span style="font-size: 19px;">Is there a deterministic algorithm for selection that uses linear worst-case time? Yes. If you are interested, you can read the following optional reading material: Section 9.3 in CLRS. The determinist linear-time algorithm is interesting but fairly complicated. The randomized algorithm above is better in practice.</span></p>
  <h2><span style="font-size: 24px; color: red;">9-Graph Algorithms</span></h2>
  <p><span style="font-size: 19px;">In previous lectures, we learned 8 algorithm design paradigms, which gives us a rich toolset for problem solving. Now we use some of these paradigms to solve problems in graphs.</span></p>
  <p><span style="font-size: 19px;">We learned graphs in CSCI 2110 before, and also in discrete math. I will first review some definitions and graph representations briefly; you have seen most of these in 2110. Then I will focus on the problems that can be solved using paradigms learned in this course.</span></p>
  <p><span style="font-size: 19px;">As we learned before, a graph is a discrete structure. It is an abstract representation of a set of objects where some pairs of objects are connected by links. We call each object a vertex or node, and each link an edge. Thus, formally, a graph G can be defined as a pair (V, E), in which V is a set of vertices, and E is a set of edges, and E is a subset of V x V. The figure below is the graph that I drew in class.</span></p>
  <p><span style="font-size: 19px;"><img src="Pictures/graph.jpg" alt="graph" title="graph" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></span></p>
  <p><span style="font-size: 19px;">In a directed graph, each edge has a direction. When we draw a directed graph, we will use arrows to orient edges to show the direction. We say that edge (u, v) is incident from (leaves) vertex u and is incident to (enters) vertex v. Or, simply edge (u, v) is from u to v.</span></p>
  <p><span style="font-size: 19px;">In an undirected graph (as the example above), edges do not have directions. Edge (u, v) is incident on u and v. We say vertex u is adjacent to v if (u, v) is in E.</span></p>
  <p><span style="font-size: 19px;">We often use graphs to model relationships. Some examples are:</span></p>
  <ol>
  <li><span style="font-size: 19px;">A vertex could be a country on the world map, and an edge can be used to join two countries sharing the same border.</span></li>
  <li><span style="font-size: 19px;">Vertices could be used to represent cities and edges could join cities if there is a direct flight between these two cities.</span></li>
  <li><span style="font-size: 19px;">Vertices could be electronic components, and an edge would join two components that have a wire connecting them.</span></li>
  <li><span style="font-size: 19px;">Those social websites such as linked in are essentially based on social graphs, in which each vertex represents a person and an edge joins two persons who know each other.</span></li>
  </ol>
  <p><span style="font-size: 19px;">Now, let's quickly go over some definitions.</span></p>
  <p><span style="font-size: 19px;">The degree of a vertex in an undirected graph is the number of edges incident on it. In the example above, the degree of vertex 1 is 3. In a directed graph, the in-degree of a vertex is the number of edges entering it, and its out-degree is the number of edges leaving it.</span></p>
  <p><span style="font-size: 19px;">A loop, or self-loop, is an edge from a vertex to itself. A multi-edge is two or more edges incident to the same pair of vertices (we do not study multi-edge in this course). A simple graph is a graph that has no loops or multi-edges.</span></p>
  <p><span style="font-size: 19px;">A path is a series of vertices in which each vertex is connected to the previous vertex by an edge. One example for the figure above is the path 1, 2, 4, 1, 5. The length of a path is the number of edges in the path. In this example, the length is 4. A simple path is a path without repeated vertices or edges. For example, path 1, 2, 4, 5.</span></p>
  <p><span style="font-size: 19px;">A cycle is a path whose first vertex is the same as the last vertex. One example is 1, 2, 4, 5, 1. A cycle is simple if, except the repetition of the start and end vertices, there are no repeated vertices or edges.</span></p>

<p><span style="font-size: 19px;">An undirected graph is connected if there is a path between any two vertices.</span></p>
<p><span style="font-size: 19px;">A tree is a special type of graph. In particular, it is a connected, acyclic graph. If we pick one node of the tree as the root, then we call it a rooted tree. If not, it is an unrooted tree.</span></p>
<h3><span style="font-size: 19px;">1. Graph Representations</span></h3>
<p><span style="font-size: 19px;">There are two common approaches of representing a graph.</span></p>
<p><span style="font-size: 19px;">In the adjacency matrix representation, we represent a graph G = (V, E) as a |V| x |V| matrix A, in which A[i, j] = 1 if (i, j) is in E, and A[i, j] = 0 otherwise. Take the graph shown in the previous lecture as an example, its adjacency matrix representation is:</span></p>
<table border="1" width="200">
  <tbody>
  <tr>
    <td><span style="font-size: 19px;">0</span></td>
    <td><span style="font-size: 19px;">1</span></td>
    <td><span style="font-size: 19px;">0</span></td>
    <td><span style="font-size: 19px;">1</span></td>
    <td><span style="font-size: 19px;">1</span></td>
  </tr>
  <tr>
    <td><span style="font-size: 19px;">1</span></td>
    <td><span style="font-size: 19px;">0</span></td>
    <td><span style="font-size: 19px;">1</span></td>
    <td><span style="font-size: 19px;">1</span></td>
    <td><span style="font-size: 19px;">0</span></td>
  </tr>
  <tr>
    <td><span style="font-size: 19px;">0</span></td>
    <td><span style="font-size: 19px;">1</span></td>
    <td><span style="font-size: 19px;">0</span></td>
    <td><span style="font-size: 19px;">1</span></td>
    <td><span style="font-size: 19px;">0</span></td>
  </tr>
  <tr>
    <td><span style="font-size: 19px;">1</span></td>
    <td><span style="font-size: 19px;">1</span></td>
    <td><span style="font-size: 19px;">1</span></td>
    <td><span style="font-size: 19px;">0</span></td>
    <td><span style="font-size: 19px;">1</span></td>
  </tr>
  <tr>
    <td><span style="font-size: 19px;">1</span></td>
    <td><span style="font-size: 19px;">0</span></td>
    <td><span style="font-size: 19px;">0</span></td>
    <td><span style="font-size: 19px;">1</span></td>
    <td><span style="font-size: 19px;">0</span></td>
  </tr>
  </tbody>
</table>
<p><span style="font-size: 19px;">If a graph is directed, then the number of 1's in its adjacency matrix is equal to the number of edges, |E|. If a graph is undirected and has no loops, then the number of 1's is equal to 2|E|.</span></p>
<p><span style="font-size: 19px;">The adjacency list represents a graph as an array, Adj[1..|V|], of lists. Adj[u] is a list that contains all the vertices v s.t. (u, v) is in E. Take the graph shown in the previous lecture as an example, its adjacency list representation is:</span></p>
<p><img src="Pictures/adjlist.jpg" alt="adjacency list" title="adjacency list" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></p>
<p><span style="font-size: 19px;">&nbsp;</span></p>
<p><span style="font-size: 19px;">To compare these two representations, we have</span></p>
<table border="1" width="467">
  <tbody>
  <tr>
    <td width="102"><span style="font-size: 19px;">&nbsp;</span></td>
    <td width="101"><span style="font-size: 19px;">space</span></td>
    <td width="95"><span style="font-size: 19px;">adjacency test</span></td>
    <td width="141"><span style="font-size: 19px;">list neighbors of u</span></td>
  </tr>
  <tr>
    <td><span style="font-size: 19px;">adjacency matrix</span></td>
    <td><span style="font-size: 19px;">O(|V|<sup>2</sup>)</span></td>
    <td><span style="font-size: 19px;">O(1)</span></td>
    <td><span style="font-size: 19px;">O(|V|)</span></td>
  </tr>
  <tr>
    <td><span style="font-size: 19px;">adjacency list</span></td>
    <td><span style="font-size: 19px;">O(|V|+|E|)</span></td>
    <td><span style="font-size: 19px;">O(|V|)</span></td>
    <td><span style="font-size: 19px;">O(d(u))</span></td>
  </tr>
  </tbody>
</table>
<p><span style="font-size: 19px;">Thus adjacency matrix is more efficient for adjacency test, while adjacency list is more efficient when listing the neighbors of a give node. When space usage is a concern, then, if the graph is sparse (informally, if the number of edges is much smaller than |V|<sup>2</sup>), then adjacency list uses less space.</span></p>
<h3><span style="font-size: 19px;">2. Weighted Graphs</span></h3>
<p><span style="font-size: 19px;">When we assign a weight to each edge, then we have a weighted graph. We can modify the above two representations to represent a weighted graph.</span></p>
<p><span style="font-size: 19px;">In the adjacency matrix, A[i, j] stores w if edge (i, j) has weight w, and ∞ if there is no edge from i to j.</span></p>
<p><span style="font-size: 19px;">In the adjacency list, we can store weights together with vertices in a list.</span></p>
<h3><span style="font-size: 19px;">3. Graph Traversals</span></h3>
<p><span style="font-size: 19px;">In 2110, we learned two algorithms using which we can traverse all the vertices of a graph. One is breadth-first search, which makes uses of a queue. The other is depth-first search, which is either recursive, or makes use of a stack. They both require O(|V| + |E|) time. Now is a good time to review these two algorithms, and you can find them in Chapters 20.2 and 20.3 in CLRS (22.2 &amp; 22.3 in 3rd ed).</span></p>
<h3><span style="font-size: 19px;">4. Minimum Spanning Trees</span></h3>
<p><span style="font-size: 19px;">The first problem that we discuss here is the problem of finding a minimum spanning tree of a graph.</span></p>
<p><span style="font-size: 19px;">To describe this problem, let's first learn what a spanning tree is. A spanning tree is an (unrooted) tree that connects all the vertices of a connected, undirected graph. For the graph given in the previous lecture, one minimum spanning tree is:</span></p>
<p><img src="Pictures/spanningtree.jpg" alt="spanning tree" title="spanning tree" data-d2l-editor-default-img-style="true" style="max-width: 100%;"></p>
<p><span style="font-size: 19px;">Every connected graph has at least one spanning tree.</span></p>
<p><span style="font-size: 19px;">If there are weights on edges, then the minimum spanning tree (MST) is the spanning tree that minimizes the sum of the weights of the edges used.</span></p>
<p><span style="font-size: 19px;">We now discuss two algorithms that can be used to compute a MST of a given weighted graph. These two algorithms are both greedy algorithms.</span></p>
<h3><span style="font-size: 19px;">5. Kruskal's Algorithm</span></h3>
<p><span style="font-size: 19px;">In Kruskal's algorithm, at each step, we choose the minimum-weight edge that does not create a cycle.</span></p>
<p><span style="font-size: 19px;">More precisely, in this algorithm, we work with the following two sets:</span></p>
<ul>
  <li><span style="font-size: 19px;">A forest (i.e., a set of trees). Initially, each vertex of the graph is a tree by itself in this forest.</span></li>
  <li><span style="font-size: 19px;">A set of edges E'. Initially, E' = E, i.e., E' holds all the edges of the graph.</span></li>
</ul>
<p><span style="font-size: 19px;">At each step, we remove and consider a minimum-weight edge from E', breaking ties arbitrarily. If this edge connects two different trees in the forest, then add it to the forest, joining two trees. Otherwise, it connects two edges from the same tree, forming a cycle. In this case, we simply discard this edge. When E' is empty, the forest becomes a MST.</span></p>
<p><span style="font-size: 19px;">In class, I walked through an example of using Kruskal's algorithm to compute a MST. A different example can be found on pp. 632 in the textbook. When reviewing this part, it is sufficient to study either one of these two examples.</span></p>
<p><span style="font-size: 19px;">Now let's argue the correctness of Kruskal's algorithm.</span></p>
<p><span style="font-size: 19px;">Proof. First, we prove that the algorithm produces a spanning tree. The resulting graph produced by Kruskal's algorithm cannot have a cycle, because if it did, the edge added that forms the cycle would join together two vertices in the same tree, and this is not possible by the algorithm.</span></p>
<p><span style="font-size: 19px;">We also claim that the resulting graph must be connected, for if it weren't, when we considered the edge that connected two components, we would have added it, but we didn't.</span></p>
<p><span style="font-size: 19px;">Now we prove that the spanning tree, T, produced by Kruskal's algorithm is minimum. Here we give a proof by contradiction.</span></p>
<p><span style="font-size: 19px;">Assume to the contrary that T is not a MST. Among all the MST's of the graph G, let T' be the one that share the largest number of edges with T. Consider the edge e in T-T' which was the first edge added to T by Kruskal's algorithm.</span></p>
<p><span style="font-size: 19px;">Because T' is a spanning tree, adding e into T' would create a cycle. Let C be the cycle in T' ∪ {e}. Since T is a tree, it cannot contain all the edges in C. Let f be an edge in C that is not in T.</span></p>
<p><span style="font-size: 19px;">Now consider T" = (T'∪ {e}) - {f}, i.e., the tree obtained from T' by adding edge e and removing edge f. T" is also a spanning tree. Since T' is minimum, we have weight(e) &gt;= weight(f), for otherwise, T" would have a smaller total weight than T', but T' is a MST.</span></p>
<p><span style="font-size: 19px;">If weight(f) &lt; weight(e), then Kruskal's algorithm would have considered edge f before edge e. As e was the first edge of T that is not in T', all the edges added before f are also in T'. Since f is in T', adding it would not create a cycle. Thus Kruskal's algorithm would have added f. As f was not added, this means weight(f) &gt;= weight(e).</span></p>
<p><span style="font-size: 19px;">Therefore, weight(f) = weight(e). Thus the total weight of T" is equal to the total weight of T', which means that T" is also a MST. We also observe that T" shares one more edge with T than T' does, which contradicts the assumption that T' is the MST that shares the largest number of edges with T.</span></p>
<p><span style="font-size: 19px;">This completes the proof.</span></p>
<p><span style="font-size: 19px;">How do we implement Kruskal's algorithm? Naturally we would sort edges by weight. .When considering edge (u, v), how can we decide whether they are in the same tree? For this, we apply another algorithm design paradigm: invent or augment a data structure.</span></p>
<p><span style="font-size: 19px;">If we treat each tree in the forest as a set of vertices, then we can abstract the problem of implementing Kruskal's algorithm and define a data structure problem called the disjoint-set data structure problem. In this problem, we maintain a collection of disjoint sets, and support the following operations:</span></p>
<ul>
  <li><span style="font-size: 19px;">MAKE-SET: Create a set containing a single vertex.</span></li>
  <li><span style="font-size: 19px;">FIND: Given a vertex, find the name of the set it is in.</span></li>
  <li><span style="font-size: 19px;">UNION: union together two sets.</span></li>
</ul>
<p><span style="font-size: 19px;">This is also called the "union-find" problem.</span></p>
<p><span style="font-size: 19px;">Assume that we have a data structure solution to the union-find problem, then we can use it to implement Kruskal's algorithm as follows:</span></p>
<pre><span style="font-size: 19px;">Kruskal(G, w) // G = (V, E) and w is the weight function on edges
1  A ← Φ

2  for each v in V do
3    MAKE-SET(v)

4  Sort the edges of E by increasing order of weight

5  for each edge(u, v) in E in increasing order of weight do
6    if FIND(u) ≠ FIND(v) then
7      A ← A ∪ {(u, v)}
8     UNION(u, v)

9  return A</span></pre>
<p><span style="font-size: 19px;">Chapter 19 (or 21 in the 3rd ed) of CLRS discusses the union-find problem, and it is a typical topic for a 4th-year (elective) algorithm course. If you are interested, you can read that chapter.</span></p>
<p><span style="font-size: 19px;">In this course, I will show an easy solution whose running time is acceptable here. When applying it to Kruskal's algorithm, we will achieve the same asymptotic running time as what we would achieve by applying the more advanced solutions in Chapter 19 (though, for other problems, those more advanced solutions may be better).</span></p>
<p><span style="font-size: 19px;">In our solution, we represent each set as a linked list of vertices. The name of each set is the first element in its list. For each set, we also maintain a tail pointer that points to the end of the list, and a number filed which records the number of elements in the list. Each element in the list stores two pointers: one to it next element, and the other to its list head (we call this pointer the head pointer).</span></p>
<p><span style="font-size: 19px;">With the above structure, it is easy to support MAKE-SET and FIND in O(1) time. To support union(u, v), we perform the following steps:</span></p>
<ol>
<li><span style="font-size: 19px;">Follow the head pointer of u to its head. Follow the head pointer of v to its head.</span></li>
<li><span style="font-size: 19px;">From the number field we can tell which list is smaller. Assume without loss of generality that u is in the smaller list. We link u's list to the end of v's list (the bigger list).</span></li>
<li><span style="font-size: 19px;">Traverse u's list, updating head pointers.</span></li>
<li><span style="font-size: 19px;">Update the number field and tail pointer of v's list.</span></li>
</ol>
<p><span style="font-size: 19px;">The above process uses O(t) time, where t is the size of the smaller list.</span></p>
<p><span style="font-size: 19px;">To analyze the running time, we can tell that lines 2-3 use O(|V|) time, line 4 uses O(|E| lg |E|) time, and lines 5-7 use O(|E|) time. What about line 8?</span></p>
<p><span style="font-size: 19px;">To analyze line 8, let's consider the following problem: If we call UNION n-1 times to union n sets (the initial size of each set is 1), what is the total running time? For this we first observe that in each union, the smaller list becomes part of a list of at least twice the size. Therefore, an element's head pointer is updated at most O(lg n) times during all the n operations. Thus n-1 UNION operations use O(n lg n) time in total.</span></p>
<p><span style="font-size: 19px;">With the above analysis, we can see that line 8 uses O(|V| lg |V|) time, and thus the total running time of our implementation of Kruskal's running time is O(|E| lg |E|), which is also O(|E| lg |V|), as |V| &lt; |E|<sup>2</sup>.</span></p>
<h3><span style="font-size: 19px;">6. Prim's Algorithm</span></h3>
<p><span style="font-size: 19px;">The second algorithm for the MST problem that we discuss is Prim's algorithm. Its greedy strategy is: At each step, choose a previously unconnected vertex that becomes connected by a lowest-weight edge. More precisely, we work with a set of vertices V', which initially contains an initial vertex arbitrarily chosen. We also work with a set of edges E', which is initially empty. At each step of the algorithm, we choose an edge (u, v) of minimal weight with u in V' and v not in V', breaking ties arbitrarily. We add v to V' and (u, v) to E'. When V' = V, E' becomes the MST.</span></p>
<p><span style="font-size: 19px;">From this description, we can see that Prim's algorithm "grows" a tree into a MST, while Kruskal's algorithm merges a set of trees into a MST.</span></p>
<p><span style="font-size: 19px;">In class we walked through an example to learn Prim's algorithm. When reviewing this lecture, you could also study the example on page 595 of the textbook.</span></p>
<p><span style="font-size: 19px;">As with Kruskal's algorithm, we can also prove the correctness of Prim's algorithm using proof by contradiction. I left this as an exercise.</span></p>
<p><span style="font-size: 19px;">To implement Prim's algorithm, observe that in each step, we pick a vertex v not in V, and this vertex v has the minimum-cost edge connecting v to some vertex in V'. This suggests us to use a priority queue, which can be implemented as a heap. We learned priority queue/heap in CSCI 2110. You could also read Chapter 6 of CLRS to review it. Here for each vertex u, we set its key, key[u], to be the weight of the minimum-cost edge connecting u to some vertex in V', and maintain the vertices not in V' in a heap structure. The following two operations are needed:</span></p>
<ul>
<li><span style="font-size: 19px;">EXTRAT-MIN, which removes the element with the minimum key value from the heap and returns it.</span></li>
<li><span style="font-size: 19px;">DECREASE-KEY, which decreases the key value of a given element.</span></li>
</ul>
<p><span style="font-size: 19px;">Both operations can be supported in O(lg n) time, where n is the number of elements in the heap.</span></p>
<p><span style="font-size: 19px;">In Prim's algorithm, we select an arbitrary vertex as the initial vertex. If we define this vertex as the root of the MST computed, and then we can define the following array π[1..|V|]. In this array, π[u] is the parent of u in MST. Thus in the MST, for each vertex u, there is an edge between u and π(u). Hence the array π defines the MST computed.</span></p>
<p><span style="font-size: 19px;">We have the following pseudocode:</span></p>
<pre><span style="font-size: 19px;">PRIM(G, w, r)   // r is the initial vertex / root of MST to be computed
1  for each vertex u in V do
2    key[u] ← ∞
3    π[u] ← NULL

4  key[r] ← 0
5  Q ← V  // build a heap containing all the vertices in Q

6  while Q ≠ Φ do
7    u ← EXTRACT-MIN(Q)
8    for each v in Adj[u] do // Adj is the adjacency list representation of G
9      if v in Q and w(u,v) &lt; key[v]
10       π[v] ← u
11       key[v] ← w(u,v)  // DECREASE-KEY is used here</span></pre>
<p><span style="font-size: 19px;">There is one minor detail: In line 9, how do we tell if v is in Q? This is easy, as we can simply maintain a flag for each vertex to indicate that.</span></p>
<p><span style="font-size: 19px;">To analyze the running time, we have that lines 1-3 use O(|V|) time, line 5 uses O(|V| lg |V|) time (as it requires O(n lg n) time to build a heap over n elements), and line 7 uses O(|V| lg |V|) time. To see the number of times that line 8 is executed, we observe that, since this line uses the adjacency list to visit each vertex's neighbor exactly once, it is executed O(|E|) times. From this we can see that line 11 uses O(|E| lg |V|) time. From the above analysis, we can see that the total running time is O(|E| lg |V|), which matches the running time of Kruskal's algorithm.</span></p>
<p><span style="font-size: 19px;">Finally, I would like to mention that if we use Fibonacci heaps (instead of the standard binary heap) to implement Prim's algorithm, we can improve its running time to O(|E| + |V| lg |V|). Fibonacci heaps are introduced in Chapter 17 (or 19 in the 3rd ed) of CLRS, and they are typically taught in 4th year elective algorithm courses.</span></p>
<h3><span style="font-size: 19px;">7. Single-Source Shortest Path Problems</span></h3>
<p><span style="font-size: 19px;">In a weighted, directed graph, the shortest path from a vertex s to another vertex v is a sequence of directed edges from s to v with the smallest total weight.</span></p>
<p><span style="font-size: 19px;">The computation of shortest path has many applications. The road maps stored in a GPS can be modeled as a graph, in which the vertices are intersections and the edges are roads between intersections. If we assign distances as weights of the edges, then the shortest path corresponds to the route of the shortest distance. Another application is in network routing. There are network routing protocols that use shortest path algorithms, such as IS-IS and GSPF.</span></p>
<p><span style="font-size: 19px;">In the single-source shortest path problem, we are given a source vertex s and a destination vertex v, and we would like to find the shortest path from s to v. For most of the known algorithms, it is as efficient to find the shortest path from s to all the other vertices of the graph, so in the rest of the lecture, we will compute the shortest paths from s to all other vertices.</span></p>
<p><span style="font-size: 19px;">Before discussing algorithmic solutions, let's see what happens if some edges weights could be negative. In particular, if there is a negative-weight cycle reachable from S, then there may not be a shortest path from s to another vertex. In this case, our task is to detect the negative-weight cycle. If there is no negative-weight cycle, from s to any vertex, there exists a shortest path that is simple.</span></p>
<p><span style="font-size: 19px;">We define the following structures that will be used in the two algorithmic solutions that we will learn:</span></p>
<ul>
<li><span style="font-size: 19px;">An array π[1..|V|], in which π[v] stores the predecessor of v along a path.</span></li>
<li><span style="font-size: 19px;">A shortest-path estimate array d[1..|V|], in which d[v] stores the current upper bound on the weight of the shortest path from s to v.</span></li>
</ul>
<p><span style="font-size: 19px;">The following procedure will be used to initialize these two arrays:</span></p>
<pre><span style="font-size: 19px;">INITIALIZE(G, s)</span><br><span style="font-size: 19px;">for each vertex v in V do
  d[v] ← ∞
  π[v] ← NULL
d[s] ← 0</span></pre>
<p><span style="font-size: 19px;">One key operation used in the two algorithms that we will learn is the relaxation of the edges. In this operation, we have shortest path estimates to two vertices, u and v. Then we consider an edge (u, v) to update our estimate on v. The following is the pseudocode:</span></p>
<pre><span style="font-size: 19px;">RELAX(u, v, w)</span><br><span style="font-size: 19px;">t ← d[u] + w(u, v)

if d[v] &gt; t then
  d[v] ← t
  π[v] ← u
</span></pre>
<p><span style="font-size: 19px;">From the pseudocode, we can see that this operation attempts to find a more accurate estimate on v.</span></p>

  <p><span style="font-size: 19px; color: #ffba59;">Section 2: Please finish the notes from the last lecture first.</span></p>
  <h3><span style="font-size: 19px;">The Bellman-Ford Algorithm</span></h3>
  <p><span style="font-size: 19px;">The Bellman-Ford algorithm solves the problem in the general case in which edge weights may be negative. When there is a reachable negative-weight cycle, it returns false.</span></p>
  <p><span style="font-size: 19px;">The algorithm essentially uses a loop to keep performing RELAX on all edges. Later we will prove that it is sufficient to iterate this loop |V|-1 times, but first let's see the pseudocode.</span></p>
  <pre><span style="font-size: 19px;">BELLMAN-FORD(G, w, s)</span><br><span style="font-size: 19px;">1  INITIALIZE(G, s)
  
  2  for i ← 1 to |V|-1 do
  3    for each edge (u, v) in E do
  4      RELAX(u, v, w)
  
  5  for each edge (u, v) in E do
  6    if d[v] &gt; d[u] + w(u, v) then
  7      return false
  
  8  return true</span></pre>
  <p><span style="font-size: 19px;">In the above pseudocode, lines 5-7 perform negative cycle detection, by finding whether we can further update the estimates using RELAX on all edges. It is easy to see that this algorithm uses O(|V||E|) time.</span></p>
  <p><span style="font-size: 19px;">Now let's prove the correctness of the algorithm. We do it in two major steps.</span></p>
  <p><span style="font-size: 19px;">In the first step, we prove the following lemma: After i iterations of the first "for" loop, the following two properties hold:</span></p>
  <ol>
  <li><span style="font-size: 19px;">If d[v] &lt; ∞, then d[v] stores the weight of some path from s to v.</span></li>
  <li><span style="font-size: 19px;">If there is a path from s to v with at most i edges, then d[v] is less than or equal to the weight of the shortest path from s to v having at most i edges.</span></li>
  </ol>
  <p><span style="font-size: 19px;">(Why do we need this lemma? Consider the following question: what is the maximum number of edges in a simple path? The answer is |V|-1. Thus, when i = |V|-1, the algorithm has considered all simple paths, and from the two properties above, we can claim that, if there is no negative-weight cycle, d[v] would store the weight of the shortest-path from s to v after the first "for" loop.)</span></p>
  <p><span style="font-size: 19px;">Proof: We prove by induction on i.</span></p>
  <p><span style="font-size: 19px;">In the base case, i = 0. Before the first "for" loop, d[s] = 0, which is correct. For any other vertex u, d[u] = ∞, which is also correct, as there is no path from s to u using 0 edge.</span></p>
  <p><span style="font-size: 19px;">Assume the claim is true for i-1; we now prove it for i.</span></p>
  <p><span style="font-size: 19px;">To prove property 1, consider when a vertex v's estimated distance is updated by d[u] + w(u, v). By our induction hypothesis, d[u] is the weight of some path, p, from s to u. Then d[u] + w(u, v) is the weight of the path that follows p from s to u, and then follows a single edge to v.</span></p>
  <p><span style="font-size: 19px;">To prove property 2, consider the shortest path q from s to v with at most i edges. Let u be the last vertex before v on q. Then the part of q from s to u is the shortest path from s to u with at most i-1 edges. By the induction hypothesis, d[u] after i-1 iterations of the 1st "for" loop is less than or equal to the weight of this path. Therefore, d[u] + w(u, v) is less than or equal to the weight of q. In the i-th iteration of the first "for" loop, d[v] is compared with d[u] + w(u, v), and is set equal to it if d[u] + w(u, v) is smaller. Therefore, after i iterations of the first "for" loop, d[v] is less than or equal to the weight of the shortest path from s to v that uses at most i edges.</span></p>
  <p><span style="font-size: 19px;">To show the correctness of negative-weight cycle detention, we prove that the algorithm returns true iff there is no reachable negative-weight cycle.</span></p>
  <p><span style="font-size: 19px;">If there is no reachable negative-weight cycle, then every shortest path visits each vertex at most once, so the second "for" loop doesn't detect any possible shorter path. The algorithm returns true.</span></p>
  <p><span style="font-size: 19px;">If the algorithm returns true, then no shorter path is detected in the second "for" loop. Then for any reachable cycle v<sub>0</sub>, v<sub>1</sub>, ..., v<sub>k-1</sub>, v<sub>0</sub>, the following inequalities must hold:</span></p>
  <p><span style="font-size: 19px;">d[v<sub>1</sub>] &lt;= d[v<sub>0</sub>] + w(v<sub>0</sub>, v<sub>1</sub>)</span></p>
  <p><span style="font-size: 19px;">d[v<sub>2</sub>] &lt;= d[v<sub>1</sub>] + w(v<sub>1</sub>, v<sub>2</sub>)</span></p>
  <p><span style="font-size: 19px;">...</span></p>
  <p><span style="font-size: 19px;">d[v<sub>0</sub>] &lt;= d[v<sub>k-1</sub>] + w(v<sub>k-1</sub>, v<sub>0</sub>)</span></p>
  <p><span style="font-size: 19px;">If we add all these inequalities up, we have 0 &lt;= w(v<sub>0</sub>, v<sub>1</sub>) + w(v<sub>1</sub>, v<sub>2</sub>) + ... + w(v<sub>k-1</sub>, v<sub>0</sub>). That is, the weight of this cycle is nonnegative.</span></p>
  <h3><span style="font-size: 19px;">Dijkstra's Algorithm</span></h3>
  <p><span style="font-size: 19px;">Another algorithm for the single-source shortest path problem is Dijkstra's algorithm. This algorithm works for the special case in which all edge weights are nonnegative values. When implemented carefully, it is more efficient than Bellman-Ford.</span></p>
  <p><span style="font-size: 19px;">The idea is to maintain a set, S, of vertices for which you already know the shortest paths from the source. At each step, choose a vertex u from V-S with the minimum shortest-path estimate, add u to s, and relax all the edges leaving u.</span></p>
  <p><span style="font-size: 19px;">You can see that this algorithm is similar to the greedy process of Prim's algorithm. Indeed, this algorithm is also a greedy algorithm.</span></p>
  <p><span style="font-size: 19px;">We use priority queues again to implement Dijkstra's algorithm.</span></p>
  <pre><span style="font-size: 19px;">DIJKSTRA(G, w, s)</span><br><span style="font-size: 19px;">
  INITIALIZE(G, s)
  
  S ← Φ
  Q ← a min-priority queue on the vertices in V, keyed by their d values
  
  while Q ≠ Φ do
    u ← EXTRACT-MIN(Q)
    S ← S ∪ {u}
    for each vertex v in Adj[u] do
      RELAX(u, v, w)</span></pre>
  <p><span style="font-size: 19px; color: #ffba59;">Section 2: We did not have the time to talk about correctness, but we finished the running time analysis after the correctness part in the lecture. We will prove correctness in the next lecture.</span></p>
  <p><span style="font-size: 19px;">The correctness of Dijkstra's algorithm can be shown by proving the following statement: When a vertex u is added to S, d[u] stores the weight, δ(s, u), of the shortest path from s to u.</span></p>
  <p><span style="font-size: 19px;">This can be proved by contradiction. Let u be the first vertex added to S for which d[u] ≠ δ(s, u). In class I showed the main idea of this proof, and you can find its details on pages 622-623 (or 660-661 in the 3rd edition) of the textbook. Using the figure on page 622 (or 660 in the 3rd edition), the main steps in the proof show:</span></p>
  <ol>
  <li><span style="font-size: 19px;">d[x] = δ(s, x)</span></li>
  <li><span style="font-size: 19px;">d[y] = δ(s, y) &lt;= δ(s, u) &lt;= d[u] (reasons: edge weighs are nonnegative; assumption)</span></li>
  <li><span style="font-size: 19px;">d[y] &gt;= δ(s, u) (due to the EXTRACT-MIN step in the algorithm)</span></li>
  <li><span style="font-size: 19px;">d[y] = δ(s, u), and thus all the &lt;= has to be = in 2.</span></li>
  </ol>
  <p><span style="font-size: 19px;">The analysis of the running time of Dijkstra's algorithm is similar to that of the Prim's algorithm. If we use a standard binary heap, the running time is O((|V|+|E|)lg |V|). If all vertices are reachable from s, then this can be simplified to O(|E| lg |V|). If we use a Fibonacci heap, then the running time can be improved to O(|V| lg |V| + |E|). From this we can see that Dijkstra's algorithm is more efficient than Bellman-Ford, but it requires that edge weights are nonnegative.</span></p>


<p>Lecture21</p>
<p><span style="font-size: 19px;"></span></p>
<h3><span style="font-size: 19px;">All-Pairs Shortest Path</span></h3>
<p><span style="font-size: 19px;">In the all-pairs shortest path problem, our goal is to find the shortest paths between all pairs of vertices in a graph.</span></p>
<p><span style="font-size: 19px;">One way of solving this problem is to run algorithms for the single-source shortest path problem |V| times, and each time we run it, we choose a different vertex as the source vertex. If all edge weights are non-negative, we can use Dijkstra's algorithm here. If we implement Dijkstra's algorithm using a heap, then the running time is O(|V|<sup>2</sup>lg|V| + |V||E| lg|V|) = O(|V|<sup>3</sup>lg|V|). If we use a Fibonacci heap instead, the running time is O(|V|<sup>2</sup>lg|V| + |V||E|) = O(|V|<sup>3</sup>). If negative edge weights are allowed, we can use the Bellman-Ford algorithm, requiring O(|V|<sup>2</sup>|E|) = O(|V|<sup>4</sup>) time.</span></p>
<p><span style="font-size: 19px;">Now, let us learn an elegant solution based on dynamic programming that was specifically designed for this problem.</span></p>
<h3><span style="font-size: 19px;">Floyd-Warshall Algorithm</span></h3>
<p><span style="font-size: 19px;">The Floyd-Warshall Algorithm is a dynamic programming algorithm designed for the all-pairs shortest path problem. It allows negative edge weights, but requires that there are no negative-weight cycle in the graph. This means that we just have to consider simple paths when computing the shortest paths.</span></p>
<p><span style="font-size: 19px;">For this algorithm, we number the vertices 1, 2, ..., n, where n = |V|. Now, consider all paths from vertex i to vertex j in which all intermediate vertices are numbered k or lower (a vertex in the path from i to j that is neither i or j is an intermediate vertex). Then, if vertex k appears in the shortest path, p, from i to j, we refer to the subpath of p from vertex i to vertex k as path p<sub>1</sub>, and the subpath from k to j as p<sub>2</sub>. (It may help if you check the figure on <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=230447&amp;type=content&amp;rcode=dalhousie-2566823" target="_self">page 4 here</a>.) We can immediately conclude that p<sub>1</sub>&nbsp;must be the shortest path from i to k in which all intermediate vertices are numbered k - 1 or lower, and p<sub>2</sub>&nbsp;must be the shortest path from k to j in which all intermediate vertices are numbered k - 1 or lower.</span></p>
<p><span style="font-size: 19px;">Let d<sub>ij</sub><sup>(k)</sup>&nbsp;denote the weight of the shortest path from i to j in which all intermediate vertices are numbered k or lower. Then, based on the above observation, we have the following recurrence: If k = 0, then d<sub>ij</sub><sup>(k)</sup>&nbsp;= w<sub>ij</sub>. Otherwise, d<sub>ij</sub><sup>(k)</sup>&nbsp;= min(d<sub>ij</sub><sup>(k-1)</sup>, d<sub>ik</sub><sup>(k-1)</sup>&nbsp;+ d<sub>kj</sub><sup>(k-1)</sup>).</span></p>
<p><span style="font-size: 19px;">We can compute d<sub>ij</sub><sup>(k)</sup>&nbsp;in increasing order of k as follows:</span></p>
<pre><span style="font-size: 19px;">Floyd-Warshall(W, n)
   // W is an n by n matrix of weights (the adjacency matrix representation of the weighted graph G)
   // output array is an n by n array of shortest path weights
   
   D<sup>(0)</sup> ← W
   for k ← 1 to n do
     let D<sup>(k)</sup> = ( d<sub>ij</sub><sup>(k)</sup>) be a new n by n matrix
     for i ← 1 to n do
       for j ← 1 to n do
         d<sub>ij</sub><sup>(k)</sup> ← min(d<sub>ij</sub><sup>(k-1)</sup>, d<sub>ik</sub><sup>(k-1)</sup> + d<sub>kj</sub><sup>(k-1)</sup>)
         
   return D<sup>(n)</sup></span></pre>
<p><span style="font-size: 19px;">The running time is O(n<sup>3</sup>).&nbsp;</span></p>
<p><span style="font-size: 19px; color: #ffba59;">Section 2 students can stop here.</span></p>
<p><span style="font-size: 19px;">The D<sup>(n) </sup>table gives us the weights of shortest paths. How do we construct the shortest paths themselves? There are multiple approaches. Here I will describe one that is the easiest to explain. Alternative approaches can be found in the textbook.</span></p>
<p><span style="font-size: 19px;">First, recall that we use a predecessor array π to encode the shortest paths in the algorithms for single-source shortest path problem. This time, since any vertex can be the source vertex, we need to define π as an n by n 2D array, in which π[i, j] stores the predecessor of j on the shortest path from i to j. If we can compute all entries of π, then we can get the nodes on the shortest path from vertex i to vertex j in reverse order: j, π[i, j], π[i, π[i, j]], ..., like what we did for the single-source shortest path problem.</span></p>
<p><span style="font-size: 19px;">Then, how do we compute the entries of π? Let us focus on an arbitrary entry π[i, j]. (It may help if you check the figure on <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=230447&amp;type=content&amp;rcode=dalhousie-2566823" target="_self">page 7 here</a>.) We can see that, if v is the predecessor of the vertex j on the shortest path from i to j, then we have d<sub>iv</sub><sup>(n)</sup> + w(v, j) = d<sub>ij</sub><sup>(n)</sup>. Therefore, we can compute π[i, j] by checking all edges that enter j and find the edge (v, j) that satisfies this equation, and then π[i, j] = v. This can be done by search through the j-th column of the adjacency matrix, which requires O(|V|) time.&nbsp;</span></p>
<p><span style="font-size: 19px;">Since π has |V|<sup>2</sup> entries, this procedure uses O(|V|<sup>3</sup>) time.</span></p>


<p><span style="font-size: 19px;">At the beginning of the term, I said that since the 4th edition of the textbook has been released, I plan to update a few lectures this term. At that time I said I wouldn't have any recorded videos to post for lectures that are heavily updated, but detailed HTML notes will be posted; it should be fine to study these notes if you miss any of these lectures.</span></p>
<p><span style="font-size: 19px;">This topic is updated; I didn't teach it in the last few years. It is however an important topic which also happens to reinforce our learning of dynamic programming. Therefore, I removed some content from some earlier lectures (in the first half of the term) to have the time to teach this one.&nbsp;</span></p>
<p><span style="font-size: 19px;">Since I am still recovering from the broken elbow, it is not possible to raise my arm high to write on the whiteboard.&nbsp; On the other hand, in a follow-up doctor's appointment, the doctor told me that it is OK to move my right arm gently now. Therefore, I chose to write on a tablet with a digital stylus in class and project the writing in class. This is close to what we did before I broke the elbow (writing on a tablet in real time vs on whiteboards). Since I was using a tablet anyway, I told the class that I would record the screencast and post it.</span></p>
<p><span style="font-size: 19px;">However, it turned out that the voice in the second half of the recording is heavily distorted and unusable at all (almost white noise with some unrecognizable words). I tried again in section 2, doing shorter recordings instead of a full-length one, but it turned out to be even worse. After spending much time trying to diagnose this problem, my conclusion is that, because a tablet is typically passively cooled, prolonged screencasts and writing with a stylus in Microsoft OneNote simultaneously would create overheating. The internal sound module is then heavily affected.&nbsp;</span></p>
<p><span style="font-size: 19px;">This is different from recording lectures given by pre-typed PowerPoint slides, which can be recorded at very low framerates to avoid overheating. A screencast like what I did for online teaching before required high framerates; otherwise, the video would be too choppy to be bearable for most viewers, making it unusable. A PowerPoint presentation would probably also use less CPU than writing in OneNote at real time.</span></p>
<p><span style="font-size: 19px;">Back then, to make proper screencasts, I used a desktop with dedicated GPU and used a professional grade microphone, so I never encountered this issue. I do not have a highly portable setup for in-class teaching. It is not practical to investigate whether such a portable solution exists near the end of the term. Furthermore, I am also concerned that overheating might even affect in-class teaching when I use the tablet.</span></p>
<p><span style="font-size: 19px;">Therefore, in the remaining three lectures, I will continue to write on the tablet, but I will not attempt to record the live lectures to avoid hardware failure. I will ensure that detailed HTML notes will be posted, and the notes I wrote in class in OneNote will be posted as well. This is the original plan set at the beginning of the term for heavily updated lectures, with the notes from OneNote as a bonus.</span></p>
<p><span style="font-size: 19px;">The video posted below has only the first half of the live lecture. Please refer to the HTML notes and handwritten notes for the second half.</span></p>

<video width="800" height="400" controls="controls" preload="none" ><source src="Video/lec21.mp4" type="video/mp4" /></video>
<p>Lecture note in onenote</p>
<img src="Pictures/21-1.png" alt="21-1">
<img src="Pictures/21-2.png" alt="21-2">
<img src="Pictures/21-3.png" alt="21-3">
<img src="Pictures/21-4.png" alt="21-4">
<img src="Pictures/21-5.png" alt="21-5">
<img src="Pictures/21-6.png" alt="21-6">
<img src="Pictures/21-7.png" alt="21-7">

</body>